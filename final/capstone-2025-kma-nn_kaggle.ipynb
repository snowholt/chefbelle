{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Recipe & Kitchen Management Assistant\n",
    "\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The Interactive Recipe & Kitchen Management Assistant helps users:\n",
    "1. Discover recipes based on available ingredients\n",
    "2. Customize recipes according to dietary needs\n",
    "3. Receive step-by-step cooking guidance\n",
    "\n",
    "This assistant will use multiple Gen AI capabilities including:\n",
    "- Audio Understanding: Processed voice commands (Speech-to-Text).\n",
    "- Function Calling: Enabled agent interaction with database/APIs.\n",
    "- Agents (LangGraph): Orchestrated conversational flow and tool use.\n",
    "- Grounding: Used Google Search for external knowledge/answers.\n",
    "- Embeddings: Powered semantic search for recipes.\n",
    "- Retrieval Augmented Generation (RAG): Retrieved recipes from DB to enhance responses.\n",
    "- Vector Database (ChromaDB): Stored and searched recipe embeddings.\n",
    "- Structured Output (JSON): Facilitated tool interactions and data exchange.\n",
    "- Context Caching & Stateful Conversation: cached state to generate context-aware responses\n",
    "- GenAI evaluation: enabling the agent to reason with structured, fetched data, not just hallucinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \"Good code is like a good joke â€“ it needs no explanation.\"\n",
    "â€” Gregor Hohpe\n",
    "\n",
    "- However... since we're still leveling up, I'll add a few helpful comments along the way.\n",
    "- Because even the best jokes need subtitles when you're just getting started! ðŸ˜…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Source & Setup\n",
    "\n",
    "This notebook implements the first step of our Interactive Recipe & Kitchen Management Assistant capstone project for the Google Gen AI Intensive Course. We'll acquire, explore, and prepare the recipe dataset that will serve as the foundation for our recipe retrieval and customization system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "Let's start by installing and importing the necessary libraries for data processing.\n",
    "\n",
    "This cell handles the installation and potential cleanup of required Python libraries. It ensures compatible versions of `google-generativeai`, `chromadb`, `google-cloud-speech`, `pandas`, `matplotlib`, `seaborn`, `kagglehub`, `soundfile`, `pydub`, `ipywidgets`, and `openai` are installed for the project. It includes commands to uninstall potentially conflicting versions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:23:45.903525Z",
     "iopub.status.busy": "2025-04-18T13:23:45.903203Z",
     "iopub.status.idle": "2025-04-18T13:24:30.069863Z",
     "shell.execute_reply": "2025-04-18T13:24:30.068832Z",
     "shell.execute_reply.started": "2025-04-18T13:23:45.903501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up and install compatible versions\n",
    "#!pip uninstall -y tensorflow protobuf google-api-core google-cloud-automl google-generativeai google-cloud-translate chromadb\n",
    "!pip uninstall -qqy kfp > /dev/null 2>&1\n",
    "\n",
    "# Install chromadb with compatible versions\n",
    "!pip install -qU --no-warn-conflicts \"google-genai==1.7.0\" chromadb==0.6.3 \n",
    "# #!pip install -U google-api-core==2.16.0\n",
    "\n",
    "!pip install -q --no-warn-conflicts google-cloud-speech\n",
    "\n",
    "# Install base packages with minimal dependencies\n",
    "!pip install -q --no-warn-conflicts pandas matplotlib seaborn \n",
    "!pip install -q --no-warn-conflicts kagglehub[pandas-datasets]\n",
    "!pip install -q --no-warn-conflicts soundfile pydub ipywidgets openai\n",
    "\n",
    "# Install compatible versions\n",
    "#!pip install -q google-generativeai  # Latest version instead of 1.7.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install LangGraph and Dependencies\n",
    "\n",
    "Uninstalls potentially conflicting packages from the base environment and installs specific versions of `langgraph`, `langchain-google-genai`, and `langgraph-prebuilt` required for the agent implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:24:30.072146Z",
     "iopub.status.busy": "2025-04-18T13:24:30.071858Z",
     "iopub.status.idle": "2025-04-18T13:24:44.722536Z",
     "shell.execute_reply": "2025-04-18T13:24:44.721389Z",
     "shell.execute_reply.started": "2025-04-18T13:24:30.072119Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove conflicting packages from the Kaggle base environment.\n",
    "!pip uninstall -qqy kfp libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n",
    "# Install langgraph and the packages used in this lab.\n",
    "!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Step 1: Define State Schema (`KitchenState`)\n",
    "\n",
    "Defines the `TypedDict` class `KitchenState` which represents the shared memory or state of the LangGraph agent. It includes fields for message history, user input, parsed intent, context (like selected recipe ID), raw tool outputs, processed data, user preferences, and control flow flags. An initial state dictionary is also defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:24:44.724507Z",
     "iopub.status.busy": "2025-04-18T13:24:44.724101Z",
     "iopub.status.idle": "2025-04-18T13:24:48.205959Z",
     "shell.execute_reply": "2025-04-18T13:24:48.205006Z",
     "shell.execute_reply.started": "2025-04-18T13:24:44.724473Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "\n",
    "Imports all necessary libraries for the project, organized by category (Standard Library, Data Analysis, Networking, Widgets, Vector DB, LangChain/LangGraph, Google APIs, Audio, Speech-to-Text). It also includes setup for Gemini API retries, optional audio/speech library checks, and visualization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:24:48.208547Z",
     "iopub.status.busy": "2025-04-18T13:24:48.208230Z",
     "iopub.status.idle": "2025-04-18T13:24:52.642150Z",
     "shell.execute_reply": "2025-04-18T13:24:52.641173Z",
     "shell.execute_reply.started": "2025-04-18T13:24:48.208520Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ðŸ“¦ Standard Library Imports\n",
    "# ===============================\n",
    "import contextlib\n",
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sqlite3\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Any, Dict, List, Literal, Optional, Sequence, Tuple\n",
    "from io import StringIO\n",
    "\n",
    "# ===============================\n",
    "# ðŸ“Š Data Analysis & Visualization\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ===============================\n",
    "# ðŸŒ Networking & APIs\n",
    "# ===============================\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# ===============================\n",
    "# ðŸ§  Widgets & Interactive Display\n",
    "# ===============================\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, Image, Markdown, clear_output, display, HTML\n",
    "\n",
    "# ===============================\n",
    "# ðŸ” ChromaDB Vector DB\n",
    "# ===============================\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# ===============================\n",
    "# ðŸ’¬ LangChain / LangGraph\n",
    "# ===============================\n",
    "from langchain_core.messages import (\n",
    "    AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage\n",
    ")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# ===============================\n",
    "# ðŸ¤– Google Gemini & Speech APIs\n",
    "# ===============================\n",
    "from google import genai\n",
    "from google.api_core import retry\n",
    "from google.genai import types\n",
    "\n",
    "# Retry mechanism for Gemini API\n",
    "is_retriable = lambda e: (\n",
    "    isinstance(e, genai.errors.APIError) and e.code in {429, 503}\n",
    ")\n",
    "genai.models.Models.generate_content = retry.Retry(predicate=is_retriable)(\n",
    "    genai.models.Models.generate_content\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# ðŸŽ¤ Audio Processing (Optional)\n",
    "# ===============================\n",
    "try:\n",
    "    import soundfile as sf\n",
    "    import sounddevice as sd\n",
    "    AUDIO_LIBRARIES_AVAILABLE = True\n",
    "    print(\"Audio libraries imported successfully!\")\n",
    "except (ImportError, OSError) as e:\n",
    "    AUDIO_LIBRARIES_AVAILABLE = False\n",
    "    print(f\"Warning: Audio libraries could not be imported: {e}\")\n",
    "\n",
    "# ===============================\n",
    "# ðŸ—£ï¸ Google Cloud Speechâ€‘toâ€‘Text (Optional)\n",
    "# ===============================\n",
    "try:\n",
    "    from google.cloud import speech\n",
    "    GOOGLE_SPEECH_AVAILABLE = True\n",
    "    print(\"Google Cloud Speechâ€‘toâ€‘Text is imported successfully!\")\n",
    "except ImportError:\n",
    "    GOOGLE_SPEECH_AVAILABLE = False\n",
    "    print(\n",
    "        \"Google Cloud Speechâ€‘toâ€‘Text not available. \"\n",
    "        \"Will use simulation for speech recognition.\"\n",
    "    )\n",
    "\n",
    "# ===============================\n",
    "# ðŸŽ¨ Visualization & Display Setup\n",
    "# ===============================\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(\"Google Gemini version:\", genai.__version__)\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# Extra helper / utility imports\n",
    "# --------------------------------\n",
    "from typing import Dict, Optional  # additional hints for helpers\n",
    "from langgraph.graph import END   # used by visualize_nutrition_node\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# ---> ADDED Markdown display <---\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "# Sentimentâ€‘analysis placeholder (optional)\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"Warning: vaderSentiment library not found. \"\n",
    "        \"pip install vaderSentiment for review sentiment analysis.\"\n",
    "    )\n",
    "    analyzer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Database Paths\n",
    "\n",
    "Sets the file paths for storing the ChromaDB vector database and the SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:24:52.643442Z",
     "iopub.status.busy": "2025-04-18T13:24:52.642974Z",
     "iopub.status.idle": "2025-04-18T13:24:52.647829Z",
     "shell.execute_reply": "2025-04-18T13:24:52.646807Z",
     "shell.execute_reply.started": "2025-04-18T13:24:52.643419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths for ChromaDB and SQL database\n",
    "VECTOR_DB_PATH = \"/kaggle/input/food-com-vectorized-with-chromadb/vector_db\"\n",
    "#DB_PATH = \"final/kitchen_db.sqlite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:24:52.650939Z",
     "iopub.status.busy": "2025-04-18T13:24:52.650056Z",
     "iopub.status.idle": "2025-04-18T13:24:53.052160Z",
     "shell.execute_reply": "2025-04-18T13:24:53.051337Z",
     "shell.execute_reply.started": "2025-04-18T13:24:52.650905Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # Check Python paths\n",
    "# print(\"Python path:\")\n",
    "# for path in sys.path:\n",
    "#     print(path)\n",
    "\n",
    "# # Try to find chromadb\n",
    "# try:\n",
    "#     import chromadb\n",
    "#     print(f\"\\nchromadb imported as: {type(chromadb)}\")\n",
    "#     print(f\"chromadb location: {chromadb.__file__}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nError importing chromadb: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`, `GOOGLE_APPLICATION_CREDENTIALS`, `OPENAI_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.\n",
    "\n",
    "Furthermore, for the Google Cloud Client Libraries (like the google-cloud-speech Python library you're using), you generally cannot authenticate using only an API Key. ðŸš«ðŸ”‘, So you need to provide and import Service Account Credentials (JSON Key File)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:33:29.831675Z",
     "iopub.status.busy": "2025-04-18T13:33:29.830871Z",
     "iopub.status.idle": "2025-04-18T13:33:30.518112Z",
     "shell.execute_reply": "2025-04-18T13:33:30.517333Z",
     "shell.execute_reply.started": "2025-04-18T13:33:29.831642Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n",
    "SecretValueJson = UserSecretsClient().get_secret(\"GOOGLE_APPLICATION_CREDENTIALS\") # Use the label you gave the secret\n",
    "USDA_API_KEY = UserSecretsClient().get_secret(\"USDA_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:24:53.732485Z",
     "iopub.status.busy": "2025-04-18T13:24:53.732077Z",
     "iopub.status.idle": "2025-04-18T13:24:53.738796Z",
     "shell.execute_reply": "2025-04-18T13:24:53.737891Z",
     "shell.execute_reply.started": "2025-04-18T13:24:53.732447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the os module to access environment variables\n",
    "\n",
    "# In cell 1cef5527...\n",
    "\n",
    "# Import the os module to access environment variables\n",
    "# import os\n",
    "\n",
    "\n",
    "# # --- Function to retrieve keys (optional verification) ---\n",
    "# def get_api_key(key_name):\n",
    "#     \"\"\"\n",
    "#     Retrieve an API key from environment variables.\n",
    "#     \"\"\"\n",
    "#     api_key = os.environ.get(key_name)\n",
    "#     if api_key is None:\n",
    "#         print(f\"Warning: {key_name} environment variable not found.\")\n",
    "#     return api_key\n",
    "\n",
    "# # Example verification\n",
    "# GOOGLE_API_KEY= get_api_key(\"GOOGLE_API_KEY\")\n",
    "# SecretValueJson = get_api_key(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "# USDA_API_KEY = get_api_key(\"USDA_API_KEY\")\n",
    "# OPENAI_API_KEY = get_api_key(\"OPENAI_API_KEY\")\n",
    "\n",
    "# print(f\"Google API Key exists: {GOOGLE_API_KEY is not None}\")\n",
    "# print(f\"SecretValueJson Path exists: {SecretValueJson is not None}\")\n",
    "# print(f\"USDA API Key exists: {USDA_API_KEY is not None}\")\n",
    "# print(f\"OpenAI API Key exists: {OPENAI_API_KEY is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Loading\n",
    "\n",
    "### Importing the Dataset in Kaggle\n",
    "\n",
    "Since you're using Kaggle, you can easily import the Food.com Recipes dataset directly:\n",
    "\n",
    "1. Search for \"Food.com Recipes and User Interactions\" in the Kaggle datasets section\n",
    "2. Or use this direct link: https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions\n",
    "\n",
    "In Kaggle, you can either:\n",
    "- Add the dataset to your notebook directly from the \"Add data\" button in the right sidebar\n",
    "- Use the Kaggle datasets API as shown below\n",
    "\n",
    "\n",
    "We'll use the Food.com Recipes and Interactions dataset. This contains recipe information including ingredients, steps, and user interactions.\n",
    "\n",
    "If you've downloaded the dataset using the Kaggle API, uncomment and use the data loading code below. Otherwise, we'll use a direct URL to access the data.\n",
    "\n",
    "loading both the vectorized and raw data and nutritional breakdown dataset that will be used in subsequent steps, particularly for the few-shot prompting recipe customization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:24:53.740127Z",
     "iopub.status.busy": "2025-04-18T13:24:53.739793Z",
     "iopub.status.idle": "2025-04-18T13:25:32.183453Z",
     "shell.execute_reply": "2025-04-18T13:25:32.182562Z",
     "shell.execute_reply.started": "2025-04-18T13:24:53.740096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Option 1: Direct Kaggle dataset import\n",
    "# This is the easiest way to import datasets in Kaggle notebooks\n",
    "\n",
    "try:\n",
    "    # If the dataset is added via the \"Add data\" button, it will be available at /kaggle/input/\n",
    "    recipes_df = pd.read_csv('/kaggle/input/food-com-recipes-and-user-interactions/RAW_recipes.csv')\n",
    "    interactions_df = pd.read_csv('/kaggle/input/food-com-recipes-and-user-interactions/RAW_interactions.csv')\n",
    "    nutrition_df = pd.read_csv('/kaggle/input/nutritional-breakdown-of-foods/cleaned_nutrition_dataset.csv')\n",
    "    print(f\"Successfully loaded {len(recipes_df)} recipes\")\n",
    "    print(f\"Successfully loaded {len(interactions_df)} interactions\")\n",
    "    print(f\"Successfully loaded nutritional dataset with {len(nutrition_df)} records\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset files not found. Please make sure you've added the dataset to your Kaggle notebook.\")\n",
    "    print(\"You can add it by clicking the 'Add data' button in the right sidebar.\")\n",
    "    print(\"Alternatively, you can use direct URLs if available.\")\n",
    "\n",
    "# Let's parse the JSON strings in the columns that contain lists\n",
    "if 'recipes_df' in locals():\n",
    "    # Check the actual structure of the dataframe\n",
    "    \n",
    "    # For Food.com dataset, ingredients, steps, and tags are stored as strings that represent lists\n",
    "    # We need to convert them from string representation to actual Python lists\n",
    "    try:\n",
    "        if 'ingredients' in recipes_df.columns:\n",
    "            recipes_df['ingredients'] = recipes_df['ingredients'].apply(eval)\n",
    "            print(\"Successfully parsed ingredients column\")\n",
    "        \n",
    "        if 'steps' in recipes_df.columns:\n",
    "            recipes_df['steps'] = recipes_df['steps'].apply(eval)\n",
    "            print(\"Successfully parsed steps column\")\n",
    "        \n",
    "        if 'tags' in recipes_df.columns:\n",
    "            recipes_df['tags'] = recipes_df['tags'].apply(eval)\n",
    "            print(\"Successfully parsed tags column\")\n",
    "            \n",
    "            # Add cuisine type based on tags\n",
    "            recipes_df['cuisine_type'] = recipes_df['tags'].apply(\n",
    "                lambda x: next((tag for tag in x if tag in ['italian', 'persian', 'mexican', 'chinese', 'indian', 'french', 'thai']), 'other')\n",
    "            )\n",
    "        \n",
    "      \n",
    "        # Count number of ingredients\n",
    "        recipes_df['n_ingredients'] = recipes_df['ingredients'].apply(len)\n",
    "            \n",
    "        print(\"\\nDataset successfully processed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {e}\")\n",
    "        print(\"Column sample values:\")\n",
    "        for col in recipes_df.columns:\n",
    "            print(f\"{col}: {recipes_df[col].iloc[0]}\")\n",
    "\n",
    "print(f\"\\n{'-' * 30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's explore the dataset to understand its structure and content. This will help us plan our cleaning and preprocessing steps.\n",
    "### Analyze DataFrame Properties\n",
    "\n",
    "Defines and calls a function `analyze_dataframe` to inspect the loaded DataFrames (`recipes_df`, `interactions_df`, `nutrition_df`). It prints data types, missing value percentages, and summary statistics for numeric columns for each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:32.186923Z",
     "iopub.status.busy": "2025-04-18T13:25:32.186577Z",
     "iopub.status.idle": "2025-04-18T13:25:38.767602Z",
     "shell.execute_reply": "2025-04-18T13:25:38.766746Z",
     "shell.execute_reply.started": "2025-04-18T13:25:32.186903Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to analyze dataframe properties with visualizations\n",
    "def analyze_dataframe(df, df_name):\n",
    "    print(f\"\\n{'-' * 30}\")\n",
    "    print(f\"Analysis for {df_name}:\")\n",
    "    print(f\"{'-' * 30}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(\"\\nData types:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].dtype}\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    has_missing = missing_values[missing_values > 0]\n",
    "    if not has_missing.empty:\n",
    "        for col, missing in has_missing.items():\n",
    "            print(f\"{col}: {missing} missing values ({missing/len(df):.2%})\")\n",
    "        \n",
    "        # ðŸ”´ Plot missing values\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.barplot(x=has_missing.index, y=has_missing.values)\n",
    "        plt.title(f\"Missing Values in {df_name}\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No missing values\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nNumeric columns summary:\")\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    if numeric_cols:\n",
    "        stats = df[numeric_cols].describe().T[['count', 'mean', 'min', 'max']]\n",
    "        print(stats)\n",
    "        \n",
    "        # ðŸ“Š Histograms of numeric columns\n",
    "        df[numeric_cols].hist(bins=30, figsize=(15, 10), color='lightblue', edgecolor='black')\n",
    "        plt.suptitle(f\"Distributions of Numeric Columns - {df_name}\", fontsize=14)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No numeric columns found.\")\n",
    "\n",
    "print(\"\\n=== DATA ANALYSIS FOR ALL DATAFRAMES ===\")\n",
    "analyze_dataframe(recipes_df, \"Recipes\")\n",
    "analyze_dataframe(interactions_df, \"Interactions\")\n",
    "analyze_dataframe(nutrition_df, \"Nutrition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Sample Recipe Data\n",
    "\n",
    "Prints a random sample of 3 rows from the `recipes_df` DataFrame to provide a quick look at the data structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:38.768616Z",
     "iopub.status.busy": "2025-04-18T13:25:38.768396Z",
     "iopub.status.idle": "2025-04-18T13:25:38.784385Z",
     "shell.execute_reply": "2025-04-18T13:25:38.783502Z",
     "shell.execute_reply.started": "2025-04-18T13:25:38.768599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample a few rows instead of full stats\n",
    "print(\"\\nSample rows:\")\n",
    "print(recipes_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Cuisine Distribution\n",
    "\n",
    "Generates a bar chart showing the distribution of the top 15 most frequent cuisine types found in the `recipes_df` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:38.785675Z",
     "iopub.status.busy": "2025-04-18T13:25:38.785367Z",
     "iopub.status.idle": "2025-04-18T13:25:39.203941Z",
     "shell.execute_reply": "2025-04-18T13:25:39.203038Z",
     "shell.execute_reply.started": "2025-04-18T13:25:38.785651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Top 15 cuisine types\n",
    "if 'cuisine_type' in recipes_df.columns:\n",
    "    top_cuisines = recipes_df['cuisine_type'].value_counts().nlargest(15)\n",
    "    df_top = top_cuisines.reset_index()\n",
    "    df_top.columns = ['cuisine', 'count']\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        data=df_top,\n",
    "        x='cuisine',\n",
    "        y='count',\n",
    "        hue='cuisine',\n",
    "        dodge=False,\n",
    "        palette=sns.color_palette(\"husl\", len(df_top)),\n",
    "    )\n",
    "\n",
    "    plt.legend().remove()  # ðŸ‘ˆ remove the legend here\n",
    "\n",
    "    plt.title('Top 15 Cuisine Types', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Cuisine Type', fontsize=12)\n",
    "    plt.ylabel('Number of Recipes', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:39.205144Z",
     "iopub.status.busy": "2025-04-18T13:25:39.204853Z",
     "iopub.status.idle": "2025-04-18T13:25:39.209265Z",
     "shell.execute_reply": "2025-04-18T13:25:39.208478Z",
     "shell.execute_reply.started": "2025-04-18T13:25:39.205115Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Distribution of cooking time - use smaller bins\n",
    "# if 'cooking_time' in recipes_df.columns:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     # Use log scale for better visualization if the range is large\n",
    "#     if recipes_df['cooking_time'].max() > 5 * recipes_df['cooking_time'].median():\n",
    "#         sns.histplot(recipes_df['cooking_time'].clip(upper=recipes_df['cooking_time'].quantile(0.95)), bins=20)\n",
    "#         plt.title('Distribution of Cooking Time (minutes) - Clipped at 95th percentile')\n",
    "#     else:\n",
    "#         sns.histplot(recipes_df['cooking_time'], bins=20)\n",
    "#         plt.title('Distribution of Cooking Time (minutes)')\n",
    "#     plt.xlabel('Cooking Time (minutes)')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Number of Ingredients Distribution\n",
    "\n",
    "Generates a histogram showing the distribution of the number of ingredients per recipe, using bins appropriate for typical ingredient counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:39.210568Z",
     "iopub.status.busy": "2025-04-18T13:25:39.210229Z",
     "iopub.status.idle": "2025-04-18T13:25:39.234985Z",
     "shell.execute_reply": "2025-04-18T13:25:39.234208Z",
     "shell.execute_reply.started": "2025-04-18T13:25:39.210550Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if 'n_ingredients' in recipes_df.columns:\n",
    "    data = recipes_df['n_ingredients'].dropna()\n",
    "    mean_val = data.mean()\n",
    "    std_val = data.std()\n",
    "\n",
    "    # Count frequency\n",
    "    counts = data.value_counts().sort_index()\n",
    "    df_hist = pd.DataFrame({'n_ingredients': counts.index, 'count': counts.values})\n",
    "\n",
    "    # Determine color based on statistical ranges\n",
    "    def color_range(val):\n",
    "        if val < mean_val - std_val:\n",
    "            return 'red'\n",
    "        elif val < mean_val:\n",
    "            return 'blue'\n",
    "        elif val < mean_val + std_val:\n",
    "            return 'green'\n",
    "        else:\n",
    "            return 'orange'\n",
    "\n",
    "    df_hist['color'] = df_hist['n_ingredients'].apply(color_range)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:39.236446Z",
     "iopub.status.busy": "2025-04-18T13:25:39.236110Z",
     "iopub.status.idle": "2025-04-18T13:25:40.206091Z",
     "shell.execute_reply": "2025-04-18T13:25:40.205312Z",
     "shell.execute_reply.started": "2025-04-18T13:25:39.236420Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=recipes_df, x='n_ingredients', bins=range(1, 31), kde=False, color='skyblue')\n",
    "plt.axvline(recipes_df['n_ingredients'].mean(), color='black', linestyle='--', label='Mean')\n",
    "plt.axvline(recipes_df['n_ingredients'].mean() + recipes_df['n_ingredients'].std(), color='orange', linestyle=':', label='+1 STD')\n",
    "plt.axvline(recipes_df['n_ingredients'].mean() - recipes_df['n_ingredients'].std(), color='red', linestyle=':', label='-1 STD')\n",
    "plt.title('How Many Recipes Use X Ingredients?')\n",
    "plt.xlabel('Number of Ingredients')\n",
    "plt.ylabel('Number of Recipes')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:40.207469Z",
     "iopub.status.busy": "2025-04-18T13:25:40.207115Z",
     "iopub.status.idle": "2025-04-18T13:25:41.675239Z",
     "shell.execute_reply": "2025-04-18T13:25:41.674383Z",
     "shell.execute_reply.started": "2025-04-18T13:25:40.207441Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(recipes_df['n_ingredients'].reset_index(drop=True), color='violet')\n",
    "plt.axhline(recipes_df['n_ingredients'].mean(), color='black', linestyle='--', label='Mean')\n",
    "plt.title('Ingredients Count per Recipe (Sequence)')\n",
    "plt.xlabel('Recipe Index')\n",
    "plt.ylabel('Number of Ingredients')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Now we'll clean the data by:\n",
    "1. Removing duplicate recipes\n",
    "2. Normalizing ingredient names\n",
    "3. Standardizing measurements\n",
    "4. Handling missing values\n",
    "5. Creating dietary tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Duplicate Removal Function\n",
    "\n",
    "Defines a function `check_remove_duplicates` that checks for and removes duplicate rows from a pandas DataFrame based on specified subset columns (or automatically detected ID/name columns). It prints the number of duplicates found and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:41.676649Z",
     "iopub.status.busy": "2025-04-18T13:25:41.676415Z",
     "iopub.status.idle": "2025-04-18T13:25:41.684309Z",
     "shell.execute_reply": "2025-04-18T13:25:41.683355Z",
     "shell.execute_reply.started": "2025-04-18T13:25:41.676631Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to check and remove duplicates in dataframes\n",
    "def check_remove_duplicates(df, df_name, subset_cols=None):\n",
    "    \"\"\"\n",
    "    Check and remove duplicates from a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: The dataframe to process\n",
    "        df_name: Name of the dataframe for printing\n",
    "        subset_cols: List of columns to consider for duplicates. If None, all columns are used.\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe with duplicates removed\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'-' * 30}\")\n",
    "    print(f\"Duplicate analysis for {df_name}:\")\n",
    "    print(f\"{'-' * 30}\")\n",
    "    \n",
    "    # If subset not specified, identify potential key columns\n",
    "    if subset_cols is None:\n",
    "        # Try to find ID-like columns first\n",
    "        id_cols = [col for col in df.columns if 'id' in col.lower()]\n",
    "        name_cols = [col for col in df.columns if 'name' in col.lower()]\n",
    "        \n",
    "        if id_cols:\n",
    "            subset_cols = id_cols\n",
    "            print(f\"Using ID columns for duplicate check: {subset_cols}\")\n",
    "        elif name_cols:\n",
    "            subset_cols = name_cols\n",
    "            print(f\"Using name columns for duplicate check: {subset_cols}\")\n",
    "        else:\n",
    "            # Use all columns if no suitable identifiers found\n",
    "            subset_cols = df.columns.tolist()\n",
    "            print(\"Using all columns for duplicate check\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    dup_count = df.duplicated(subset=subset_cols).sum()\n",
    "    print(f\"Number of duplicates in {df_name}: {dup_count} ({dup_count/len(df):.2%} of data)\")\n",
    "    \n",
    "    if dup_count > 0:\n",
    "        # Remove duplicates\n",
    "        df_cleaned = df.drop_duplicates(subset=subset_cols).reset_index(drop=True)\n",
    "        print(f\"Number of records after removing duplicates: {len(df_cleaned)}\")\n",
    "        return df_cleaned\n",
    "    else:\n",
    "        print(\"No duplicates found\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:41.685448Z",
     "iopub.status.busy": "2025-04-18T13:25:41.685180Z",
     "iopub.status.idle": "2025-04-18T13:25:42.162985Z",
     "shell.execute_reply": "2025-04-18T13:25:42.162185Z",
     "shell.execute_reply.started": "2025-04-18T13:25:41.685428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check and remove duplicates from all dataframes\n",
    "print(\"\\n=== DUPLICATE ANALYSIS FOR ALL DATAFRAMES ===\")\n",
    "recipes_df = check_remove_duplicates(recipes_df, \"Recipes\", subset_cols=['name'])\n",
    "interactions_df = check_remove_duplicates(interactions_df, \"Interactions\")\n",
    "nutrition_df = check_remove_duplicates(nutrition_df, \"Nutrition\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Ingredient Names\n",
    "\n",
    "Defines and applies a function `normalize_ingredients` to the `ingredients` column of `recipes_df`. This function converts ingredients to lowercase, removes leading quantities (simple regex), and strips whitespace. A new `normalized_ingredients` column is created. A sample comparison is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:42.164786Z",
     "iopub.status.busy": "2025-04-18T13:25:42.163961Z",
     "iopub.status.idle": "2025-04-18T13:25:47.139450Z",
     "shell.execute_reply": "2025-04-18T13:25:47.138659Z",
     "shell.execute_reply.started": "2025-04-18T13:25:42.164764Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to normalize ingredient names\n",
    "def normalize_ingredients(ingredient_list):\n",
    "    \"\"\"\n",
    "    Normalize ingredient names by removing quantities and standardizing format\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    # If ingredient_list is already a list of strings\n",
    "    if isinstance(ingredient_list, list):\n",
    "        for ingredient in ingredient_list:\n",
    "            # Skip empty ingredients\n",
    "            if not ingredient or not isinstance(ingredient, str):\n",
    "                continue\n",
    "            \n",
    "            # Remove quantities (simplified for demonstration)\n",
    "            cleaned = re.sub(r'^\\d+\\s+\\d+/\\d+\\s+', '', ingredient)\n",
    "            cleaned = re.sub(r'^\\d+/\\d+\\s+', '', cleaned)\n",
    "            cleaned = re.sub(r'^\\d+\\s+', '', cleaned)\n",
    "            \n",
    "            # Convert to lowercase and strip whitespace\n",
    "            cleaned = cleaned.lower().strip()\n",
    "            \n",
    "            normalized.append(cleaned)\n",
    "    else:\n",
    "        # Handle the case where ingredient_list might be a string or another format\n",
    "        print(\"Warning: Expected ingredient_list to be a list, but got:\", type(ingredient_list))\n",
    "        if isinstance(ingredient_list, str):\n",
    "            # Try to interpret as a string representation of a list\n",
    "            try:\n",
    "                actual_list = eval(ingredient_list) if ingredient_list.startswith('[') else [ingredient_list]\n",
    "                return normalize_ingredients(actual_list)\n",
    "            except:\n",
    "                normalized = [ingredient_list.lower().strip()]\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Apply normalization to ingredients - with error handling\n",
    "recipes_df['normalized_ingredients'] = recipes_df['ingredients'].apply(\n",
    "    lambda x: normalize_ingredients(x) if isinstance(x, list) or isinstance(x, str) else []\n",
    ")\n",
    "\n",
    "# Show a sample recipe with normalized ingredients\n",
    "if len(recipes_df) > 0:\n",
    "    sample_idx = 0\n",
    "    print(f\"Original ingredients: {recipes_df.iloc[sample_idx]['ingredients']}\")\n",
    "    print(f\"Normalized ingredients: {recipes_df.iloc[sample_idx]['normalized_ingredients']}\")\n",
    "else:\n",
    "    print(\"No recipes found in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and Visualize Dietary Tags\n",
    "\n",
    "Defines a function `identify_dietary_tags` that assigns basic dietary tags (vegetarian, vegan, gluten-free, low-carb, dairy-free) based on keyword matching within the `normalized_ingredients`. Applies this function to create a `dietary_tags` column. It then generates a bar chart showing the distribution of these tags and prints sample recipes with their assigned tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:47.140657Z",
     "iopub.status.busy": "2025-04-18T13:25:47.140367Z",
     "iopub.status.idle": "2025-04-18T13:25:49.670590Z",
     "shell.execute_reply": "2025-04-18T13:25:49.669769Z",
     "shell.execute_reply.started": "2025-04-18T13:25:47.140628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify dietary tags based on ingredients\n",
    "def identify_dietary_tags(ingredients):\n",
    "    \"\"\"\n",
    "    Identify dietary preferences based on ingredients\n",
    "    \"\"\"\n",
    "    # Handle empty ingredients list\n",
    "    if not ingredients or not isinstance(ingredients, (list, str)):\n",
    "        return []\n",
    "        \n",
    "    # Convert list of ingredients to a single string for easier checking\n",
    "    ingredients_str = ' '.join(ingredients).lower()\n",
    "    \n",
    "    tags = []\n",
    "    \n",
    "    # Vegetarian check (simplified)\n",
    "    meat_ingredients = ['chicken', 'beef', 'pork', 'lamb', 'turkey', 'veal', 'bacon']\n",
    "    if not any(meat in ingredients_str for meat in meat_ingredients):\n",
    "        tags.append('vegetarian')\n",
    "        \n",
    "        # Vegan check (simplified)\n",
    "        animal_products = ['cheese', 'milk', 'cream', 'yogurt', 'butter', 'egg', 'honey']\n",
    "        if not any(product in ingredients_str for product in animal_products):\n",
    "            tags.append('vegan')\n",
    "    \n",
    "    # Gluten-free check (simplified)\n",
    "    gluten_ingredients = ['flour', 'wheat', 'barley', 'rye', 'pasta', 'bread']\n",
    "    if not any(gluten in ingredients_str for gluten in gluten_ingredients):\n",
    "        tags.append('gluten-free')\n",
    "    \n",
    "    # Low-carb check (simplified)\n",
    "    high_carb_ingredients = ['sugar', 'pasta', 'rice', 'potato', 'bread', 'flour']\n",
    "    if not any(carb in ingredients_str for carb in high_carb_ingredients):\n",
    "        tags.append('low-carb')\n",
    "    \n",
    "    # Dairy-free check\n",
    "    dairy_ingredients = ['milk', 'cheese', 'cream', 'yogurt', 'butter']\n",
    "    if not any(dairy in ingredients_str for dairy in dairy_ingredients):\n",
    "        tags.append('dairy-free')\n",
    "    \n",
    "    return tags\n",
    "\n",
    "# Apply dietary tagging\n",
    "recipes_df['dietary_tags'] = recipes_df['normalized_ingredients'].apply(identify_dietary_tags)\n",
    "\n",
    "# Show the distribution of dietary tags\n",
    "diet_counts = {}\n",
    "for tags in recipes_df['dietary_tags']:\n",
    "    for tag in tags:\n",
    "        diet_counts[tag] = diet_counts.get(tag, 0) + 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "diet_df = pd.Series(diet_counts).sort_values(ascending=False)\n",
    "diet_df.plot(kind='bar')\n",
    "plt.title('Distribution of Dietary Tags')\n",
    "plt.xlabel('Dietary Tag')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show sample recipes with their dietary tags\n",
    "print(\"\\nSample recipes with dietary tags:\")\n",
    "sample_recipes = recipes_df[['name', 'normalized_ingredients', 'dietary_tags']].sample(5)\n",
    "for _, recipe in sample_recipes.iterrows():\n",
    "    print(f\"\\nRecipe: {recipe['name']}\")\n",
    "    print(f\"Ingredients: {', '.join(recipe['normalized_ingredients'])}\")\n",
    "    print(f\"Dietary Tags: {', '.join(recipe['dietary_tags']) if recipe['dietary_tags'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Final Dataset Information\n",
    "\n",
    "Prints a summary of the processed datasets, including the number of records and column names for `recipes_df`, `interactions_df`, and `nutrition_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:49.672378Z",
     "iopub.status.busy": "2025-04-18T13:25:49.671609Z",
     "iopub.status.idle": "2025-04-18T13:25:49.678342Z",
     "shell.execute_reply": "2025-04-18T13:25:49.677645Z",
     "shell.execute_reply.started": "2025-04-18T13:25:49.672347Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Basic dataset information\n",
    "print(\"Raw Datasets information:\")\n",
    "print(f\"Number of recipes: {len(recipes_df)}\")\n",
    "print(\"\\nrecipes_df columns:\")\n",
    "print(recipes_df.columns.tolist())\n",
    "print(15 * \"-\")\n",
    "print(f\"Number of interactions: {len(interactions_df)}\")\n",
    "print(\"\\ninteractions_df columns:\")\n",
    "print(interactions_df.columns.tolist())\n",
    "print(15 * \"-\")\n",
    "print(f\"Number of nutritions: {len(nutrition_df)}\")\n",
    "print(\"\\nnutrition_df columns:\")\n",
    "print(nutrition_df.columns.tolist())\n",
    "print(15 * \"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Structure and Storage\n",
    "\n",
    "### Prepare Datasets Proper Format for RAG Implementation\n",
    "\n",
    "Let's save each dataset in JSON format to facilitate their use in our Retrieval Augmented Generation (RAG) system. JSON format is highly compatible with various RAG implementations and will make it easier to load the data in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:49.679609Z",
     "iopub.status.busy": "2025-04-18T13:25:49.679275Z",
     "iopub.status.idle": "2025-04-18T13:25:49.694367Z",
     "shell.execute_reply": "2025-04-18T13:25:49.693547Z",
     "shell.execute_reply.started": "2025-04-18T13:25:49.679582Z"
    }
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "print(chromadb.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Database Setup Functions (SQL & ChromaDB)\n",
    "\n",
    "Defines functions to set up the data persistence layers:\n",
    "1.  `safe_convert`, `preprocess_dataframe`, `setup_sql_database`: Prepare DataFrames and store them in an SQLite database (`kitchen_db.sqlite`). Handles conversion of list/array types to strings for SQL compatibility.\n",
    "2.  `setup_vector_database`: Sets up a ChromaDB persistent client, creates collections for recipes (and optionally interactions), processes the DataFrames to extract documents and metadata, and adds them to the respective ChromaDB collections in batches. This enables semantic search later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:49.696083Z",
     "iopub.status.busy": "2025-04-18T13:25:49.695447Z",
     "iopub.status.idle": "2025-04-18T13:25:49.716794Z",
     "shell.execute_reply": "2025-04-18T13:25:49.715750Z",
     "shell.execute_reply.started": "2025-04-18T13:25:49.696054Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define paths for SQL database\n",
    "DB_PATH = \"final/kitchen_db.sqlite\"\n",
    "\n",
    "#####################\n",
    "# SQL Database Setup\n",
    "#####################\n",
    "def safe_convert(x):\n",
    "    \"\"\"\n",
    "    Safely converts a value to a string:\n",
    "      - If x is a list or numpy array, join its elements into a space-separated string.\n",
    "      - If x is not a list/array and is not null, convert to string.\n",
    "      - Otherwise, return an empty string.\n",
    "    \"\"\"\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        return \" \".join([str(item) for item in x])\n",
    "    return str(x) if pd.notna(x) else \"\"\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess DataFrame columns to be SQLite-compatible.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = df[column].apply(safe_convert)\n",
    "    return df\n",
    "\n",
    "def setup_sql_database(\n",
    "    recipes_df: pd.DataFrame, \n",
    "    interactions_df: pd.DataFrame, \n",
    "    nutrition_df: Optional[pd.DataFrame] = None,\n",
    "    db_path: str = DB_PATH\n",
    ") -> sqlite3.Connection:\n",
    "    \"\"\"\n",
    "    Set up SQLite database with raw dataframes.\n",
    "    \"\"\"\n",
    "    recipes_df = preprocess_dataframe(recipes_df)\n",
    "    interactions_df = preprocess_dataframe(interactions_df)\n",
    "    if nutrition_df is not None:\n",
    "        nutrition_df = preprocess_dataframe(nutrition_df)\n",
    "\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    print(f\"Creating SQLite database at {db_path}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    print(f\"Storing {len(recipes_df)} recipes in the database\")\n",
    "    recipes_df.to_sql('recipes', conn, if_exists='replace', index=False)\n",
    "    print(f\"Storing {len(interactions_df)} interactions in the database\")\n",
    "    interactions_df.to_sql('interactions', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    if nutrition_df is not None:\n",
    "        print(f\"Storing {len(nutrition_df)} nutrition entries in the database\")\n",
    "        nutrition_df.to_sql('nutrition', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    print(\"SQL database setup complete\")\n",
    "    return conn\n",
    "\n",
    "#############################\n",
    "# Vector Database Setup (ChromaDB)\n",
    "#############################\n",
    "def setup_vector_database(\n",
    "    vectorized_recipes_df: pd.DataFrame,\n",
    "    vectorized_interactions_df: Optional[pd.DataFrame] = None,\n",
    "    vector_db_path: str = VECTOR_DB_PATH\n",
    ") -> Tuple[Any, Any, Optional[Any]]:\n",
    "    \"\"\"\n",
    "    Set up ChromaDB using the precomputed dataframes for recipes and interactions.\n",
    "    \n",
    "    Arguments:\n",
    "        vectorized_recipes_df: DataFrame with your recipe data.\n",
    "        vectorized_interactions_df: DataFrame with your interaction data.\n",
    "        vector_db_path: Directory where ChromaDB will store its data.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing the ChromaDB client, the recipe collection, and \n",
    "        the interactions collection (if interactions_df is provided).\n",
    "    \"\"\"\n",
    "    os.makedirs(vector_db_path, exist_ok=True)\n",
    "    print(f\"Creating ChromaDB at {vector_db_path}\")\n",
    "    client = chromadb.PersistentClient(path=vector_db_path)\n",
    "    \n",
    "    #########################\n",
    "    # Load recipes into ChromaDB\n",
    "    #########################\n",
    "    print(f\"Setting up recipe collection with {len(vectorized_recipes_df)} recipes\")\n",
    "    recipe_collection = client.get_or_create_collection(name=\"recipes\")\n",
    "    \n",
    "    recipe_documents = []\n",
    "    recipe_metadatas = []\n",
    "    recipe_ids = []\n",
    "    \n",
    "    # Define which recipe columns to include as metadata\n",
    "    metadata_fields = ['name', 'minutes', 'contributor_id', 'submitted',\n",
    "                       'tags', 'nutrition', 'n_steps', 'cuisine_type',\n",
    "                       'n_ingredients', 'dietary_tags']\n",
    "    \n",
    "    for i, row in vectorized_recipes_df.iterrows():\n",
    "        # Determine a unique recipe ID. Use 'id' column if available.\n",
    "        recipe_id = row.get('id')\n",
    "        if recipe_id is None or (isinstance(recipe_id, float) and pd.isna(recipe_id)) or recipe_id == \"\":\n",
    "            recipe_id = str(i)\n",
    "        else:\n",
    "            recipe_id = str(recipe_id)\n",
    "        recipe_ids.append(recipe_id)\n",
    "        \n",
    "        # Build a document string by concatenating key text fields.\n",
    "        # You may adjust the fields below to better capture recipe information.\n",
    "        doc_text = \" \".join([\n",
    "            safe_convert(row.get('name', '')),\n",
    "            safe_convert(row.get('ingredients', '')),\n",
    "            safe_convert(row.get('steps', '')),\n",
    "            safe_convert(row.get('description', ''))\n",
    "        ])\n",
    "        recipe_documents.append(doc_text)\n",
    "        \n",
    "        # Build richer metadata from the chosen fields.\n",
    "        metadata = {key: safe_convert(row.get(key, \"\")) for key in metadata_fields}\n",
    "        metadata['recipe_id'] = recipe_id\n",
    "        recipe_metadatas.append(metadata)\n",
    "    \n",
    "    batch_size = 1000\n",
    "    for j in range(0, len(recipe_documents), batch_size):\n",
    "        end_idx = min(j + batch_size, len(recipe_documents))\n",
    "        recipe_collection.add(\n",
    "            documents=recipe_documents[j:end_idx],\n",
    "            metadatas=recipe_metadatas[j:end_idx],\n",
    "            ids=recipe_ids[j:end_idx]\n",
    "        )\n",
    "    \n",
    "    #########################\n",
    "    # Load interactions into ChromaDB (if provided)\n",
    "    #########################\n",
    "    interactions_collection = None\n",
    "    if vectorized_interactions_df is not None and not vectorized_interactions_df.empty:\n",
    "        print(f\"Setting up interactions collection with {len(vectorized_interactions_df)} interactions\")\n",
    "        interactions_collection = client.get_or_create_collection(name=\"interactions\")\n",
    "        \n",
    "        interaction_documents = []\n",
    "        interaction_metadatas = []\n",
    "        interaction_ids = []\n",
    "        \n",
    "        for i, row in vectorized_interactions_df.iterrows():\n",
    "            # Create a unique interaction ID from user_id, recipe_id, and index.\n",
    "            user_id = safe_convert(row.get('user_id', ''))\n",
    "            recipe_id = safe_convert(row.get('recipe_id', ''))\n",
    "            interaction_id = f\"{user_id}_{recipe_id}_{i}\"\n",
    "            interaction_ids.append(interaction_id)\n",
    "            \n",
    "            # Use the review text as the primary document.\n",
    "            review_text = safe_convert(row.get('review', ''))\n",
    "            if not review_text:\n",
    "                review_text = \"No review provided.\"\n",
    "            interaction_documents.append(review_text)\n",
    "            \n",
    "            # Build metadata for this interaction.\n",
    "            int_metadata = {\n",
    "                'interaction_id': interaction_id,\n",
    "                'user_id': user_id,\n",
    "                'recipe_id': recipe_id,\n",
    "                'date': safe_convert(row.get('date', '')),\n",
    "                'rating': safe_convert(row.get('rating', ''))\n",
    "            }\n",
    "            interaction_metadatas.append(int_metadata)\n",
    "        \n",
    "        for j in range(0, len(interaction_documents), batch_size):\n",
    "            end_idx = min(j + batch_size, len(interaction_documents))\n",
    "            interactions_collection.add(\n",
    "                documents=interaction_documents[j:end_idx],\n",
    "                metadatas=interaction_metadatas[j:end_idx],\n",
    "                ids=interaction_ids[j:end_idx]\n",
    "            )\n",
    "    \n",
    "    print(\"Vector database setup complete\")\n",
    "    return client, recipe_collection, interactions_collection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution Block (Database Setup Call - Placeholder)\n",
    "\n",
    "This block contains the main execution logic guarded by `if __name__ == \"__main__\":`. It includes commented-out calls to the database setup functions (`setup_sql_database`, `setup_vector_database`), assuming the DataFrames are loaded. It prints a confirmation message once the setup (or placeholder) is complete. *Note: In this notebook run, the actual database setup might have occurred implicitly or needs to be explicitly called if the DB files don't exist. Actually creating Vectorized DB needs a lot of time, so we just did it and uploaded in Kaggle, to be used for this project or future use for others.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:25:49.717888Z",
     "iopub.status.busy": "2025-04-18T13:25:49.717640Z",
     "iopub.status.idle": "2025-04-18T13:26:04.008097Z",
     "shell.execute_reply": "2025-04-18T13:26:04.007361Z",
     "shell.execute_reply.started": "2025-04-18T13:25:49.717870Z"
    }
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# Main Execution\n",
    "##############################\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume recipes_df and interactions_df have been loaded previously.\n",
    "    # For example:\n",
    "    # recipes_df = pd.read_pickle(\"your_recipes.pkl\")\n",
    "    # interactions_df = pd.read_pickle(\"your_interactions.pkl\")\n",
    "\n",
    "    # Set up the SQL database\n",
    "    sqlite_conn = setup_sql_database(\n",
    "        recipes_df=recipes_df,\n",
    "        interactions_df=interactions_df,\n",
    "        nutrition_df=nutrition_df,  # Modify if you have nutrition data.\n",
    "        db_path=DB_PATH\n",
    "    )\n",
    "    \n",
    "    # Set up ChromaDB with recipes and interactions\n",
    "    # chroma_client, recipe_collection, interactions_collection = setup_vector_database(\n",
    "    #     vectorized_recipes_df=recipes_df,\n",
    "    #     vectorized_interactions_df=interactions_df,\n",
    "    #     vector_db_path=VECTOR_DB_PATH\n",
    "    # )\n",
    "    \n",
    "    print(\"ChromaDB is ready for similarity search!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Database Inspection Functions\n",
    "\n",
    "Defines utility functions to inspect the created databases:\n",
    "1.  `view_schema_info`: Connects to ChromaDB, retrieves a sample of metadata from a specified collection, and prints the detected field names and their data types.\n",
    "2.  `collection_info`: Connects to ChromaDB, lists all collections, and prints basic information (name, estimated count, sample IDs, metadata keys) for each collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:26:04.009721Z",
     "iopub.status.busy": "2025-04-18T13:26:04.009432Z",
     "iopub.status.idle": "2025-04-18T13:26:04.021425Z",
     "shell.execute_reply": "2025-04-18T13:26:04.020676Z",
     "shell.execute_reply.started": "2025-04-18T13:26:04.009692Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path to SQL database\n",
    "DB_PATH = \"/kaggle/working/final/kitchen_db.sqlite\"\n",
    "# Path to Vectorized database\n",
    "VECTOR_DB_PATH = \"/kaggle/input/food-com-vectorized-with-chromadb/vector_db\"\n",
    "\n",
    "\n",
    "def view_schema_info(collection_name: str, db_path: str = VECTOR_DB_PATH):\n",
    "    \"\"\"\n",
    "    View schema information for a collection (metadata fields and their data types).\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to analyze\n",
    "        db_path: Path to the ChromaDB database\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    \n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"Collection '{collection_name}' not found. Error: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Get a sample of records to analyze schema\n",
    "    try:\n",
    "        results = collection.get(\n",
    "            limit=100,\n",
    "            include=['metadatas']\n",
    "        )\n",
    "        \n",
    "        if not results['metadatas']:\n",
    "            print(f\"Collection '{collection_name}' is empty or has no metadata.\")\n",
    "            return None\n",
    "        \n",
    "        # Analyze metadata fields\n",
    "        print(f\"\\n=== Schema for '{collection_name}' collection ===\\n\")\n",
    "        print(\"Metadata fields:\")\n",
    "        \n",
    "        # Collect all possible keys and their types\n",
    "        all_keys = set()\n",
    "        key_types = {}\n",
    "        key_examples = {}\n",
    "        \n",
    "        for metadata in results['metadatas']:\n",
    "            for key, value in metadata.items():\n",
    "                all_keys.add(key)\n",
    "                \n",
    "                # Track the data type\n",
    "                value_type = type(value).__name__\n",
    "                if key not in key_types:\n",
    "                    key_types[key] = set()\n",
    "                key_types[key].add(value_type)\n",
    "                \n",
    "                # Store an example value\n",
    "                if key not in key_examples and value:\n",
    "                    example = str(value)\n",
    "                    if len(example) > 50:\n",
    "                        example = example[:50] + \"...\"\n",
    "                    key_examples[key] = example\n",
    "        \n",
    "        # Display the schema information\n",
    "        for key in sorted(all_keys):\n",
    "            types_str = \", \".join(key_types[key])\n",
    "            example = key_examples.get(key, \"N/A\")\n",
    "            print(f\"  - {key}: {types_str}\")\n",
    "            print(f\"    Example: {example}\")\n",
    "        \n",
    "        return key_types\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting schema info: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def collection_info(db_path: str = VECTOR_DB_PATH):\n",
    "    \"\"\"\n",
    "    A simple function to display basic information about all collections.\n",
    "    More robust against API changes than the other functions.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the ChromaDB database\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    \n",
    "    try:\n",
    "        collection_names = client.list_collections()\n",
    "        print(f\"Found {len(collection_names)} collections in {db_path}:\")\n",
    "        \n",
    "        for name in collection_names:\n",
    "            print(f\"\\nCollection: {name}\")\n",
    "            \n",
    "            try:\n",
    "                collection = client.get_collection(name=str(name))\n",
    "                \n",
    "                # Try to get count\n",
    "                try:\n",
    "                    count = collection.count(where={})\n",
    "                    print(f\"  Records: {count}\")\n",
    "                except:\n",
    "                    print(\"  Count: Could not retrieve\")\n",
    "                \n",
    "                # Try to get the first few items\n",
    "                try:\n",
    "                    first_items = collection.get(limit=3, include=[\"metadatas\"])\n",
    "                    print(f\"  Sample IDs: {first_items['ids']}\")\n",
    "                    \n",
    "                    # Show first item metadata as example\n",
    "                    if first_items['metadatas'] and len(first_items['metadatas']) > 0:\n",
    "                        print(\"  Sample metadata keys:\", list(first_items['metadatas'][0].keys()))\n",
    "                except:\n",
    "                    print(\"  Sample: Could not retrieve\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error accessing collection: {str(e)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing collections: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List ChromaDB Collections\n",
    "\n",
    "Creates a ChromaDB client instance pointing to the defined vector database path and lists the names of all existing collections within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:26:04.022711Z",
     "iopub.status.busy": "2025-04-18T13:26:04.022426Z",
     "iopub.status.idle": "2025-04-18T13:26:04.425953Z",
     "shell.execute_reply": "2025-04-18T13:26:04.425168Z",
     "shell.execute_reply.started": "2025-04-18T13:26:04.022685Z"
    }
   },
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=VECTOR_DB_PATH)\n",
    "print(client.list_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display ChromaDB Collection Information\n",
    "\n",
    "Calls the `collection_info` utility function to print details about the collections found in the ChromaDB instance (name, count, sample IDs, metadata keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:26:04.427016Z",
     "iopub.status.busy": "2025-04-18T13:26:04.426764Z",
     "iopub.status.idle": "2025-04-18T13:26:36.386601Z",
     "shell.execute_reply": "2025-04-18T13:26:36.385743Z",
     "shell.execute_reply.started": "2025-04-18T13:26:04.426996Z"
    }
   },
   "outputs": [],
   "source": [
    "collection_info(VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 'recipes' Collection Schema\n",
    "\n",
    "Calls the `view_schema_info` utility function specifically for the 'recipes' collection in ChromaDB to display its metadata fields and inferred data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:26:36.391503Z",
     "iopub.status.busy": "2025-04-18T13:26:36.391247Z",
     "iopub.status.idle": "2025-04-18T13:26:41.147126Z",
     "shell.execute_reply": "2025-04-18T13:26:41.146457Z",
     "shell.execute_reply.started": "2025-04-18T13:26:36.391485Z"
    }
   },
   "outputs": [],
   "source": [
    "view_schema_info(\"recipes\", VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 'interactions' Collection Schema\n",
    "\n",
    "Calls the `view_schema_info` utility function specifically for the 'interactions' collection in ChromaDB to display its metadata fields and inferred data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:26:41.148155Z",
     "iopub.status.busy": "2025-04-18T13:26:41.147913Z",
     "iopub.status.idle": "2025-04-18T13:27:09.735533Z",
     "shell.execute_reply": "2025-04-18T13:27:09.734606Z",
     "shell.execute_reply.started": "2025-04-18T13:26:41.148137Z"
    }
   },
   "outputs": [],
   "source": [
    "view_schema_info(\"interactions\", VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Similarity Search Functions (ChromaDB)\n",
    "\n",
    "Defines functions to perform similarity searches using the ChromaDB vector database:\n",
    "1.  `gemini_recipe_similarity_search`: Queries the 'recipes' collection based on text similarity, allowing optional filtering by cuisine, dietary tags, and max cooking time. Returns formatted results including metadata and similarity scores.\n",
    "2.  `gemini_interaction_similarity_search`: Queries the 'interactions' collection based on text similarity (likely searching review text). Returns formatted results including metadata and similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:09.736646Z",
     "iopub.status.busy": "2025-04-18T13:27:09.736404Z",
     "iopub.status.idle": "2025-04-18T13:27:09.749355Z",
     "shell.execute_reply": "2025-04-18T13:27:09.748449Z",
     "shell.execute_reply.started": "2025-04-18T13:27:09.736626Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def gemini_recipe_similarity_search(query_text: str, n_results: int, cuisine: Optional[str] = None, dietary_tag: Optional[str] = None, max_minutes: Optional[int] = None) -> str:\n",
    "    \"\"\"\n",
    "    Searches for similar recipes based on a query, with optional filters and returns full metadata.\n",
    "\n",
    "    Args:\n",
    "        query_text: The text to search for in recipes.\n",
    "        n_results: The number of top similar recipes to return.\n",
    "        cuisine: (Optional) Filter by cuisine type (e.g., 'mexican', 'italian').\n",
    "        dietary_tag: (Optional) Filter by dietary tag (e.g., 'vegetarian', 'gluten-free').\n",
    "        max_minutes: (Optional) Filter recipes with a cooking time less than or equal to this value.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing the full metadata of the top similar recipes with similarity scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=VECTOR_DB_PATH)\n",
    "        recipe_collection = client.get_collection(name=\"recipes\")\n",
    "\n",
    "        where_clause = {}\n",
    "        if cuisine is not None:\n",
    "            where_clause[\"cuisine_type\"] = cuisine\n",
    "        if dietary_tag is not None:\n",
    "            where_clause[\"dietary_tags\"] = {\"$contains\": dietary_tag}\n",
    "        if max_minutes is not None:\n",
    "            where_clause[\"minutes\"] = {\"$lte\": str(max_minutes)} # Store as string in metadata\n",
    "\n",
    "        results = recipe_collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        if not results['ids'][0]:\n",
    "            return f\"No similar recipes found for the query: '{query_text}' with the specified criteria.\"\n",
    "\n",
    "        output = f\"Found {len(results['ids'][0])} similar recipes for query: '{query_text}'.\\n\"\n",
    "        output += \"-\" * 80 + \"\\n\"\n",
    "        for i, (doc_id, doc, metadata, distance) in enumerate(zip(\n",
    "            results['ids'][0],\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            similarity_score = (1 - distance) * 100\n",
    "            output += f\"\\n{i+1}. Recipe Name: {metadata.get('name', 'Unnamed')}\\n\"\n",
    "            output += f\"   Similarity: {similarity_score:.2f}%\\n\"\n",
    "            output += f\"   Recipe ID: {doc_id}\\n\"\n",
    "            for key, value in metadata.items():\n",
    "                output += f\"   {key.replace('_', ' ').title()}: {value}\\n\"\n",
    "            output += f\"   Ingredients: {doc}\\n\"  # Include the full document (ingredients/steps)\n",
    "            output += \"-\" * 80 + \"\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during recipe similarity search: {e}\"\n",
    "\n",
    "# Updated `gemini_interaction_similarity_search` Function:\n",
    "\n",
    "def gemini_interaction_similarity_search(query_text: str, n_results: int) -> str:\n",
    "    \"\"\"\n",
    "    Searches for similar user interactions (reviews) based on a query and returns full metadata.\n",
    "\n",
    "    Args:\n",
    "        query_text: The text to search for in user reviews.\n",
    "        n_results: The number of top similar interactions to return.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing the full metadata of the top similar interactions with similarity scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=VECTOR_DB_PATH)\n",
    "        interactions_collection = client.get_collection(name=\"interactions\")\n",
    "        results = interactions_collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        if not results['ids'][0]:\n",
    "            return f\"No similar reviews found for the query: '{query_text}'.\"\n",
    "\n",
    "        output = f\"Found {len(results['ids'][0])} similar reviews for query: '{query_text}'.\\n\"\n",
    "        output += \"-\" * 80 + \"\\n\"\n",
    "        for i, (doc_id, doc, metadata, distance) in enumerate(zip(\n",
    "            results['ids'][0],\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            similarity_score = (1 - distance) * 100\n",
    "            output += f\"\\n{i+1}. Review ID: {doc_id}\\n\"\n",
    "            output += f\"   Similarity: {similarity_score:.2f}%\\n\"\n",
    "            for key, value in metadata.items():\n",
    "                output += f\"   {key.replace('_', ' ').title()}: {value}\\n\"\n",
    "            output += f\"   Review Text: {doc}\\n\"  # Include the full document (review text)\n",
    "            output += \"-\" * 80 + \"\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    except ValueError:\n",
    "        return \"Interactions collection not found. Make sure you have interaction data loaded.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during interaction similarity search: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Recipe Similarity Search\n",
    "\n",
    "Executes a sample query (\"check for making an italian pizza\") against the `gemini_recipe_similarity_search` function and prints the formatted results for the top match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:09.750729Z",
     "iopub.status.busy": "2025-04-18T13:27:09.750446Z",
     "iopub.status.idle": "2025-04-18T13:27:18.882026Z",
     "shell.execute_reply": "2025-04-18T13:27:18.881119Z",
     "shell.execute_reply.started": "2025-04-18T13:27:09.750699Z"
    }
   },
   "outputs": [],
   "source": [
    "query_text = \"check for making an italian pizza \"\n",
    "result = gemini_recipe_similarity_search(query_text, n_results = 1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Interaction Similarity Search\n",
    "\n",
    "Executes a sample query (\"best italian pizza\") against the `gemini_interaction_similarity_search` function (searching reviews) and prints the formatted results for the top match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:18.883442Z",
     "iopub.status.busy": "2025-04-18T13:27:18.883072Z",
     "iopub.status.idle": "2025-04-18T13:27:32.273712Z",
     "shell.execute_reply": "2025-04-18T13:27:32.272799Z",
     "shell.execute_reply.started": "2025-04-18T13:27:18.883411Z"
    }
   },
   "outputs": [],
   "source": [
    "query_text = \"best italian pizza\"\n",
    "result = gemini_interaction_similarity_search(query_text, n_results = 1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Database and API Tool Functions for Agent\n",
    "\n",
    "Defines Python functions intended to be used as tools by the LangGraph agent:\n",
    "1.  `list_tables`: Lists tables in the SQLite DB.\n",
    "2.  `describe_table`: Describes columns of a specific SQL table.\n",
    "3.  `execute_query`: Executes a read-only SQL query.\n",
    "4.  `fetch_nutrition_from_openfoodfacts` (Placeholder/Alternative): Fetches nutrition from Open Food Facts API (includes retry logic). *Note: The active function used later is `fetch_nutrition_from_usda_fdc`.*\n",
    "5.  `get_recipe_by_id`: Retrieves full recipe details (including ingredients for nutrition lookup) from the SQL DB. Includes JSON parsing logic and calls the nutrition fetching function (`fetch_nutrition_from_usda_fdc`) for each ingredient with delays.\n",
    "6.  `get_ratings_and_reviews_by_recipe_id`: Fetches average rating and recent reviews from the SQL DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:41:21.514612Z",
     "iopub.status.busy": "2025-04-18T13:41:21.514183Z",
     "iopub.status.idle": "2025-04-18T13:41:21.550688Z",
     "shell.execute_reply": "2025-04-18T13:41:21.549272Z",
     "shell.execute_reply.started": "2025-04-18T13:41:21.514586Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assume DB_PATH is defined elsewhere\n",
    "# DB_PATH = \"your_database_path.db\"\n",
    "\n",
    "# --- Database Functions (Mostly unchanged, added try/finally and context managers) ---\n",
    "\n",
    "def list_tables() -> List[str]:\n",
    "    \"\"\"List all tables in the SQLite database using context managers.\"\"\"\n",
    "    tables = []\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            tables = [table[0] for table in cursor.fetchall()]\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error listing tables: {e}\")\n",
    "        # Depending on desired behavior, you might return [] or raise e\n",
    "    return tables\n",
    "\n",
    "def describe_table(table_name: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Describe the schema of a specified table using context managers.\"\"\"\n",
    "    schema = []\n",
    "    try:\n",
    "        # Basic validation/sanitization - prevent SQL injection in table names\n",
    "        if not table_name.isalnum() and '_' not in table_name:\n",
    "             print(f\"Warning: Invalid table name format '{table_name}'. Skipping.\")\n",
    "             return []\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            # Use parameterized query even for PRAGMA if possible, or ensure table_name is safe\n",
    "            # For PRAGMA table_info, direct insertion is common but requires validation ^\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            schema_raw = cursor.fetchall()\n",
    "            # Extract relevant columns (name, type) - indices 1 and 2\n",
    "            schema = [(col[1], col[2]) for col in schema_raw]\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error describing table '{table_name}': {e}\")\n",
    "    return schema\n",
    "\n",
    "\n",
    "def execute_query(sql: str) -> List[Tuple]:\n",
    "    \"\"\"Execute a potentially read-only SQL query and return the results using context managers.\"\"\"\n",
    "    results = []\n",
    "    # Basic check to prevent obviously harmful commands - enhance as needed\n",
    "    if not sql.strip().upper().startswith(\"SELECT\") and not sql.strip().upper().startswith(\"PRAGMA\"):\n",
    "         print(\"Warning: Only SELECT and PRAGMA queries are recommended via execute_query.\")\n",
    "         # return [(\"Error:\", \"Potentially unsafe query blocked.\")] # Or allow if you trust the source\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(sql)\n",
    "            results = cursor.fetchall()\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error executing SQL query: {e}\")\n",
    "        # Return error message structured like results for consistency\n",
    "        return [(\"Error executing SQL query:\", str(e))]\n",
    "    return results\n",
    "\n",
    "# --- Modified Open Food Facts Function with Retries ---\n",
    "\n",
    "def fetch_nutrition_from_openfoodfacts(ingredient_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Fetch nutrition data for an ingredient from Open Food Facts API.\n",
    "    Includes retry logic for rate limiting (429) and transient errors.\n",
    "    \"\"\"\n",
    "    #api_key = os.getenv('OPENFOODFACTS_API_KEY')\n",
    "    # You might still want a warning if the key isn't set, but OFF search often works without it.\n",
    "\n",
    "    search_url = \"https://world.openfoodfacts.org/cgi/search.pl\"\n",
    "    params = {\n",
    "        \"search_terms\": ingredient_name,\n",
    "        \"search_simple\": 1,\n",
    "        \"action\": \"process\",\n",
    "        \"json\": 1,\n",
    "        \"page_size\": 1 # We only need the top result\n",
    "    }\n",
    "    headers = {'User-Agent': 'CapstoneProject/1.0 (Language Model Integration)'} # Good practice\n",
    "\n",
    "    max_retries = 3 # Internal retry limit\n",
    "    base_timeout = 15 # Internal timeout\n",
    "    retry_delay = 1 # Initial delay in seconds for retries\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(search_url, params=params, headers=headers, timeout=base_timeout)\n",
    "            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            if data.get('products') and len(data['products']) > 0:\n",
    "                product = data['products'][0]\n",
    "                nutriments = product.get('nutriments', {})\n",
    "\n",
    "                nutrition_info = {\n",
    "                    \"food_normalized\": ingredient_name,\n",
    "                    \"source\": \"Open Food Facts\",\n",
    "                    \"product_name\": product.get('product_name', 'N/A'),\n",
    "                    \"calories_100g\": nutriments.get('energy-kcal_100g'),\n",
    "                    \"fat_100g\": nutriments.get('fat_100g'),\n",
    "                    \"saturated_fat_100g\": nutriments.get('saturated-fat_100g'),\n",
    "                    \"carbohydrates_100g\": nutriments.get('carbohydrates_100g'),\n",
    "                    \"sugars_100g\": nutriments.get('sugars_100g'),\n",
    "                    \"fiber_100g\": nutriments.get('fiber_100g'),\n",
    "                    \"proteins_100g\": nutriments.get('proteins_100g'),\n",
    "                    \"sodium_100g\": nutriments.get('sodium_100g'),\n",
    "                }\n",
    "                return {k: v for k, v in nutrition_info.items() if v is not None}\n",
    "            else:\n",
    "                # No product found, not an error, just unavailable\n",
    "                return {\"status\": \"unavailable\", \"reason\": f\"No product found for '{ingredient_name}' on Open Food Facts\"}\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # Specific handling for Rate Limiting (429)\n",
    "            if e.response.status_code == 429:\n",
    "                if attempt < max_retries - 1:\n",
    "                    # Exponential backoff with jitter\n",
    "                    wait_time = (retry_delay * (2 ** attempt)) + random.uniform(0, 1)\n",
    "                    print(f\"Rate limit hit for '{ingredient_name}'. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue # Retry the loop\n",
    "                else:\n",
    "                    print(f\"Rate limit hit for '{ingredient_name}'. Max retries exceeded.\")\n",
    "                    return {\"status\": \"unavailable\", \"reason\": f\"API rate limit exceeded after {max_retries} attempts: {e}\"}\n",
    "            # Handle other HTTP errors (e.g., 5xx server errors) potentially with retries too\n",
    "            elif e.response.status_code >= 500 and attempt < max_retries - 1:\n",
    "                 wait_time = (retry_delay * (2 ** attempt)) + random.uniform(0, 1)\n",
    "                 print(f\"Server error ({e.response.status_code}) for '{ingredient_name}'. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                 time.sleep(wait_time)\n",
    "                 continue # Retry the loop\n",
    "            else:\n",
    "                # For other client errors (4xx) or server errors after retries, report failure\n",
    "                print(f\"HTTP Error fetching nutrition for '{ingredient_name}': {e}\")\n",
    "                return {\"status\": \"unavailable\", \"reason\": f\"API request failed with HTTP error: {e}\"}\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle other connection/timeout errors\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (retry_delay * (2 ** attempt)) + random.uniform(0, 1)\n",
    "                print(f\"Request error for '{ingredient_name}': {e}. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "                continue # Retry the loop\n",
    "            else:\n",
    "                print(f\"Error fetching nutrition for '{ingredient_name}' after {max_retries} attempts: {e}\")\n",
    "                return {\"status\": \"unavailable\", \"reason\": f\"API request failed after retries: {e}\"}\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # If response is not valid JSON\n",
    "            print(f\"Error decoding JSON response for '{ingredient_name}'\")\n",
    "            # No retry for decoding error usually, indicates bad response content\n",
    "            return {\"status\": \"unavailable\", \"reason\": \"Invalid JSON response from API\"}\n",
    "\n",
    "    # Should not be reached if loop completes, but as a fallback:\n",
    "    return {\"status\": \"unavailable\", \"reason\": \"Max retries exceeded without success\"}\n",
    "\n",
    "\n",
    "# --- Modified Recipe Function with Small Delay ---\n",
    "\n",
    "def get_recipe_by_id(recipe_id: str) -> Optional[dict]:\n",
    "    \"\"\"Get a recipe by its ID, including live nutrition data (with delays & retries).\"\"\"\n",
    "    recipe = None\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            conn.row_factory = sqlite3.Row # Return rows that act like dictionaries\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            cursor.execute(\"SELECT * FROM recipes WHERE id = ?\", (recipe_id,))\n",
    "            recipe_data = cursor.fetchone()\n",
    "\n",
    "            if not recipe_data:\n",
    "                return None\n",
    "\n",
    "            # Convert Row object to a mutable dictionary\n",
    "            recipe = dict(recipe_data)\n",
    "\n",
    "            # --- Field Parsing Logic (Keep your existing logic, maybe add more logging) ---\n",
    "            for field in [\"steps\", \"ingredients\", \"nutrition\", \"tags\", \"dietary_tags\", \"normalized_ingredients\"]:\n",
    "                value = recipe.get(field)\n",
    "                if isinstance(value, str):\n",
    "                    try:\n",
    "                        recipe[field] = json.loads(value)\n",
    "                        # print(f\"Successfully parsed JSON for field '{field}' in recipe ID {recipe_id}.\") # Optional: Success log\n",
    "                    except json.JSONDecodeError:\n",
    "                        if field in [\"ingredients\", \"tags\", \"dietary_tags\", \"normalized_ingredients\"]:\n",
    "                            # Fallback: Split potentially space or comma-separated strings\n",
    "                            # Consider more robust splitting if needed (e.g., handle commas)\n",
    "                            potential_list = [item.strip() for item in value.replace(',', ' ').split() if item.strip()]\n",
    "                            if potential_list:\n",
    "                                recipe[field] = potential_list\n",
    "                                print(f\"Info: Field '{field}' in recipe ID {recipe_id} treated as separated string -> {recipe[field]}\")\n",
    "                            else:\n",
    "                                print(f\"Warning: Field '{field}' in recipe ID {recipe_id} was string but empty after split.\")\n",
    "                                recipe[field] = [] # Ensure it's an empty list\n",
    "                        elif field == \"steps\":\n",
    "                            print(f\"Warning: Could not parse JSON for field 'steps' in recipe ID {recipe_id}. Kept as string.\")\n",
    "                            # Keep as string is fine here\n",
    "                        else: # E.g., nutrition field if not JSON\n",
    "                            print(f\"Warning: Could not parse JSON for field '{field}' in recipe ID {recipe_id}. Value: {value[:100]}...\")\n",
    "                            # Decide how to handle - keep string, set to None, etc.\n",
    "                            pass\n",
    "                # Ensure expected list fields are indeed lists if they exist but aren't strings\n",
    "                elif field in [\"ingredients\", \"tags\", \"dietary_tags\", \"normalized_ingredients\"] and value is not None and not isinstance(value, list):\n",
    "                     print(f\"Warning: Field '{field}' in recipe ID {recipe_id} was type {type(value)}, expected list or string. Attempting conversion.\")\n",
    "                     try:\n",
    "                         recipe[field] = list(value) # Basic conversion attempt\n",
    "                     except TypeError:\n",
    "                         print(f\"Error: Could not convert field '{field}' to list for recipe ID {recipe_id}. Setting to empty list.\")\n",
    "                         recipe[field] = []\n",
    "\n",
    "\n",
    "            # --- Fetch nutrition for normalized ingredients ---\n",
    "            ingredient_nutrition_list = []\n",
    "            normalized_ingredients = recipe.get(\"normalized_ingredients\")\n",
    "\n",
    "            if isinstance(normalized_ingredients, list):\n",
    "                for i, ingredient in enumerate(normalized_ingredients):\n",
    "                    if isinstance(ingredient, str) and ingredient.strip():\n",
    "                        print(f\"Fetching nutrition for: '{ingredient}' (Item {i+1}/{len(normalized_ingredients)})\") # Log progress\n",
    "                        nutrition_data = fetch_nutrition_from_usda_fdc(ingredient)\n",
    "                        ingredient_nutrition_list.append(nutrition_data)\n",
    "                        # *** ADD A SMALL DELAY HERE ***\n",
    "                        time.sleep(random.uniform(0.5, 1.5)) # Wait 0.5 to 1.5 seconds before the next call\n",
    "                    elif not isinstance(ingredient, str):\n",
    "                         print(f\"Warning: Skipping non-string item in normalized_ingredients: {ingredient}\")\n",
    "                         ingredient_nutrition_list.append({\"status\": \"skipped\", \"reason\": f\"Invalid ingredient format: {type(ingredient)}\"})\n",
    "                    # else: skip empty strings silently\n",
    "            elif normalized_ingredients is not None:\n",
    "                print(f\"Error: 'normalized_ingredients' field in recipe ID {recipe_id} is not a list after processing. Type: {type(normalized_ingredients)}\")\n",
    "                ingredient_nutrition_list.append({\"status\": \"error\", \"reason\": \"normalized_ingredients field could not be processed into a list\"})\n",
    "\n",
    "            recipe['ingredient_nutrition'] = ingredient_nutrition_list\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error getting recipe ID {recipe_id}: {e}\")\n",
    "        return None # Or raise the error if preferred\n",
    "    except Exception as e: # Catch other potential errors\n",
    "        print(f\"An unexpected error occurred in get_recipe_by_id for {recipe_id}: {e}\")\n",
    "        # Return the partially processed recipe if available, or None\n",
    "        return recipe if recipe else None\n",
    "\n",
    "    return recipe\n",
    "\n",
    "\n",
    "# --- Ratings Function (Using context manager) ---\n",
    "def get_ratings_and_reviews_by_recipe_id(recipe_id: str, limit: int) -> Optional[dict]:\n",
    "    \"\"\"Get ratings and recent reviews for a recipe ID using context managers.\"\"\"\n",
    "    overall_rating = None\n",
    "    reviews_list = []\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            # Get overall rating\n",
    "            cursor.execute(\"SELECT AVG(rating) FROM interactions WHERE recipe_id = ?\", (recipe_id,))\n",
    "            overall_rating_result = cursor.fetchone()\n",
    "            # Ensure we handle None if no ratings exist before accessing index 0\n",
    "            if overall_rating_result and overall_rating_result[0] is not None:\n",
    "                 overall_rating = round(overall_rating_result[0], 2) # Round for cleaner display\n",
    "\n",
    "\n",
    "            # Get most recent reviews\n",
    "            cursor.execute(\n",
    "                \"SELECT date, rating, review FROM interactions WHERE recipe_id = ? AND review IS NOT NULL AND review != '' ORDER BY date DESC LIMIT ?\",\n",
    "                (recipe_id, limit),\n",
    "            )\n",
    "            recent_reviews = cursor.fetchall()\n",
    "            columns = [\"date\", \"rating\", \"review\"]\n",
    "            reviews_list = [dict(zip(columns, review)) for review in recent_reviews]\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error getting ratings/reviews for recipe ID {recipe_id}: {e}\")\n",
    "        # Return partial data or indicate error\n",
    "        return {\"overall_rating\": overall_rating, \"recent_reviews\": [], \"error\": str(e)}\n",
    "\n",
    "    return {\"overall_rating\": overall_rating, \"recent_reviews\": reviews_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Open Food Facts API Call (Commented Out)\n",
    "\n",
    "This cell contains a commented-out example call to the `fetch_nutrition_from_openfoodfacts` function.\n",
    "Site is down, we go by another approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:32.309424Z",
     "iopub.status.busy": "2025-04-18T13:27:32.309020Z",
     "iopub.status.idle": "2025-04-18T13:27:44.554265Z",
     "shell.execute_reply": "2025-04-18T13:27:44.552473Z",
     "shell.execute_reply.started": "2025-04-18T13:27:32.309402Z"
    }
   },
   "outputs": [],
   "source": [
    "fetch_nutrition_from_openfoodfacts(\"apple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define USDA FDC Nutrition Tool\n",
    "\n",
    "Defines the `fetch_nutrition_from_usda_fdc` function, decorated as a `@tool` for potential LangChain use (though used directly later). This function queries the USDA FoodData Central API for nutritional information of a single ingredient, using a provided API key. It includes mapping for relevant nutrients and robust retry logic for API errors or rate limits. Returns results as a JSON string. Includes a test call for \"raw apple\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:42:56.024548Z",
     "iopub.status.busy": "2025-04-18T13:42:56.024156Z",
     "iopub.status.idle": "2025-04-18T13:42:57.034145Z",
     "shell.execute_reply": "2025-04-18T13:42:57.033214Z",
     "shell.execute_reply.started": "2025-04-18T13:42:56.024523Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Consider getting the API key from an environment variable for security\n",
    "# Example: export USDA_API_KEY='USDA_API_KEY'\n",
    "# If using an environment variable:\n",
    "# USDA_API_KEY = os.environ.get(\"USDA_API_KEY\")\n",
    "# Or pass it directly as an argument to the function.\n",
    "\n",
    "# Mapping from FDC nutrient names (or IDs for more stability) to our desired keys.\n",
    "# Using names here for readability. Units are typically per 100g in FDC.\n",
    "# Note: FDC uses 'KCAL' for calories, 'G' for macros, 'MG' for sodium.\n",
    "FDC_NUTRIENT_MAP = {\n",
    "    # Nutrient Name in FDC API : Target Key\n",
    "    \"Energy\": \"calories_100g\", # Often unit KCAL\n",
    "    \"Total lipid (fat)\": \"fat_100g\", # Often unit G\n",
    "    \"Fatty acids, total saturated\": \"saturated_fat_100g\", # Often unit G\n",
    "    \"Carbohydrate, by difference\": \"carbohydrates_100g\", # Often unit G\n",
    "    \"Sugars, total including NLEA\": \"sugars_100g\", # Often unit G\n",
    "    \"Fiber, total dietary\": \"fiber_100g\", # Often unit G\n",
    "    \"Protein\": \"proteins_100g\", # Often unit G\n",
    "    \"Sodium, Na\": \"sodium_100g\", # Often unit MG\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# @tool # Uncomment this if you are using it as a LangChain/LangGraph tool\n",
    "def fetch_nutrition_from_usda_fdc(ingredient_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches nutrition data (per 100g) for a single ingredient from USDA FoodData Central API.\n",
    "    Requires a USDA FDC API key. Includes robust retry logic.\n",
    "    Returns nutrition data as a JSON string or an error/unavailable status.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG TOOL CALL: fetch_nutrition_from_usda_fdc(ingredient_name='{ingredient_name}')\")\n",
    "    api_key = UserSecretsClient().get_secret(\"USDA_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"ERROR: USDA FDC API key is required.\")\n",
    "        return json.dumps({\"error\": \"USDA FDC API key was not provided.\"})\n",
    "\n",
    "    search_url = \"https://api.nal.usda.gov/fdc/v1/foods/search\"\n",
    "    params = {\n",
    "        \"query\": ingredient_name,\n",
    "        \"api_key\": api_key,\n",
    "        \"pageSize\": 1, # Get the top hit\n",
    "        \"dataType\": \"SR Legacy,Foundation\", # Prioritize standard reference / foundation foods for generic ingredients\n",
    "        # Consider adding \"Branded\" if you need specific packaged products\n",
    "    }\n",
    "    headers = {'User-Agent': 'KitchenAssistantLangGraph/1.0 (Language: Python)'} # Good practice\n",
    "\n",
    "    max_retries = 3\n",
    "    base_timeout = 15\n",
    "    retry_delay = 1 # Initial delay\n",
    "\n",
    "    for attempt in range(max_retries): # 0, 1, 2 (3 attempts total)\n",
    "        try:\n",
    "            response = requests.get(search_url, params=params, headers=headers, timeout=base_timeout)\n",
    "            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "            data = response.json()\n",
    "\n",
    "            if data.get('foods') and len(data['foods']) > 0:\n",
    "                food_item = data['foods'][0]\n",
    "                fdc_nutrients = food_item.get('foodNutrients', [])\n",
    "\n",
    "                # Extract desired fields using the mapping\n",
    "                nutrition_info = {\n",
    "                    \"food_normalized\": ingredient_name,\n",
    "                    \"source\": \"USDA FoodData Central\",\n",
    "                    \"product_name\": food_item.get('description', ingredient_name), # Use FDC description\n",
    "                    \"fdc_id\": food_item.get('fdcId'), # Useful identifier\n",
    "                    \"data_type\": food_item.get('dataType'), # e.g., SR Legacy, Branded\n",
    "                }\n",
    "\n",
    "                # Iterate through nutrients reported by FDC for this food\n",
    "                found_nutrients = {}\n",
    "                for nutrient in fdc_nutrients:\n",
    "                    nutrient_name = nutrient.get('nutrientName')\n",
    "                    nutrient_unit = nutrient.get('unitName')\n",
    "                    nutrient_value = nutrient.get('value')\n",
    "\n",
    "                    # Check if this nutrient is one we want to map\n",
    "                    target_key = FDC_NUTRIENT_MAP.get(nutrient_name)\n",
    "                    if target_key:\n",
    "                         # Optional: Check if the unit matches expected (e.g., KCAL for Energy)\n",
    "                         # expected_unit = FDC_EXPECTED_UNITS.get(target_key)\n",
    "                         # if nutrient_unit == expected_unit:\n",
    "                        found_nutrients[target_key] = nutrient_value\n",
    "                         # else:\n",
    "                         #    print(f\"Warning: Unit mismatch for {target_key}: Expected {expected_unit}, Got {nutrient_unit}\")\n",
    "\n",
    "\n",
    "                # Add found nutrients to the main dictionary\n",
    "                nutrition_info.update(found_nutrients)\n",
    "\n",
    "                # Filter out None values BEFORE checking core nutrients\n",
    "                # (Note: FDC usually returns 0 rather than null/None for zero values)\n",
    "                filtered_nutrition = {k: v for k, v in nutrition_info.items() if v is not None}\n",
    "\n",
    "                # Check if at least one core nutrient is present and numeric\n",
    "                core_nutrients = [\"calories_100g\", \"fat_100g\", \"proteins_100g\", \"carbohydrates_100g\"]\n",
    "                has_core_data = False\n",
    "                for core_key in core_nutrients:\n",
    "                    if core_key in filtered_nutrition:\n",
    "                        try:\n",
    "                            # Check if it's actually a number (or can be converted)\n",
    "                            float(filtered_nutrition[core_key])\n",
    "                            has_core_data = True\n",
    "                            break # Found at least one valid core nutrient\n",
    "                        except (ValueError, TypeError):\n",
    "                            continue # Skip if not numeric\n",
    "\n",
    "                if not has_core_data:\n",
    "                    print(f\"--> No core numeric nutrition data found for '{ingredient_name}' in product '{filtered_nutrition.get('product_name', 'N/A')}' (FDC ID: {filtered_nutrition.get('fdc_id')})\")\n",
    "                    return json.dumps({\"status\": \"unavailable\", \"reason\": f\"No detailed numeric core nutrition data found for '{ingredient_name}'\"})\n",
    "\n",
    "                # Success: return JSON string\n",
    "                print(f\"--> Successfully found nutrition data for '{ingredient_name}' via USDA FDC\")\n",
    "                return json.dumps(filtered_nutrition, indent=2)\n",
    "            else:\n",
    "                # No food found for the query\n",
    "                print(f\"--> No product found for '{ingredient_name}' via USDA FDC\")\n",
    "                # Try again with Branded data type? Or just report unavailable.\n",
    "                # Let's report unavailable for now.\n",
    "                return json.dumps({\"status\": \"unavailable\", \"reason\": f\"No product found for '{ingredient_name}' on USDA FDC\"})\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # Specific handling for FDC API key errors (403 Forbidden often indicates bad key)\n",
    "            if e.response.status_code == 403:\n",
    "                 print(f\"HTTP Error 403 (Forbidden) for '{ingredient_name}'. Check your USDA FDC API key.\")\n",
    "                 return json.dumps({\"status\": \"error\", \"reason\": f\"API request failed with HTTP 403 (Forbidden). Check API Key.\"})\n",
    "            elif e.response.status_code == 429 and attempt < max_retries - 1:\n",
    "                wait_time = (retry_delay * (2 ** attempt)) + random.uniform(0, 1)\n",
    "                print(f\"Rate limit hit for '{ingredient_name}'. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            elif e.response.status_code >= 500 and attempt < max_retries - 1:\n",
    "                wait_time = (retry_delay * (2 ** attempt)) + random.uniform(0, 1)\n",
    "                print(f\"Server error ({e.response.status_code}) for '{ingredient_name}'. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"HTTP Error fetching nutrition for '{ingredient_name}': {e}\")\n",
    "                return json.dumps({\"status\": \"unavailable\", \"reason\": f\"API request failed with HTTP error: {e}\"})\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (retry_delay * (2 ** attempt)) + random.uniform(0, 1)\n",
    "                print(f\"Request error for '{ingredient_name}': {e}. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error fetching nutrition for '{ingredient_name}' after {max_retries} attempts: {e}\")\n",
    "                return json.dumps({\"status\": \"unavailable\", \"reason\": f\"API request failed after retries: {e}\"})\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON response for '{ingredient_name}'\")\n",
    "            return json.dumps({\"status\": \"unavailable\", \"reason\": \"Invalid JSON response from API\"})\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"ERROR in fetch_nutrition_from_usda_fdc: {e}\")\n",
    "             import traceback\n",
    "             traceback.print_exc()\n",
    "             return json.dumps({\"error\": f\"Unexpected error fetching nutrition for {ingredient_name}: {e}\"})\n",
    "\n",
    "    # If loop finishes after retries without success\n",
    "    print(f\"Max retries ({max_retries}) exceeded for API request for '{ingredient_name}'\")\n",
    "    return json.dumps({\"status\": \"unavailable\", \"reason\": f\"Max retries ({max_retries}) exceeded for API request for '{ingredient_name}'\"})\n",
    "\n",
    "ingredient = \"raw apple\"\n",
    "result_json = fetch_nutrition_from_usda_fdc(ingredient)\n",
    "print(\"\\n--- Result for 'raw apple' ---\")\n",
    "print(result_json)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `get_recipe_by_id` Tool\n",
    "\n",
    "Calls the `get_recipe_by_id` function for a specific recipe ID (71373) to test its functionality, including the internal calls to the nutrition API for each ingredient. Displays the final formatted response using Markdown. *Note: This call might be slow due to sequential API calls with delays.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:43:14.154532Z",
     "iopub.status.busy": "2025-04-18T13:43:14.154057Z",
     "iopub.status.idle": "2025-04-18T13:43:47.279790Z",
     "shell.execute_reply": "2025-04-18T13:43:47.279006Z",
     "shell.execute_reply.started": "2025-04-18T13:43:14.154505Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Tool Definitions and Instructions ---\n",
    "\n",
    "# IMPORTANT: The tool definition MUST match the actual Python function signature.\n",
    "# Since we added retry/timeout logic *inside* fetch_nutrition_from_usda_fdc,\n",
    "# the LLM doesn't need to pass `max_retries` or `timeout`.\n",
    "db_tools = [\n",
    "    list_tables,\n",
    "    describe_table,\n",
    "    execute_query,\n",
    "    get_ratings_and_reviews_by_recipe_id,\n",
    "    get_recipe_by_id,\n",
    "    fetch_nutrition_from_usda_fdc # Matches the Python function signature now\n",
    "]\n",
    "\n",
    "# Revised Instruction Prompt\n",
    "# âœ… Finalized Instruction Prompt (Gemini-Compatible)\n",
    "instruction = \"\"\"You are a helpful assistant for a Kitchen AI chatbot that can interact with a SQL database and external APIs.\n",
    "\n",
    "You have access to special tools that let you:\n",
    "- Retrieve recipe details and instructions\n",
    "- Fetch live nutrition data\n",
    "- Query the database structure\n",
    "- Retrieve user ratings and reviews\n",
    "\n",
    "---\n",
    "\n",
    "Available Tools:\n",
    "\n",
    "- **list_tables()**  \n",
    "  Lists all tables in the database.\n",
    "\n",
    "- **describe_table(table_name: str)**  \n",
    "  Describes the schema (column names and types) of a specified table.\n",
    "\n",
    "- **execute_query(sql: str)**  \n",
    "  Executes a **read-only SQL query** (e.g., SELECT or PRAGMA). Use this after understanding the schema.\n",
    "\n",
    "- **get_recipe_by_id(recipe_id: str)**  \n",
    "  Returns full details for a specific recipe, including ingredients, steps, tags, and **live nutrition lookup for normalized ingredients** using Open Food Facts.\n",
    "\n",
    "- **get_ratings_and_reviews_by_recipe_id(recipe_id: str, limit: int)**  \n",
    "  Returns the average rating and the 'limit' most recent reviews for a recipe. If the user doesnâ€™t provide a limit, use 3.\n",
    "\n",
    "- **fetch_nutrition_from_usda_fdc(ingredient_name: str)**  \n",
    "  Fetches live nutrition data (per 100g) for a *single* ingredient using the USDA FoodData Central API.  \n",
    "   Only use this tool if the user asks **specifically for nutrition of a single ingredient**, outside of a full recipe request. \n",
    "\n",
    "---\n",
    "\n",
    "How to respond:\n",
    "\n",
    "1. Identify the user's intent.\n",
    "2. Use the **most relevant tool(s)** to answer.\n",
    "3. Use `get_recipe_by_id` for full recipe lookups (do not use `fetch_nutrition_from_usda_fdc` in this case).\n",
    "4. Use `fetch_nutrition_from_usda_fdc` only for *standalone* ingredient nutrition requests.\n",
    "5. Use `list_tables`, `describe_table`, and `execute_query` for SQL exploration or advanced queries.\n",
    "6. Present results clearly using Markdown formatting.\n",
    "7. If nutrition is unavailable or skipped, include a clear note explaining why.\n",
    "\n",
    "---\n",
    "\n",
    "Examples:\n",
    "\n",
    "- **User:** \"Tell me everything about recipe 71373 including its nutrition and reviews.\"  \n",
    "  **LLM Calls:**  \n",
    "    1. `get_recipe_by_id(recipe_id=\"71373\")`  \n",
    "    2. `get_ratings_and_reviews_by_recipe_id(recipe_id=\"71373\", limit=3)`\n",
    "\n",
    "- **User:** \"How many calories are in butter?\"  \n",
    "  **LLM Call:**  \n",
    "    `fetch_nutrition_from_usda_fdc(ingredient_name=\"butter\")`\n",
    "\n",
    "- **User:** \"What tables exist in this database?\"  \n",
    "  **LLM Call:**  \n",
    "    `list_tables()`\n",
    "\n",
    "---\n",
    "\n",
    "Be smart, helpful, and accurate. Don't guess dataâ€”use the tools!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") # Make sure this is set\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Start a chat with automatic function calling enabled.\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=instruction,\n",
    "        tools=db_tools,\n",
    "    ),\n",
    ")\n",
    "    # tool_config={\"function_calling_config\": \"AUTO\"} # Enable auto function calling if using v1beta API or specific library versions that support it this way\n",
    "\n",
    "\n",
    "# Start chat (using the structure for the library version you have)\n",
    "# Example using a simple generate_content call structure\n",
    "# chat = client.start_chat(enable_automatic_function_calling=True) # Or similar depending on exact library version\n",
    "\n",
    "# --- Simplified User Prompt to LLM ---\n",
    "\n",
    "# Instead of telling it HOW to call the functions step-by-step,\n",
    "# just ask for the information. The revised instructions guide the LLM.\n",
    "user_query = \"\"\"\n",
    "Can you give me the full details for recipe ID 71373?\n",
    "I'd like to see its description, ingredients, steps, the nutritional info for the ingredients,\n",
    "its overall rating, and the 3 most recent reviews.\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(user_query) # Or client.generate_content(user_query)\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:53.977336Z",
     "iopub.status.busy": "2025-04-18T13:27:53.977001Z",
     "iopub.status.idle": "2025-04-18T13:27:53.982074Z",
     "shell.execute_reply": "2025-04-18T13:27:53.981280Z",
     "shell.execute_reply.started": "2025-04-18T13:27:53.977288Z"
    }
   },
   "outputs": [],
   "source": [
    "results = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grounding with Google Search\n",
    "\n",
    "Demonstrates using the Gemini model (`gemini-2.0-flash`) with Google Search grounding enabled (`tools=[types.Tool(google_search=types.GoogleSearch())]`). It asks a general question (\"What's a good substitute for egg yolks...\") and displays the model's grounded response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:53.983508Z",
     "iopub.status.busy": "2025-04-18T13:27:53.983004Z",
     "iopub.status.idle": "2025-04-18T13:27:58.481802Z",
     "shell.execute_reply": "2025-04-18T13:27:58.480919Z",
     "shell.execute_reply.started": "2025-04-18T13:27:53.983487Z"
    }
   },
   "outputs": [],
   "source": [
    "# And now re-run the same query with search grounding enabled.\n",
    "config_with_search = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    ")\n",
    "\n",
    "def query_with_grounding():\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=\"What's a good substitute for eggs in Country White Bread or Dinner Rolls?\",\n",
    "        config=config_with_search,\n",
    "    )\n",
    "    return response.candidates[0]\n",
    "\n",
    "\n",
    "rc = query_with_grounding()\n",
    "Markdown(rc.content.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:58.483383Z",
     "iopub.status.busy": "2025-04-18T13:27:58.482762Z",
     "iopub.status.idle": "2025-04-18T13:27:58.487203Z",
     "shell.execute_reply": "2025-04-18T13:27:58.486049Z",
     "shell.execute_reply.started": "2025-04-18T13:27:58.483358Z"
    }
   },
   "outputs": [],
   "source": [
    "# resp = chat.send_message(\"gluten free or vegeterian recipe but quick and easy\")\n",
    "# display(Markdown(resp.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Audio Input & Command Recognition with User Preferences\n",
    "\n",
    "This notebook implements the second step of our Interactive Recipe & Kitchen Management Assistant capstone project for the Google Gen AI Intensive Course. We'll create a voice interface that allows users to interact with our recipe assistant through spoken commands, recognize different types of user requests, and maintain user preferences.\n",
    "\n",
    "\n",
    "\n",
    "This step focuses on the **Audio understanding** Gen AI capability, which enables our assistant to:\n",
    "- Process voice commands using Google Cloud Speech-to-Text\n",
    "- Interpret user intent from natural language using Gemini Flash model\n",
    "- Store and retrieve user preferences for personalized experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your test prompt\n",
    "\n",
    "In this step, you will test that your API key is set up correctly by making a request.\n",
    "\n",
    "The Python SDK uses a [`Client` object](https://googleapis.github.io/python-genai/genai.html#genai.client.Client) to make requests to the API. The client lets you control which back-end to use (between the Gemini API and Vertex AI) and handles authentication (the API key).\n",
    "\n",
    "The `gemini-2.0-flash` model has been selected here.\n",
    "\n",
    "**Note**: If you see a `TransportError` on this step, you may need to **ðŸ” Factory reset** the notebook one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:58.488357Z",
     "iopub.status.busy": "2025-04-18T13:27:58.488084Z",
     "iopub.status.idle": "2025-04-18T13:27:59.188119Z",
     "shell.execute_reply": "2025-04-18T13:27:59.187111Z",
     "shell.execute_reply.started": "2025-04-18T13:27:58.488326Z"
    }
   },
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Hi, This is a test message! How are you?\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Speech-to-Text API Setup\n",
    "\n",
    "To use Google Cloud Speech-to-Text, we need to set up authentication and configure the client. In a production environment, this would involve creating a service account and downloading the credentials. For demonstration in a Kaggle/local environment, we'll simulate the API response.\n",
    "\n",
    "> Note: In a real implementation, you would:\n",
    "> 1. Create a Google Cloud project\n",
    "> 2. Enable the Speech-to-Text API\n",
    "> 3. Create a service account with appropriate permissions\n",
    "> 4. Download the credentials JSON file\n",
    "> 5. Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-to-Text Conversion\n",
    "\n",
    "Let's implement a real speech-to-text function using Google Cloud Speech-to-Text API. This will allow us to convert voice commands from audio files into text for processing. Unfortunately, the google STT needs a lot of parameters for configuration, for credential, and the auth section is headache! , I decided to move forward with lovely whisper-1 ðŸ˜‚ðŸ˜‚ðŸ˜‚, \n",
    "## Sorry Google!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Audio Transcription Function (OpenAI & Google)\n",
    "\n",
    "Defines the `transcribe_audio` function capable of using either OpenAI's Whisper model or Google Cloud Speech-to-Text. It handles API key/credential management, file reading, encoding detection (basic), and API calls. Includes logic for using temporary files for Google JSON credentials if provided as a string. Returns the transcribed text or an error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:59.189894Z",
     "iopub.status.busy": "2025-04-18T13:27:59.189202Z",
     "iopub.status.idle": "2025-04-18T13:27:59.202057Z",
     "shell.execute_reply": "2025-04-18T13:27:59.201220Z",
     "shell.execute_reply.started": "2025-04-18T13:27:59.189864Z"
    }
   },
   "outputs": [],
   "source": [
    "def transcribe_audio(service=\"openai\", file_path=None, language=\"en\", api_key=None, credentials_path=None, credentials_json=None):\n",
    "    \"\"\"\n",
    "    Transcribe audio using either OpenAI or Google Cloud Speech-to-Text API.\n",
    "    \n",
    "    Args:\n",
    "        service (str): The service to use for transcription ('openai' or 'google')\n",
    "        file_path (str): Path to the audio file to transcribe\n",
    "        language (str): Language code (e.g., 'en' for OpenAI, 'en-US' for Google)\n",
    "        api_key (str): OpenAI API key (required for OpenAI service)\n",
    "        credentials_path (str): Path to Google credentials JSON file (optional for Google service)\n",
    "        credentials_json (str): JSON string of Google credentials (optional for Google service)\n",
    "        \n",
    "    Returns:\n",
    "        str: Transcription text or error message\n",
    "    \"\"\"\n",
    "    \n",
    "    if not file_path:\n",
    "        return \"Error: No file path provided\"\n",
    "        \n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"Error: File not found at {file_path}\"\n",
    "    \n",
    "    try:\n",
    "        if service.lower() == \"openai\":\n",
    "            if not api_key:\n",
    "                return \"Error: OpenAI API key required\"\n",
    "                \n",
    "            client = OpenAI(api_key=api_key)\n",
    "            \n",
    "            with open(file_path, \"rb\") as audio_file:\n",
    "                transcription = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\", \n",
    "                    file=audio_file,\n",
    "                    language=language\n",
    "                )\n",
    "            \n",
    "            return transcription.text\n",
    "            \n",
    "        elif service.lower() == \"google\":\n",
    "            temp_cred_file = None\n",
    "            \n",
    "            # Handle Google authentication\n",
    "            if not credentials_path and not credentials_json:\n",
    "                return \"Error: Either credentials_path or credentials_json required for Google service\"\n",
    "            \n",
    "            # If credentials_json is provided, write to a temporary file\n",
    "            if credentials_json:\n",
    "                try:\n",
    "                    # Create a temporary file for credentials\n",
    "                    temp_cred_file = tempfile.NamedTemporaryFile(delete=False, suffix='.json')\n",
    "                    temp_cred_path = temp_cred_file.name\n",
    "                    temp_cred_file.write(credentials_json.encode('utf-8'))\n",
    "                    temp_cred_file.close()\n",
    "                    \n",
    "                    # Set environment variable to the temporary file\n",
    "                    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = temp_cred_path\n",
    "                except Exception as e:\n",
    "                    if temp_cred_file and os.path.exists(temp_cred_file.name):\n",
    "                        os.unlink(temp_cred_file.name)\n",
    "                    return f\"Error creating temporary credentials file: {str(e)}\"\n",
    "            else:\n",
    "                # Use provided credentials_path\n",
    "                os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "            \n",
    "            try:\n",
    "                # Initialize the Speech client\n",
    "                client = speech.SpeechClient()\n",
    "                \n",
    "                # Read the audio file\n",
    "                with io.open(file_path, \"rb\") as audio_file:\n",
    "                    content = audio_file.read()\n",
    "                \n",
    "                # Determine encoding based on file extension\n",
    "                file_ext = os.path.splitext(file_path)[1].lower()\n",
    "                if file_ext == \".ogg\":\n",
    "                    encoding = speech.RecognitionConfig.AudioEncoding.OGG_OPUS\n",
    "                elif file_ext == \".wav\":\n",
    "                    encoding = speech.RecognitionConfig.AudioEncoding.LINEAR16\n",
    "                else:\n",
    "                    return f\"Error: Unsupported file format: {file_ext}\"\n",
    "                \n",
    "                # Configure the speech recognition\n",
    "                audio = speech.RecognitionAudio(content=content)\n",
    "                config = speech.RecognitionConfig(\n",
    "                    encoding=encoding,\n",
    "                    sample_rate_hertz=48000,  # May need adjustment based on actual audio file\n",
    "                    language_code=language if language else \"en-US\",\n",
    "                )\n",
    "                \n",
    "                # Perform the transcription\n",
    "                response = client.recognize(config=config, audio=audio)\n",
    "                \n",
    "                # Extract the transcription\n",
    "                if response.results:\n",
    "                    return response.results[0].alternatives[0].transcript\n",
    "                else:\n",
    "                    return \"No transcription results found\"\n",
    "                    \n",
    "            finally:\n",
    "                # Clean up temp file if it was created\n",
    "                if temp_cred_file and os.path.exists(temp_cred_file.name):\n",
    "                    os.unlink(temp_cred_file.name)\n",
    "        \n",
    "        else:\n",
    "            return f\"Error: Unknown service '{service}'. Use 'openai' or 'google'\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Clean up temp file if exception occurs\n",
    "        if service.lower() == \"google\" and temp_cred_file and os.path.exists(temp_cred_file.name):\n",
    "            os.unlink(temp_cred_file.name)\n",
    "        return f\"Error during transcription: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Audio Transcription\n",
    "\n",
    "Calls the `transcribe_audio` function to transcribe a sample audio file (`Nariman_1.ogg`) using the Google Cloud Speech-to-Text service, providing the necessary credentials path. Prints the resulting transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:27:59.203575Z",
     "iopub.status.busy": "2025-04-18T13:27:59.203220Z",
     "iopub.status.idle": "2025-04-18T13:28:00.811141Z",
     "shell.execute_reply": "2025-04-18T13:28:00.809568Z",
     "shell.execute_reply.started": "2025-04-18T13:27:59.203548Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#OPENAI_API_KEY (openai) or  SecretValueJson (google)\n",
    "transcribe_audio(service=\"google\", file_path=\"/kaggle/input/voices-of-commands-genai-capstone-2025/Nariman_1.ogg\", language=\"en\",  credentials_json=SecretValueJson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemented the Kitchen Management Assistant interface. The assistant provides a modern, interactive interface for users to either:\n",
    "\n",
    "\n",
    "### Text Input\n",
    "1. Click on the \"Text Input\" tab\n",
    "2. Type your kitchen-related request in the text area\n",
    "3. Click the \"Submit\" button\n",
    "4. The system will process your text request\n",
    "\n",
    "### Voice Selection\n",
    "1. Click on the \"Voice Selection\" tab\n",
    "2. Select a voice recording from the dropdown list\n",
    "3. Click the \"Transcribe Voice\" button\n",
    "4. The system will transcribe the audio and process the request\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Voice File Data Structure\n",
    "\n",
    "Defines a Python dictionary (`voices`) containing metadata about available voice recording files, including their paths, language, description, speaker, and processing status. This structure is intended for use with the UI simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:28:00.813451Z",
     "iopub.status.busy": "2025-04-18T13:28:00.812431Z",
     "iopub.status.idle": "2025-04-18T13:28:00.818098Z",
     "shell.execute_reply": "2025-04-18T13:28:00.817203Z",
     "shell.execute_reply.started": "2025-04-18T13:28:00.813423Z"
    }
   },
   "outputs": [],
   "source": [
    "voices = {\n",
    "  \"version\": \"1.0\",\n",
    "  \"voices\": [\n",
    "    {\n",
    "      \"file_path\": \"/kaggle/input/voice-tests/test.ogg\",\n",
    "      \"language\": \"en\",\n",
    "      \"description\": \"Voice instruction for baking a pizza\",\n",
    "      \"speaker_id\": \"nariman\",\n",
    "      \"is_processed\": False\n",
    "    },\n",
    "    {\n",
    "      \"file_path\": \"voices/test.wav\",\n",
    "      \"language\": \"en\",\n",
    "      \"description\": \"Test voice recording for the system\",\n",
    "      \"speaker_id\": \"user2\",\n",
    "      \"is_processed\": False\n",
    "    },\n",
    "  \n",
    "    \n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an agent with LangGraph (Final Step! yaaay!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install LangGraph and Dependencies\n",
    "\n",
    "Uninstalls potentially conflicting packages from the base environment and installs specific versions of `langgraph`, `langchain-google-genai`, and `langgraph-prebuilt` required for the agent implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Graphviz\n",
    "\n",
    "Installs the `graphviz` library, which is required by LangGraph for visualizing the agent's graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:28:00.819660Z",
     "iopub.status.busy": "2025-04-18T13:28:00.819331Z",
     "iopub.status.idle": "2025-04-18T13:28:00.844477Z",
     "shell.execute_reply": "2025-04-18T13:28:00.843420Z",
     "shell.execute_reply.started": "2025-04-18T13:28:00.819632Z"
    }
   },
   "outputs": [],
   "source": [
    "### **Step 1: State Schema (`KitchenState`) (Revised)**\n",
    "# Step 1: State Schema (`KitchenState`)** (Revised)\n",
    "\n",
    "\n",
    "class KitchenState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the conversation and actions within the\n",
    "    Interactive Recipe & Kitchen Management Assistant agent.\n",
    "    Follows a standard LangGraph pattern where tool results are processed\n",
    "    from ToolMessages by the parser node or dedicated processing nodes.\n",
    "\n",
    "    Attributes:\n",
    "        messages: The history of messages (human, AI, tool). Tool results appear here.\n",
    "        user_input: The latest raw input from the user (text or transcribed audio).\n",
    "        intent: The determined intent (used for routing).\n",
    "        selected_recipe_id: The ID of the recipe currently in context.\n",
    "        customization_request: Details of a requested recipe customization (passed to tool).\n",
    "        nutrition_query: The ingredient name for a specific nutrition lookup.\n",
    "        grounding_query: A specific question requiring web search grounding.\n",
    "\n",
    "        # Raw Tool Outputs (potentially stored before processing nodes)\n",
    "        current_recipe_details: Parsed details of the recipe after get_recipe_by_id runs.\n",
    "        recipe_reviews: Raw ratings and reviews after get_ratings_and_reviews runs.\n",
    "        ingredient_nutrition_list: Temp storage for results from fetch_nutrition_from_usda_fdc.\n",
    "        live_recipe_details: Raw result from fetch_live_recipe_data tool. # ---> ADDED <---\n",
    "        # customization_tool_output: Raw output from customize_recipe tool (optional, if needed before node)\n",
    "\n",
    "        # Processed Data (output from custom nodes, ready for formatting)\n",
    "        nutritional_info: Aggregated/final nutritional info prepared for display.\n",
    "        processed_review_data: Aggregated/formatted review data with sentiment. # ---> ADDED <---\n",
    "        customization_results: Processed customization suggestions. # ---> ADDED <---\n",
    "        grounding_results_formatted: Formatted web search results prepared for display.\n",
    "\n",
    "        # User Context\n",
    "        user_ingredients: A list of ingredients the user currently has available.\n",
    "        dietary_preferences: The user's specified dietary restrictions or preferences.\n",
    "\n",
    "        # Control Flow\n",
    "        needs_clarification: Flag indicating if the agent requires more information.\n",
    "        finished: Flag indicating if the conversation/task is complete.\n",
    "        last_assistant_response: The last text response generated by the assistant for UI display.\n",
    "        audio_file_path: Path to the audio file if input was voice.\n",
    "    \"\"\"\n",
    "    # Conversation history\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "    # User input & context\n",
    "    user_input: Optional[str]\n",
    "    audio_file_path: Optional[str]\n",
    "    intent: Optional[str] # e.g., 'get_details', 'get_reviews', 'customize', 'aggregate_nutrition', 'fetch_live', 'general_chat', 'exit'\n",
    "    selected_recipe_id: Optional[str]\n",
    "    customization_request: Optional[str] # Stored temporarily by parser to pass to tool\n",
    "    nutrition_query: Optional[str]\n",
    "    grounding_query: Optional[str]\n",
    "\n",
    "    # Raw Tool Results / Intermediate Data\n",
    "    current_recipe_details: Optional[Dict[str, Any]] # From get_recipe_by_id\n",
    "    recipe_reviews: Optional[Dict[str, Any]] # From get_ratings_and_reviews\n",
    "    ingredient_nutrition_list: Optional[List[Dict[str, Any]]] # Temp storage for nutrition tool messages\n",
    "    live_recipe_details: Optional[Dict[str, Any]] # ---> ADDED: From fetch_live_recipe_data <---\n",
    "\n",
    "    # Processed Data (Ready for Formatter)\n",
    "    nutritional_info: Optional[Dict[str, Any]] # From AggregateNutritionNode\n",
    "    processed_review_data: Optional[Dict[str, Any]] # ---> ADDED: From ReviewDashboardNode <---\n",
    "    customization_results: Optional[Dict[str, Any]] # ---> ADDED: From ProcessCustomizationNode <---\n",
    "    grounding_results_formatted: Optional[str] # From potential future grounding node\n",
    "\n",
    "    # User Context (Could be loaded/persisted)\n",
    "    user_ingredients: List[str]\n",
    "    dietary_preferences: List[str]\n",
    "\n",
    "    # Control Flow / Output\n",
    "    needs_clarification: bool\n",
    "    finished: bool\n",
    "    last_assistant_response: Optional[str] # Final formatted response\n",
    "\n",
    "# Initialize the state (optional, for testing/default values)\n",
    "initial_state: KitchenState = {\n",
    "    \"messages\": [],\n",
    "    \"user_input\": None,\n",
    "    \"audio_file_path\": None,\n",
    "    \"intent\": None,\n",
    "    \"selected_recipe_id\": None,\n",
    "    \"customization_request\": None,\n",
    "    \"nutrition_query\": None,\n",
    "    \"grounding_query\": None,\n",
    "    \"current_recipe_details\": None,\n",
    "    \"recipe_reviews\": None,\n",
    "    \"ingredient_nutrition_list\": None,\n",
    "    \"live_recipe_details\": None, # ---> ADDED <---\n",
    "    \"nutritional_info\": None,\n",
    "    \"processed_review_data\": None, # ---> ADDED <---\n",
    "    \"customization_results\": None, # ---> ADDED <---\n",
    "    \"grounding_results_formatted\": None,\n",
    "    \"user_ingredients\": [],\n",
    "    \"dietary_preferences\": [],\n",
    "    \"needs_clarification\": False,\n",
    "    \"finished\": False,\n",
    "    \"last_assistant_response\": None,\n",
    "}\n",
    "\n",
    "print(\"âœ… LangGraph Step 1: State Schema Defined (Revised)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Step 2: Define System Instructions & Core Nodes\n",
    "\n",
    "Defines constants for response headers and the main system instruction prompt (`KITCHEN_ASSISTANT_SYSINT`) for the LLM. This prompt guides the agent's behavior, tool usage (including the new `fetch_live_recipe_data` tool and its specific use case), context management, and output formatting using dashboards. It initializes the `ChatGoogleGenerativeAI` model (`llm`) with grounding enabled. It also defines helper functions (`format_recipe_dashboard`, `format_review_dashboard`) and the `response_formatter_node` responsible for creating the final user-facing output based on the agent's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:58:26.902270Z",
     "iopub.status.busy": "2025-04-18T14:58:26.901430Z",
     "iopub.status.idle": "2025-04-18T14:58:26.953314Z",
     "shell.execute_reply": "2025-04-18T14:58:26.952348Z",
     "shell.execute_reply.started": "2025-04-18T14:58:26.902242Z"
    }
   },
   "outputs": [],
   "source": [
    "# LangGraph Step 2: System Instructions & Base LLM Initialization (Revised Prompt for Live Data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Constants ---\n",
    "NUTRITION_RESPONSE_HEADER = \"Here's the approximate average nutrition per 100g for ingredients in\"\n",
    "RECIPE_DASHBOARD_HEADER = \"ðŸ“Š Recipe Dashboard for\"\n",
    "REVIEW_DASHBOARD_HEADER = \"â­ Reviews Dashboard for\"\n",
    "CUSTOMIZATION_HEADER = \"ðŸ› ï¸ Recipe Customization Suggestions for\"\n",
    "\n",
    "# --- System Instructions (Revised for Liveâ€‘Data Rule) ---\n",
    "KITCHEN_ASSISTANT_SYSINT = (\n",
    "    \"system\",\n",
    "    \"\"\"You are a helpful, friendly, and knowledgeable Interactive Recipe & Kitchen Management Assistant.\n",
    "Your goal is to understand the user's request, use the available tools effectively, process the results, manage conversation context, and provide a clear, concise, and helpful response, often including informative dashboards.\n",
    "\n",
    "**Core Principles:**\n",
    "- **Be Conversational:** Engage naturally, ask clarifying questions when needed.\n",
    "- **Maintain Context:** Remember the `selected_recipe_id` and `current_recipe_details` from previous turns unless the user starts a new search or explicitly asks about a different recipe.\n",
    "- **Use Tools Appropriately:** Choose the best tool for the job based on the user's request and the tool descriptions. Only call tools listed below.\n",
    "- **Handle Errors Gracefully:** If a tool fails or returns an error, inform the user politely and suggest alternatives. Do not expose raw error messages.\n",
    "- **Summarize & Visualize Tool Results:** When you receive `ToolMessage` results, process their content (parse JSON if needed), update your understanding, and generate a userâ€‘facing summary or answer. **Crucially, use the specialized dashboard formats (Recipe, Review, Nutrition, Customization) when presenting relevant information.** Donâ€™t just repeat the raw tool output.\n",
    "- **Greeting Rule:** On the very first user turn (no conversation history), reply with *one short paragraph* outlining your capabilities, then immediately ask how you can help.\n",
    "\n",
    "**Capabilities & Tool Usage Guide:**\n",
    "\n",
    "- **Recipe Discovery (`gemini_recipe_similarity_search`):**\n",
    "    â€“ Use when the user asks for recipe ideas.  \n",
    "    â€“ Extract keywords, cuisine, dietary needs, max cooking time. Ask for clarification if vague.  \n",
    "    â€“ **Arguments:** `query_text` (required), `n_results` (required, defaultÂ 5), `cuisine` (optional), `dietary_tag` (optional), `max_minutes` (optional).  \n",
    "    â€“ **Action:** Call the tool. Summarize the results clearly (name, time, ID). Ask if they want details.\n",
    "\n",
    "- **Recipe Details (`get_recipe_by_id`):**\n",
    "    â€“ Use when the user asks for details about a *specific* recipe ID **or** refers to a recipe from a list you just provided (e.g., â€œtell me about the second oneâ€). **This is the default way to get recipe details.**  \n",
    "    â€“ **RequiresÂ `recipe_id`.**  \n",
    "    â€“ **Context Rule:** If referring to an item from your *immediately preceding* list, pull its `recipe_id` from history; if you canâ€™t find it, **ASK** for it.  \n",
    "    â€“ If a `selected_recipe_id` is already established, use that unless the user asks about a different one.  \n",
    "    â€“ **Action:** Call the tool with the determined `recipe_id`. The `ResponseFormatterNode` will then generate the **Recipe Dashboard** based on `current_recipe_details`. Your job is just to call the tool.\n",
    "\n",
    "- **Live Recipe Data (`fetch_live_recipe_data`):**\n",
    "    â€“ Use **ONLY IF** the user explicitly asks for *latest*, *live*, or *most upâ€‘toâ€‘date* info (e.g., â€œget the latest ingredients for recipeÂ 123â€). **DO NOT** use for normal detail requests.  \n",
    "    â€“ **RequiresÂ `recipe_id`.**  \n",
    "    â€“ **Action:** Call the tool. The `ResponseFormatterNode` will prefer `live_recipe_details` (statusÂ `live_success`) and fall back to `current_recipe_details` if the fetch fails.  \n",
    "    â€“ **If the user did *not* say â€œliveâ€ or â€œlatestâ€, do *not* mention live data at all.**\n",
    "\n",
    "- **Ratings & Reviews (`get_ratings_and_reviews_by_recipe_id`):**\n",
    "    â€“ Use when the user asks for reviews/ratings for a *specific* recipe.  \n",
    "    â€“ **RequiresÂ `recipe_id`** (use context or ask) and **`limit`Â (defaultÂ 5).**  \n",
    "    â€“ **Action:** Call the tool with `limit=5`. `ReviewDashboardNode` processes `recipe_reviews`, then `ResponseFormatterNode` shows the **Review Dashboard**.\n",
    "\n",
    "- **Ingredient Nutrition (`fetch_nutrition_from_usda_fdc`):**\n",
    "    â€“ Use *only* for nutrition of a *single, specific ingredient*.  \n",
    "    â€“ **RequiresÂ `ingredient_name`.**  \n",
    "    â€“ **Action:** Call the tool and present key facts (perÂ 100Â g).\n",
    "\n",
    "- **Recipe Nutrition Analysis (Multiâ€‘Step Flow):**\n",
    "    â€“ Use when the user wants nutrition info for the *current* recipe. Trigger phrases: â€œRun the nutrition analysisâ€¦â€, â€œGet nutrition informationâ€¦â€.  \n",
    "    â€“ **DO NOT** call `get_recipe_by_id` if `current_recipe_details` already exist.  \n",
    "    â€“ **If `current_recipe_details` are NOT available:** first call `get_recipe_by_id` for `selected_recipe_id` **and set `suppress_recipe_dashboard=True`, then continue** with the steps below in the same run.  \n",
    "    â€“ **StepÂ 1Â â€“Â Ensure Details.** Confirm `current_recipe_details` are present.  \n",
    "    â€“ **StepÂ 2Â â€“Â Identify Ingredients.** Extract `normalized_ingredients`.  \n",
    "    â€“ **StepÂ 3Â â€“Â Generate Tool Calls.** Create *separate* calls to `fetch_nutrition_from_usda_fdc` for *each* ingredient.  \n",
    "    â€“ **StepÂ 4Â â€“Â Wait for Aggregation.** `AggregateNutritionNode` processes the results.  \n",
    "    â€“ **StepÂ 5Â â€“Â Present Results.** `ResponseFormatterNode` generates the **Nutrition Summary** (using `NUTRITION_RESPONSE_HEADER`); `VisualizeNutritionNode` draws the chart.\n",
    "\n",
    "- **Recipe Customization (`customize_recipe`):**\n",
    "    â€“ **USE THIS TOOL** when the user asks to *modify* the current recipe (make it vegan, substitute ingredients, reduce fat, etc.).  \n",
    "    â€“ **RequiresÂ `recipe_id`** (context or ask) and **`request`** (e.g., â€œmake it lowâ€‘fatâ€).  \n",
    "    â€“ **Action:** Call `customize_recipe`. Pass `recipe_id`, `request`, and optionally `recipe_details_json` if available. `ProcessCustomizationNode` handles output; `ResponseFormatterNode` shows suggestions under **Customization Header**. **Never refuse without trying the tool.**\n",
    "\n",
    "- **Grounding / General Questions (builtâ€‘in search):**\n",
    "    â€“ Use for cooking questions, techniques, or definitions *not* tied to a recipe.  \n",
    "    â€“ **Action:** Answer directly using internal knowledge and builtâ€‘in search. (You do **not** have a `google_search` tool; if you searched, say â€œBased on a quick searchâ€¦â€.)\n",
    "\n",
    "**Conversation Flow & Output Formatting:**\n",
    "1. Analyze the latest human message and state; infer `recipe_id` if needed.  \n",
    "2. Determine intent and required parameters. **Check explicitly for LIVE/LATEST requests.**  \n",
    "3. If a tool is needed, create `tool_calls` with proper contextâ€”recipeÂ ID, `request`, `limit=5`, etc. **Call `fetch_live_recipe_data` only if explicitly asked.**  \n",
    "4. If no tool is required, answer directly.  \n",
    "5. Ask for clarification if necessary.  \n",
    "6. **Receiving Tool Results:** Subsequent nodes (ToolExecutor, AggregateNutrition, ReviewDashboard, ProcessCustomization) will process them.  \n",
    "7. **Formatting Responses (handled by `ResponseFormatterNode`):**\n",
    "    - **Recipe Details:** Use `RECIPE_DASHBOARD_HEADER`; prefer `live_recipe_details` (statusÂ `live_success`) else `current_recipe_details`.  \n",
    "    - **Reviews:** Use `REVIEW_DASHBOARD_HEADER` and `processed_review_data`.  \n",
    "    - **Nutrition:** Use `NUTRITION_RESPONSE_HEADER` and `nutritional_info`.  \n",
    "    - **Customization:** Use `CUSTOMIZATION_HEADER` and `customization_results`.  \n",
    "    - **General Chat / Grounding:** Output your generated text.  \n",
    "    - If `suppress_recipe_dashboard` is True *and* intent is â€œnutritionâ€, omit the recipe dashboard until nutrition results are ready.  \n",
    "8. If the user says goodbye, set `intent='exit'` and reply politely.  \n",
    "9. Use Markdown for lists, bold text, and dashboard elements.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- LLM Initialization (Assuming this was correct in the original file) ---\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", # Consider gemini-1.5-flash or pro\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    generation_config=types.GenerateContentConfig(\n",
    "        tools=[types.Tool(google_search=types.GoogleSearch())]\n",
    "    )\n",
    "    # safety_settings=[...]\n",
    ")\n",
    "\n",
    "# --- Helper function for Recipe Dashboard (Emoji/Text Style) ---\n",
    "def format_recipe_dashboard(details: Dict[str, Any]) -> str:\n",
    "    # ... (Existing code from Step 2, ensure '\\n' is used for line breaks) ...\n",
    "    if not details:\n",
    "        return \"No recipe details available to display.\"\n",
    "\n",
    "    # --- Check for the raw JSON structure seen in Kaggle output ---\n",
    "    # This suggests maybe 'details' sometimes contains the raw JSON string instead of a parsed dict\n",
    "    if isinstance(details, str):\n",
    "        try:\n",
    "            details = json.loads(details)\n",
    "            if not isinstance(details, dict):\n",
    "                 return f\"Error: Could not parse recipe details JSON string into a dictionary. Content: {details[:100]}...\"\n",
    "        except json.JSONDecodeError:\n",
    "             return f\"Error: Could not parse recipe details JSON string. Content: {details[:100]}...\"\n",
    "\n",
    "    if not isinstance(details, dict):\n",
    "         return f\"Error: Recipe details are not in the expected dictionary format. Type: {type(details)}\"\n",
    "    # --- End Check ---\n",
    "\n",
    "    name = details.get(\"name\", \"N/A\")\n",
    "    recipe_id = details.get(\"id\", \"N/A\")\n",
    "    # ---> SAFE ACCESS with get and provide default <---\n",
    "    description = details.get(\"description\", \"No description available.\")\n",
    "    minutes = details.get(\"minutes\")\n",
    "    n_ingredients = details.get(\"n_ingredients\", len(details.get(\"ingredients\", [])))\n",
    "    n_steps = details.get(\"n_steps\", len(details.get(\"steps\", [])))\n",
    "    ingredients = details.get(\"ingredients\", [])\n",
    "    steps_data = details.get(\"steps\") # Can be list or string based on revised tool\n",
    "    source = details.get(\"source\", \"internal database\") # Check if live data was used\n",
    "\n",
    "    # Infer difficulty (simple example)\n",
    "    difficulty = \"Medium\"\n",
    "    difficulty_emoji = \"ðŸŒ¶ï¸ðŸŒ¶ï¸\"\n",
    "    if minutes is not None:\n",
    "        try: # Add try-except for potential non-numeric minutes\n",
    "            minutes_int = int(minutes)\n",
    "            n_steps_int = int(n_steps) if n_steps is not None else 0\n",
    "            if minutes_int <= 30 and n_steps_int <= 5:\n",
    "                difficulty = \"Easy\"\n",
    "                difficulty_emoji = \"ðŸŒ¶ï¸\"\n",
    "            elif minutes_int > 90 or n_steps_int > 10:\n",
    "                difficulty = \"Hard\"\n",
    "                difficulty_emoji = \"ðŸŒ¶ï¸ðŸŒ¶ï¸ðŸŒ¶ï¸\"\n",
    "        except (ValueError, TypeError):\n",
    "             minutes = \"N/A\" # Mark as N/A if conversion fails\n",
    "\n",
    "    time_emoji = \"â±ï¸\"\n",
    "    ingredients_emoji = \"ðŸ¥•\"\n",
    "    steps_emoji = \"ðŸ”¢\"\n",
    "\n",
    "    # ---> Make description formatting safer <---\n",
    "    formatted_description = f\"_{description}_\" if description else \"_No description available._\"\n",
    "\n",
    "    dashboard_lines = [\n",
    "        f\"{RECIPE_DASHBOARD_HEADER} **{name}** (ID: {recipe_id})\",\n",
    "        f\"*Source: {source}*\",\n",
    "        f\"{formatted_description}\\n\", # Ensure newline after description\n",
    "        \"---\",\n",
    "        f\"| Metric          | Value                     |\",\n",
    "        f\"|-----------------|---------------------------|\",\n",
    "        f\"| {difficulty_emoji} Difficulty    | {difficulty}                |\",\n",
    "        # ---> Ensure minutes are displayed safely <---\n",
    "        f\"| {time_emoji} Total Time    | {minutes if minutes is not None else 'N/A'} minutes         |\",\n",
    "        f\"| {ingredients_emoji} Ingredients | {n_ingredients if n_ingredients is not None else 'N/A'} count             |\",\n",
    "        f\"| {steps_emoji} Steps         | {n_steps if n_steps is not None else 'N/A'} count               |\",\n",
    "        \"---\\n\", # Ensure newline\n",
    "        \"**Ingredients:**\",\n",
    "    ]\n",
    "\n",
    "    # ---> Handle list or string ingredients safely <---\n",
    "    if isinstance(ingredients, list) and ingredients:\n",
    "        dashboard_lines.append(\"\\n\".join([f\"- {ing}\" for ing in ingredients]))\n",
    "    elif isinstance(ingredients, str) and ingredients.strip():\n",
    "         # Attempt to display string ingredients somewhat nicely\n",
    "         dashboard_lines.append(f\"- {ingredients.strip()}\") # Treat as one item if string\n",
    "    else:\n",
    "        dashboard_lines.append(\"- N/A\")\n",
    "\n",
    "    dashboard_lines.append(\"\\n**Steps:**\") # Ensure newline\n",
    "\n",
    "    # ---> Handle list or string steps safely <---\n",
    "    if isinstance(steps_data, list) and steps_data:\n",
    "        dashboard_lines.extend([f\"{i+1}. {step}\" for i, step in enumerate(steps_data)])\n",
    "    elif isinstance(steps_data, str) and steps_data.strip():\n",
    "        # Split string steps by common delimiters if it looks like a list-as-string\n",
    "        # Or just display the raw string\n",
    "        # Simple approach: Display raw string\n",
    "        dashboard_lines.append(steps_data.strip())\n",
    "    else:\n",
    "        dashboard_lines.append(\"- N/A\")\n",
    "\n",
    "    return \"\\n\".join(dashboard_lines)\n",
    "\n",
    "# --- Helper function for Review Dashboard (Emoji/Text Style) ---\n",
    "# In cell 3d5997b6-b301-4af4-a8ec-d6b5116a8934\n",
    "\n",
    "# --- Helper function for Review Dashboard (Emoji/Text Style) ---\n",
    "def format_review_dashboard(review_data: Dict[str, Any]) -> str:\n",
    "    # ... (Existing code from Step 2, ensure '\\n' is used for line breaks) ...\n",
    "    if not review_data:\n",
    "        return \"No review data available to display.\"\n",
    "\n",
    "    # --- Check for the raw JSON structure seen in Kaggle output ---\n",
    "    if isinstance(review_data, str):\n",
    "        try:\n",
    "            review_data = json.loads(review_data)\n",
    "            if not isinstance(review_data, dict):\n",
    "                 return f\"Error: Could not parse review data JSON string into a dictionary. Content: {review_data[:100]}...\"\n",
    "        except json.JSONDecodeError:\n",
    "             return f\"Error: Could not parse review data JSON string. Content: {review_data[:100]}...\"\n",
    "\n",
    "    if not isinstance(review_data, dict):\n",
    "         return f\"Error: Review data is not in the expected dictionary format. Type: {type(review_data)}\"\n",
    "    # --- End Check ---\n",
    "\n",
    "    name = review_data.get(\"recipe_name\", \"the recipe\")\n",
    "    recipe_id = review_data.get(\"recipe_id\", \"N/A\")\n",
    "    overall_rating = review_data.get(\"overall_rating\")\n",
    "    rating_counts = review_data.get(\"rating_counts\", {}) # e.g., {5: 10, 4: 5, ...}\n",
    "    sentiment_scores = review_data.get(\"sentiment_scores\", {}) # e.g., {'positive': 3, 'negative': 1, 'neutral': 1}\n",
    "    reviews_to_display = review_data.get(\"reviews_for_display\", []) # List of dicts\n",
    "\n",
    "    dashboard_lines = [\n",
    "        f\"{REVIEW_DASHBOARD_HEADER} **{name}** (ID: {recipe_id})\",\n",
    "        f\"**Overall Rating:** {'â­' * int(round(overall_rating)) if overall_rating is not None else 'N/A'} ({overall_rating:.1f}/5.0)\" if overall_rating is not None else \"**Overall Rating:** N/A\",\n",
    "        \"---\",\n",
    "        \"**Rating Breakdown:**\"\n",
    "    ]\n",
    "    if rating_counts and isinstance(rating_counts, dict): # Ensure it's a dict\n",
    "        total_ratings = sum(rating_counts.values())\n",
    "        for i in range(5, 0, -1):\n",
    "            count = rating_counts.get(i, 0)\n",
    "            percent = (count / total_ratings * 100) if total_ratings > 0 else 0\n",
    "            stars = 'â­' * i\n",
    "            dashboard_lines.append(f\"- {stars} : {count} ratings ({percent:.0f}%)\")\n",
    "    else:\n",
    "        dashboard_lines.append(\"- No rating breakdown available.\")\n",
    "\n",
    "    dashboard_lines.append(\"\\n**Sentiment:**\")\n",
    "    if sentiment_scores and isinstance(sentiment_scores, dict): # Ensure it's a dict\n",
    "        pos = sentiment_scores.get('positive', 0)\n",
    "        neg = sentiment_scores.get('negative', 0)\n",
    "        neu = sentiment_scores.get('neutral', 0)\n",
    "        total_sent = pos + neg + neu\n",
    "        pos_pct = (pos / total_sent * 100) if total_sent > 0 else 0\n",
    "        neg_pct = (neg / total_sent * 100) if total_sent > 0 else 0\n",
    "        sentiment_meter = \"\"\n",
    "        if pos_pct > 60: sentiment_meter = \"ðŸ˜Š Mostly Positive\"\n",
    "        elif neg_pct > 40: sentiment_meter = \"ðŸ˜Ÿ Mostly Negative\"\n",
    "        else: sentiment_meter = \"ðŸ˜ Mixed/Neutral\"\n",
    "        dashboard_lines.append(f\"- {sentiment_meter} (Pos: {pos}, Neg: {neg}, Neu: {neu})\")\n",
    "    else:\n",
    "        dashboard_lines.append(\"- Sentiment analysis not available.\")\n",
    "\n",
    "    dashboard_lines.append(\"\\n**Recent Reviews:**\")\n",
    "    if reviews_to_display and isinstance(reviews_to_display, list): # Ensure it's a list\n",
    "        for review in reviews_to_display:\n",
    "             # ---> Safe access for review details <---\n",
    "             rating = review.get('rating', 0)\n",
    "             rating_stars = 'â­' * int(rating) if isinstance(rating, (int, float)) else ''\n",
    "             sentiment_emoji = {'positive': 'ðŸ˜Š', 'negative': 'ðŸ˜Ÿ', 'neutral': 'ðŸ˜'}.get(review.get('sentiment'), '')\n",
    "             date_str = review.get('date', 'N/A')\n",
    "             review_text = review.get(\"review\", \"...\")\n",
    "             original_length = len(review_text)\n",
    "             display_text = review_text[:200] # Keep truncation\n",
    "             ellipsis = \"...\" if original_length > 200 else \"\"\n",
    "\n",
    "             dashboard_lines.append(f\"\\n- {rating_stars} {sentiment_emoji} *({date_str})*\")\n",
    "             dashboard_lines.append(f'> \"{display_text}{ellipsis}\"')\n",
    "    else:\n",
    "        dashboard_lines.append(\"- No reviews found.\")\n",
    "\n",
    "    return \"\\n\".join(dashboard_lines)\n",
    "\n",
    "\n",
    "\n",
    "# --- Core Nodes (Only showing revised response_formatter_node) ---\n",
    "\n",
    "# input_parser_node was revised in Step 3.5 section above\n",
    "# human_input_node remains the same\n",
    "\n",
    "# --- Core Nodes (response_formatter_node) ---\n",
    "def response_formatter_node(state: KitchenState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Formats the final response for the user. Prioritizes dashboards (Recipe, Review, Customization)\n",
    "    or aggregated nutrition if available, otherwise uses the last AI message content or a default.\n",
    "    Adds the final formatted response as an AIMessage to history.\n",
    "    \"\"\"\n",
    "    print(\"---NODE: ResponseFormatterNode---\")\n",
    "    formatted_response = \"Okay, let me know how else I can help!\" # Default fallback\n",
    "    final_intent_for_history = state.get(\"intent\", \"general_chat\") # Capture intent before reset\n",
    "    final_message_obj = None # To store the final AIMessage object\n",
    "\n",
    "    # --- Data Prioritization and Formatting ---\n",
    "    # 1. Customization Results\n",
    "    if state.get(\"customization_results\") and state.get(\"intent\") == \"customization_processed\":\n",
    "         print(\"Formatting customization suggestions.\")\n",
    "         results = state[\"customization_results\"]\n",
    "         recipe_name = \"the recipe\"\n",
    "         recipe_id = state.get(\"selected_recipe_id\")\n",
    "         if state.get(\"current_recipe_details\"):\n",
    "             recipe_name = state[\"current_recipe_details\"].get(\"name\", f\"recipe {recipe_id}\" if recipe_id else \"the recipe\")\n",
    "         elif recipe_id:\n",
    "             recipe_name = f\"recipe {recipe_id}\"\n",
    "\n",
    "         header = f\"{CUSTOMIZATION_HEADER} **{recipe_name}** (ID: {recipe_id})\"\n",
    "         message = results.get(\"message\", \"Could not process customization request.\")\n",
    "         formatted_response = f\"{header}\\n\\n{message}\" # Use \\n\n",
    "         final_intent_for_history = \"customization_presented\"\n",
    "\n",
    "    # 2. Review Dashboard\n",
    "    elif state.get(\"processed_review_data\") and state.get(\"intent\") == \"reviews_processed\":\n",
    "        print(\"Formatting review dashboard.\")\n",
    "        review_data = state[\"processed_review_data\"]\n",
    "        # Add recipe name if available\n",
    "        if \"recipe_name\" not in review_data and state.get(\"current_recipe_details\"):\n",
    "            review_data[\"recipe_name\"] = state[\"current_recipe_details\"].get(\"name\", f\"recipe {state.get('selected_recipe_id')}\")\n",
    "        elif \"recipe_name\" not in review_data and state.get('selected_recipe_id'):\n",
    "             review_data[\"recipe_name\"] = f\"recipe {state.get('selected_recipe_id')}\"\n",
    "\n",
    "        formatted_response = format_review_dashboard(review_data)\n",
    "        final_intent_for_history = \"review_dashboard_presented\"\n",
    "\n",
    "    # 3. Recipe Dashboard (Prioritize live data if valid)\n",
    "    elif (state.get(\"live_recipe_details\") or state.get(\"current_recipe_details\")) and \\\n",
    "         state.get(\"intent\") in [\"recipe_details_fetched\", \"live_data_fetched\", \"live_data_requested\"]:\n",
    "\n",
    "        recipe_details_to_format = None\n",
    "        live_details = state.get(\"live_recipe_details\")\n",
    "        # Check if live details exist, came from the tool successfully, and have data\n",
    "        if live_details and isinstance(live_details, dict) and live_details.get('status') in ['live_success'] and live_details.get('data'):\n",
    "             print(\"Using live recipe data for dashboard.\")\n",
    "             recipe_details_to_format = live_details['data']\n",
    "             # Ensure key fields are present in live data, fallback to internal if missing\n",
    "             recipe_details_to_format['source'] = recipe_details_to_format.get(\"source\", \"food.com (live)\")\n",
    "             recipe_details_to_format['id'] = state.get(\"selected_recipe_id\")\n",
    "             internal_details = state.get(\"current_recipe_details\", {})\n",
    "             # Safe gets for fallbacks\n",
    "             recipe_details_to_format['name'] = recipe_details_to_format.get('name', internal_details.get(\"name\", \"N/A\"))\n",
    "             recipe_details_to_format['description'] = recipe_details_to_format.get('description', internal_details.get(\"description\", \"\"))\n",
    "             recipe_details_to_format['minutes'] = recipe_details_to_format.get('minutes', internal_details.get(\"minutes\"))\n",
    "             recipe_details_to_format['ingredients'] = recipe_details_to_format.get('ingredients', internal_details.get(\"ingredients\", []))\n",
    "             recipe_details_to_format['steps'] = recipe_details_to_format.get('steps', internal_details.get(\"steps\", []))\n",
    "             recipe_details_to_format['n_ingredients'] = len(recipe_details_to_format.get('ingredients', []))\n",
    "             recipe_details_to_format['n_steps'] = len(recipe_details_to_format.get('steps', []))\n",
    "\n",
    "        elif state.get(\"current_recipe_details\"):\n",
    "             print(\"Using internal recipe data for dashboard.\")\n",
    "             recipe_details_to_format = state[\"current_recipe_details\"]\n",
    "             if isinstance(recipe_details_to_format, dict): # Ensure it's a dict\n",
    "                 recipe_details_to_format['source'] = \"internal database\" # Ensure source is set\n",
    "             else:\n",
    "                  print(f\"Warning: Internal recipe details are not a dictionary. Type: {type(recipe_details_to_format)}\")\n",
    "                  recipe_details_to_format = None # Prevent formatting error\n",
    "\n",
    "        if recipe_details_to_format:\n",
    "             formatted_response = format_recipe_dashboard(recipe_details_to_format)\n",
    "             final_intent_for_history = \"recipe_dashboard_presented\"\n",
    "        else:\n",
    "             # Fallback if data is bad\n",
    "             print(\"Warning: No valid recipe details found to format dashboard.\")\n",
    "             formatted_response = \"I found the recipe, but had trouble displaying the details.\"\n",
    "             final_intent_for_history = \"recipe_details_error\"\n",
    "\n",
    "\n",
    "    # 4. Aggregated Nutrition Info\n",
    "    elif state.get(\"nutritional_info\"):\n",
    "        print(\"Formatting nutrition summary.\")\n",
    "        agg_info = state[\"nutritional_info\"]\n",
    "        nutrient_counts_from_state = agg_info.get(\"nutrient_counts\", {})\n",
    "\n",
    "        recipe_name = \"the recipe\"\n",
    "        recipe_id = state.get(\"selected_recipe_id\")\n",
    "        if state.get(\"current_recipe_details\"):\n",
    "            recipe_name = state[\"current_recipe_details\"].get(\"name\", f\"recipe {recipe_id}\" if recipe_id else \"the recipe\")\n",
    "        elif recipe_id:\n",
    "            recipe_name = f\"recipe {recipe_id}\"\n",
    "\n",
    "        response_lines = [f\"{NUTRITION_RESPONSE_HEADER} **{recipe_name}**:\\n\"] # Use \\n\n",
    "        processed_count = agg_info.get('processed_ingredient_count', 0)\n",
    "\n",
    "        display_order = [\"calories_100g\", \"fat_100g\", \"saturated_fat_100g\", \"carbohydrates_100g\", \"sugars_100g\", \"fiber_100g\", \"proteins_100g\", \"sodium_100g\"]\n",
    "        has_data = False\n",
    "        for key in display_order:\n",
    "            if key in agg_info and nutrient_counts_from_state.get(key, 0) > 0:\n",
    "                 val = agg_info[key]\n",
    "                 unit = 'kcal' if 'calories' in key else ('mg' if key == 'sodium_100g' else 'g')\n",
    "                 display_key = key.replace('_100g', '').replace('_', ' ').capitalize()\n",
    "                 display_val = f\"{val:.1f}\"\n",
    "                 response_lines.append(f\"- {display_key}: {display_val} {unit}\")\n",
    "                 has_data = True\n",
    "\n",
    "        if has_data and processed_count > 0:\n",
    "             source_name = \"USDA FDC\" # Default source for nutrition\n",
    "             response_lines.append(f\"\\n(Note: Based on average of {processed_count} ingredients with available data from {source_name}. Actual recipe nutrition will vary.)\")\n",
    "        elif processed_count > 0:\n",
    "             response_lines.append(\"\\n(Note: Could not retrieve detailed nutrition data for the ingredients, only partial information might be available.)\")\n",
    "        else:\n",
    "             response_lines.append(\"\\n(Note: Could not retrieve nutrition data for the ingredients.)\")\n",
    "\n",
    "        formatted_response = \"\\n\".join(response_lines) # Use \\n\n",
    "        final_intent_for_history = \"nutrition_presented\"\n",
    "\n",
    "    # 5. Last AI message (e.g., from parser node direct response)\n",
    "    elif state.get(\"last_assistant_response\"):\n",
    "         formatted_response = state[\"last_assistant_response\"]\n",
    "         # Try to find if this came from a message object already\n",
    "         last_msg = state['messages'][-1] if state.get('messages') and isinstance(state['messages'][-1], AIMessage) else None\n",
    "         if last_msg and last_msg.content == formatted_response:\n",
    "             final_message_obj = last_msg # Use the existing object\n",
    "         # else: # This text response might not be in messages yet, will add below\n",
    "\n",
    "    # 6. Check messages list for last AI response if not set above\n",
    "    elif state.get('messages') and isinstance(state['messages'][-1], AIMessage) and state['messages'][-1].content:\n",
    "         # This case handles when the parser node returns a direct text response\n",
    "         final_message_obj = state['messages'][-1]\n",
    "         formatted_response = final_message_obj.content\n",
    "         # Capture intent if available from metadata\n",
    "         final_intent_for_history = final_message_obj.metadata.get(\"intent\", final_intent_for_history)\n",
    "\n",
    "    # 7. Handle explicit exit intent if no other content generated\n",
    "    elif state.get(\"intent\") == \"exit\" or state.get(\"finished\"):\n",
    "        formatted_response = \"Okay, goodbye! Feel free to ask if you need recipes later.\"\n",
    "        final_intent_for_history = \"exit\"\n",
    "\n",
    "    # --- Final State Update ---\n",
    "    print(f\"Final Formatted Response Type: {type(formatted_response)}\")\n",
    "    print(f\"Final Formatted Response (first 100 chars): {str(formatted_response)[:100]}...\")\n",
    "\n",
    "    # Create the final AIMessage if it wasn't already captured\n",
    "    if not final_message_obj:\n",
    "         final_message_obj = AIMessage(content=str(formatted_response), metadata={\"intent\": final_intent_for_history})\n",
    "\n",
    "    # ---> STRENGTHENED: Explicitly ensure last_assistant_response is the final formatted string <---\n",
    "    updates = {\n",
    "        \"last_assistant_response\": str(formatted_response), # Ensure it's a string\n",
    "        \"intent\": None, # Reset intent after formatting\n",
    "        \"needs_clarification\": False, # Reset flag\n",
    "        # ---> CRUCIAL: Replace message history with ONLY the final AI response <---\n",
    "        # This prevents the display helper from showing intermediate steps or old messages\n",
    "        \"messages\": [final_message_obj],\n",
    "        # Clear transient data fields used to generate this response\n",
    "        \"nutritional_info\": None,\n",
    "        \"ingredient_nutrition_list\": None,\n",
    "        \"grounding_results_formatted\": None,\n",
    "        \"live_recipe_details\": None,\n",
    "        \"processed_review_data\": None,\n",
    "        \"customization_results\": None,\n",
    "        \"recipe_reviews\": None, # Also clear raw reviews after processing\n",
    "        # Keep context fields unless explicitly cleared elsewhere\n",
    "        \"current_recipe_details\": state.get(\"current_recipe_details\"),\n",
    "        \"selected_recipe_id\": state.get(\"selected_recipe_id\"),\n",
    "        \"finished\": state.get(\"finished\", False) or final_intent_for_history == \"exit\" # Update finished flag if exiting\n",
    "    }\n",
    "\n",
    "    # Filter out keys not in KitchenState or unchanged values (except messages)\n",
    "    valid_keys = KitchenState.__annotations__.keys()\n",
    "    # ---> Ensure 'messages' is always included in the return if it changed <---\n",
    "    return {k: v for k, v in updates.items() if k in valid_keys and (k == 'messages' or state.get(k) != v)}\n",
    "\n",
    "\n",
    "print(\"âœ… LangGraph Step 2: System Instructions & Core Nodes Defined (Revised for Live Data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Step 3: Define and Bind Tools\n",
    "\n",
    "Defines or re-defines all the Python functions that will act as tools for the LangGraph agent, using the `@tool` decorator where appropriate (though direct use is also shown). This includes:\n",
    "*   `gemini_recipe_similarity_search` (revised for JSON output)\n",
    "*   `get_recipe_by_id` (revised for more robust parsing)\n",
    "*   `get_ratings_and_reviews_by_recipe_id` (revised with default limit and rating sample)\n",
    "*   `fetch_nutrition_from_usda_fdc` (using environment variable for key)\n",
    "*   `customize_recipe` (revised placeholder logic)\n",
    "*   `fetch_live_recipe_data` (added placeholder tool)\n",
    "*   `extract_and_visualize_nutrition` (revised to accept header constant)\n",
    "It also initializes the VADER sentiment analyzer (optional) and binds the callable tools to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:58:39.066160Z",
     "iopub.status.busy": "2025-04-18T14:58:39.065841Z",
     "iopub.status.idle": "2025-04-18T14:58:39.191934Z",
     "shell.execute_reply": "2025-04-18T14:58:39.191021Z",
     "shell.execute_reply.started": "2025-04-18T14:58:39.066128Z"
    }
   },
   "outputs": [],
   "source": [
    "### **Step 3: Tool Definition & Integration (Revised)**\n",
    "\n",
    "\n",
    "# --- NUTRITION_RESPONSE_HEADER ---\n",
    "# NUTRITION_RESPONSE_HEADER = \"Here's the approximate average nutrition per 100g for ingredients in\"\n",
    "\n",
    "# --- Helper Function ---\n",
    "def safe_convert(x):\n",
    "    # ... (keep existing function) ...\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        return \" \".join([str(item) for item in x])\n",
    "    return str(x) if pd.notna(x) else \"\"\n",
    "\n",
    "# --- Nutrient Mapping for USDA FDC ---\n",
    "FDC_NUTRIENT_MAP = {\n",
    "    \"Energy\": \"calories_100g\", # KCAL\n",
    "    \"Total lipid (fat)\": \"fat_100g\", # G\n",
    "    \"Fatty acids, total saturated\": \"saturated_fat_100g\", # G\n",
    "    \"Carbohydrate, by difference\": \"carbohydrates_100g\", # G\n",
    "    \"Sugars, total including NLEA\": \"sugars_100g\", # G\n",
    "    \"Fiber, total dietary\": \"fiber_100g\", # G\n",
    "    \"Protein\": \"proteins_100g\", # G\n",
    "    \"Sodium, Na\": \"sodium_100g\", # MG\n",
    "}\n",
    "\n",
    "# --- Tool Definitions (Revised) ---\n",
    "\n",
    "@tool\n",
    "def gemini_recipe_similarity_search(query_text: str, n_results: int = 5, cuisine: Optional[str] = None, dietary_tag: Optional[str] = None, max_minutes: Optional[int] = None) -> str:\n",
    "    # ... (keep existing function - no changes needed here) ...\n",
    "    \"\"\"\n",
    "    Searches for similar recipes based on a query text using vector embeddings.\n",
    "    Allows filtering by cuisine type, a specific dietary tag (e.g., 'vegetarian', 'gluten-free'),\n",
    "    and maximum cooking time in minutes. Returns a JSON string list of matching recipe summaries.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG TOOL CALL: gemini_recipe_similarity_search(query_text='{query_text}', n_results={n_results}, cuisine='{cuisine}', dietary_tag='{dietary_tag}', max_minutes={max_minutes})\\\")\")\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=VECTOR_DB_PATH)\n",
    "        recipe_collection = client.get_collection(name=\"recipes\")\n",
    "\n",
    "        filter_dict = {}\n",
    "        if cuisine:\n",
    "            filter_dict[\"cuisine_type\"] = cuisine\n",
    "        if dietary_tag:\n",
    "             filter_dict[\"dietary_tags\"] = {\"$in\": [dietary_tag]}\n",
    "        if max_minutes is not None:\n",
    "            try:\n",
    "                filter_dict[\"minutes\"] = {\"$lte\": int(max_minutes)}\n",
    "            except ValueError:\n",
    "                return json.dumps({\"error\": f\"Invalid max_minutes: '{max_minutes}'. Must be an integer.\"})\n",
    "\n",
    "        where_clause = None\n",
    "        if len(filter_dict) == 1:\n",
    "            where_clause = filter_dict\n",
    "        elif len(filter_dict) > 1:\n",
    "            and_conditions = [{field: condition} for field, condition in filter_dict.items()]\n",
    "            where_clause = {\"$and\": and_conditions}\n",
    "\n",
    "\n",
    "        print(f\"ChromaDB Where Clause: {where_clause}\")\n",
    "\n",
    "        results = recipe_collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=int(n_results),\n",
    "            where=where_clause,\n",
    "            include=[\"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        if not results or not results.get('ids') or not results['ids'][0]:\n",
    "            return json.dumps({\"status\": \"not_found\", \"message\": f\"No similar recipes found for '{query_text}' with the specified criteria.\"})\n",
    "\n",
    "        output_list = []\n",
    "        for metadata, distance in zip(results['metadatas'][0], results['distances'][0]):\n",
    "            similarity = round((1 - distance) * 100, 2) if distance is not None else None\n",
    "            tags = metadata.get('dietary_tags', '')\n",
    "            if isinstance(tags, str):\n",
    "                 tag_list = [tag.strip() for tag in tags.split(',') if tag.strip()]\n",
    "            elif isinstance(tags, (list, set)):\n",
    "                 tag_list = list(tags)\n",
    "            else:\n",
    "                 tag_list = []\n",
    "\n",
    "\n",
    "            output_list.append({\n",
    "                \"recipe_id\": str(metadata.get('recipe_id', 'N/A')),\n",
    "                \"name\": metadata.get('name', 'N/A'),\n",
    "                \"minutes\": metadata.get('minutes', 'N/A'),\n",
    "                \"cuisine_type\": metadata.get('cuisine_type', 'N/A'),\n",
    "                \"dietary_tags\": tag_list,\n",
    "                \"similarity_score\": similarity\n",
    "            })\n",
    "        return json.dumps(output_list, indent=2)\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "         print(f\"ERROR in gemini_recipe_similarity_search (DB Connection?): {e}\")\n",
    "         return json.dumps({\"error\": f\"Database connection error during recipe search: {e}\"})\n",
    "    except ImportError as e:\n",
    "         print(f\"ERROR in gemini_recipe_similarity_search (Import Error): {e}\")\n",
    "         return json.dumps({\"error\": f\"Missing library required for search: {e}\"})\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in gemini_recipe_similarity_search: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if \"Expected where operator\" in str(e) or \"Unsupported operand\" in str(e):\n",
    "             return json.dumps({\"error\": f\"Error during recipe similarity search: Problem with filter criteria syntax or data type mismatch in database. Details: {e}\"})\n",
    "        return json.dumps({\"error\": f\"Error during recipe similarity search: {e}\"})\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_recipe_by_id(recipe_id: str) -> str:\n",
    "    # ... (keep existing function - no changes needed here) ...\n",
    "    \"\"\"\n",
    "    Retrieves full details for a specific recipe given its ID from the SQL database.\n",
    "    Returns details as a JSON string. Includes 'normalized_ingredients' used for nutrition lookup.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG TOOL CALL: get_recipe_by_id(recipe_id='{recipe_id}')\")\n",
    "    try:\n",
    "        if not isinstance(recipe_id, str) or not recipe_id.isdigit():\n",
    "             try:\n",
    "                 recipe_id_int = int(recipe_id)\n",
    "                 recipe_id = str(recipe_id_int)\n",
    "             except (ValueError, TypeError):\n",
    "                  return json.dumps({\"status\": \"error\", \"message\": f\"Invalid recipe_id format: '{recipe_id}'. Must be a numeric string.\"})\n",
    "\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            conn.row_factory = sqlite3.Row\n",
    "            cursor = conn.cursor()\n",
    "            # ---> MODIFIED: Ensure all potentially needed fields are selected <---\\n\",\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT id, name, minutes, contributor_id, submitted, tags, nutrition,\n",
    "                       n_steps, steps, description, ingredients, n_ingredients,\n",
    "                       dietary_tags, cuisine_type, normalized_ingredients\n",
    "                FROM recipes WHERE id = ?\n",
    "            \"\"\", (int(recipe_id),))\n",
    "            recipe_data = cursor.fetchone()\n",
    "\n",
    "            if not recipe_data:\n",
    "                return json.dumps({\"status\": \"not_found\", \"message\": f\"Recipe ID {recipe_id} not found.\"})\n",
    "\n",
    "            recipe_dict = dict(recipe_data)\n",
    "            recipe_dict['id'] = str(recipe_dict['id']) # Ensure ID is string\n",
    "\n",
    "            # ---> MODIFIED: More robust parsing for list-like fields <---\\n\",\n",
    "            for field in [\"ingredients\", \"steps\", \"tags\", \"dietary_tags\", \"normalized_ingredients\", \"nutrition\"]:\n",
    "                 if field in recipe_dict and isinstance(recipe_dict[field], str):\n",
    "                     raw_value = recipe_dict[field].strip()\n",
    "                     parsed_value = None\n",
    "                     try:\n",
    "                         # Try parsing as JSON list/dict first\n",
    "                         if raw_value.startswith(('[', '{')) and raw_value.endswith((']', '}')):\n",
    "                             parsed_value = json.loads(raw_value)\n",
    "                         # Try parsing Python literal lists/tuples (handle with care)\n",
    "                         elif raw_value.startswith(('(', '[')) and raw_value.endswith((')', ']')):\n",
    "                              import ast\n",
    "                              parsed_value = ast.literal_eval(raw_value)\n",
    "                         # ---> MODIFIED: Fallback splitting no longer applies to 'steps' <---\\n\",\n",
    "                         # Fallback: Split comma-separated strings for specific fields (excluding steps)\n",
    "                         elif field in [\"tags\", \"dietary_tags\", \"normalized_ingredients\", \"ingredients\"]:\n",
    "                              parsed_value = [item.strip() for item in raw_value.split(',') if item.strip()]\n",
    "\n",
    "                         if parsed_value is not None:\n",
    "                              recipe_dict[field] = parsed_value\n",
    "\n",
    "                     except (json.JSONDecodeError, SyntaxError, ValueError, TypeError) as parse_error:\n",
    "                          print(f\"Warning: Could not parse field '{field}' for recipe {recipe_id}. Keeping as string. Value: '{raw_value[:50]}...'. Error: {parse_error}\")\n",
    "\n",
    "            # Ensure key fields used later exist, even if empty\n",
    "            if \"normalized_ingredients\" not in recipe_dict or not isinstance(recipe_dict.get(\"normalized_ingredients\"), list):\n",
    "                 print(f\"Warning: 'normalized_ingredients' for recipe {recipe_id} is not a list or missing. Setting to empty list.\")\n",
    "                 recipe_dict[\"normalized_ingredients\"] = []\n",
    "            # Ensure ingredients is a list if not parsed\n",
    "            if \"ingredients\" in recipe_dict and not isinstance(recipe_dict.get(\"ingredients\"), list):\n",
    "                 recipe_dict[\"ingredients\"] = [] # Fallback to empty list if not parsable\n",
    "            # ---> REMOVED: Do not force 'steps' to be a list if it's not <---\\n\",\n",
    "            # if \"steps\" not in recipe_dict or not isinstance(recipe_dict.get(\"steps\"), list):\n",
    "            #      recipe_dict[\"steps\"] = []\n",
    "            if \"n_ingredients\" not in recipe_dict:\n",
    "                 recipe_dict[\"n_ingredients\"] = len(recipe_dict[\"ingredients\"])\n",
    "            if \"n_steps\" not in recipe_dict:\n",
    "                 recipe_dict[\"n_steps\"] = len(recipe_dict[\"steps\"])\n",
    "            # ---> END MODIFICATION <---\\n\",\n",
    "\n",
    "            return json.dumps(recipe_dict, indent=2, default=str) # Use default=str for safety\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR in get_recipe_by_id (SQL): {e}\")\n",
    "        return json.dumps({\"error\": f\"Database error fetching recipe ID {recipe_id}: {e}\"})\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in get_recipe_by_id: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return json.dumps({\"error\": f\"Unexpected error fetching recipe ID {recipe_id}: {e}\"})\n",
    "\n",
    "@tool\n",
    "def get_ratings_and_reviews_by_recipe_id(recipe_id: str, limit: int = 5) -> str: # ---> MODIFIED: Default limit to 5 <---\n",
    "    \"\"\"\n",
    "    Retrieves the overall average rating and the most recent reviews (up to 'limit')\n",
    "    for a given recipe ID from the SQL database. Also attempts to retrieve individual ratings\n",
    "    for dashboard display. Requires a positive integer for 'limit'. Returns data as a JSON string.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG TOOL CALL: get_ratings_and_reviews_by_recipe_id(recipe_id='{recipe_id}', limit={limit})\\\")\")\n",
    "    if not isinstance(recipe_id, str) or not recipe_id.isdigit():\n",
    "         try:\n",
    "             recipe_id_int = int(recipe_id)\n",
    "             recipe_id = str(recipe_id_int)\n",
    "         except (ValueError, TypeError):\n",
    "              return json.dumps({\"status\": \"error\", \"message\": f\"Invalid recipe_id format: '{recipe_id}'. Must be a numeric string.\"})\n",
    "\n",
    "    try:\n",
    "        limit_int = int(limit)\n",
    "        if limit_int <= 0: raise ValueError(\"'limit' must be positive.\")\n",
    "    except (ValueError, TypeError):\n",
    "        return json.dumps({\"error\": f\"'limit' parameter must be a positive integer. Got: {limit}\"})\n",
    "\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            # Check if recipe exists\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM recipes WHERE id = ?\", (int(recipe_id),))\n",
    "            if cursor.fetchone()[0] == 0:\n",
    "                 return json.dumps({\"status\": \"not_found\", \"message\": f\"Recipe ID {recipe_id} not found.\"})\n",
    "\n",
    "            # Get overall average rating\n",
    "            cursor.execute(\"SELECT AVG(rating) FROM interactions WHERE recipe_id = ?\", (int(recipe_id),))\n",
    "            overall_rating_result = cursor.fetchone()\n",
    "            overall_rating = round(overall_rating_result[0], 2) if overall_rating_result and overall_rating_result[0] is not None else None\n",
    "\n",
    "            # Get recent reviews (text)\n",
    "            cursor.execute(\n",
    "                \"\"\"SELECT date, rating, review\n",
    "                   FROM interactions\n",
    "                   WHERE recipe_id = ? AND review IS NOT NULL AND review != ''\n",
    "                   ORDER BY date DESC LIMIT ?\"\"\",\n",
    "                (int(recipe_id), limit_int),\n",
    "            )\n",
    "            recent_reviews_raw = cursor.fetchall()\n",
    "            review_columns = [\"date\", \"rating\", \"review\"]\n",
    "            reviews_list = [dict(zip(review_columns, review)) for review in recent_reviews_raw]\n",
    "\n",
    "            # ---> ADDED: Attempt to get individual ratings for breakdown <---\n",
    "            # This assumes the 'interactions' table has individual ratings.\n",
    "            # Fetch a larger sample for potentially better distribution, e.g., last 50 ratings\n",
    "            cursor.execute(\n",
    "                \"SELECT rating FROM interactions WHERE recipe_id = ? ORDER BY date DESC LIMIT 50\",\n",
    "                (int(recipe_id),)\n",
    "            )\n",
    "            all_ratings_raw = cursor.fetchall()\n",
    "            all_ratings = [r[0] for r in all_ratings_raw if r[0] is not None]\n",
    "            # ---> END ADDITION <---\n",
    "\n",
    "            result_dict = {\n",
    "                \"recipe_id\": recipe_id,\n",
    "                \"overall_rating\": overall_rating,\n",
    "                \"recent_reviews\": reviews_list,\n",
    "                \"all_ratings_sample\": all_ratings # Added for dashboard\n",
    "            }\n",
    "            return json.dumps(result_dict, indent=2)\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERROR in get_ratings_and_reviews_by_recipe_id (SQL): {e}\")\n",
    "        return json.dumps({\"error\": f\"Database error fetching reviews for recipe ID {recipe_id}: {e}\"})\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in get_ratings_and_reviews_by_recipe_id: {e}\")\n",
    "        return json.dumps({\"error\": f\"Unexpected error fetching reviews for recipe ID {recipe_id}: {e}\"})\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_nutrition_from_usda_fdc(ingredient_name: str) -> str:\n",
    "    # ... (keep existing function - no changes needed here) ...\n",
    "    \"\"\"\n",
    "    Fetches nutrition data (per 100g) for a single ingredient from USDA FoodData Central API.\n",
    "    Requires the USDA_API_KEY environment variable to be set. Includes robust retry logic.\n",
    "    Returns nutrition data as a JSON string or an error/unavailable status.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG TOOL CALL: fetch_nutrition_from_usda_fdc(ingredient_name='{ingredient_name}')\")\n",
    "    api_key = UserSecretsClient().get_secret(\"USDA_API_KEY\")\n",
    "\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"ERROR: USDA_API_KEY environment variable not set.\")\n",
    "        return json.dumps({\"error\": \"USDA FDC API key environment variable not set.\"})\n",
    "\n",
    "    search_url = \"https://api.nal.usda.gov/fdc/v1/foods/search\"\n",
    "    params = {\n",
    "        \"query\": ingredient_name,\n",
    "        \"api_key\": api_key,\n",
    "        \"pageSize\": 1,\n",
    "        \"dataType\": \"SR Legacy,Foundation\", # Prioritize standard reference / foundation\n",
    "    }\n",
    "    headers = {'User-Agent': 'KitchenAssistantLangGraph/1.0 (Language: Python)'}\n",
    "\n",
    "    max_retries = 3\n",
    "    base_timeout = 15\n",
    "    retry_delay = 1\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(search_url, params=params, headers=headers, timeout=base_timeout)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if data.get('foods') and len(data['foods']) > 0:\n",
    "                food_item = data['foods'][0]\n",
    "                fdc_nutrients = food_item.get('foodNutrients', [])\n",
    "\n",
    "                nutrition_info = {\n",
    "                    \"food_normalized\": ingredient_name,\n",
    "                    \"source\": \"USDA FoodData Central\",\n",
    "                    \"product_name\": food_item.get('description', ingredient_name),\n",
    "                    \"fdc_id\": food_item.get('fdcId'),\n",
    "                    \"data_type\": food_item.get('dataType'),\n",
    "                }\n",
    "\n",
    "                found_nutrients = {}\n",
    "                for nutrient in fdc_nutrients:\n",
    "                    nutrient_name = nutrient.get('nutrientName')\n",
    "                    nutrient_value = nutrient.get('value') # Keep value as reported\n",
    "                    target_key = FDC_NUTRIENT_MAP.get(nutrient_name)\n",
    "                    if target_key:\n",
    "                        found_nutrients[target_key] = nutrient_value\n",
    "\n",
    "                nutrition_info.update(found_nutrients)\n",
    "                filtered_nutrition = {k: v for k, v in nutrition_info.items() if v is not None}\n",
    "\n",
    "                core_nutrients = [\"calories_100g\", \"fat_100g\", \"proteins_100g\", \"carbohydrates_100g\"]\n",
    "                has_core_data = False\n",
    "                for core_key in core_nutrients:\n",
    "                    if core_key in filtered_nutrition:\n",
    "                        try:\n",
    "                            float(filtered_nutrition[core_key])\n",
    "                            has_core_data = True\n",
    "                            break\n",
    "                        except (ValueError, TypeError): continue\n",
    "\n",
    "                if not has_core_data:\n",
    "                    print(f\"--> No core numeric nutrition data found for '{ingredient_name}' in product '{filtered_nutrition.get('product_name', 'N/A')}' (FDC ID: {filtered_nutrition.get('fdc_id')})\\\")\")\n",
    "                    return json.dumps({\"status\": \"unavailable\", \"reason\": f\"No detailed numeric core nutrition data found for '{ingredient_name}'\"})\n",
    "\n",
    "                print(f\"--> Successfully found nutrition data for '{ingredient_name}' via USDA FDC\")\n",
    "                return json.dumps(filtered_nutrition, indent=2)\n",
    "            else:\n",
    "                print(f\"--> No product found for '{ingredient_name}' via USDA FDC\")\n",
    "                return json.dumps({\"status\": \"unavailable\", \"reason\": f\"No product found for '{ingredient_name}' on USDA FDC\"})\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 403:\n",
    "                 print(f\"HTTP Error 403 (Forbidden) for '{ingredient_name}'. Check your USDA FDC API key.\")\n",
    "                 return json.dumps({\"status\": \"error\", \"reason\": f\"API request failed with HTTP 403 (Forbidden). Check API Key.\"})\n",
    "            elif e.response.status_code == 429 and attempt < max_retries - 1:\n",
    "                wait_time = (retry_delay * (2 ** attempt)) + random.uniform(0, 1)\n",
    "                print(f\"Rate limit hit for '{ingredient_name}'. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\\\")\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            elif e.response.status_code >= 500 and attempt < max_retries - 1:\n",
    "                 wait_time = (retry_delay * (2 ** attempt)) + random.uniform(0, 1)\n",
    "                 print(f\"Server error ({e.response.status_code}) for '{ingredient_name}'. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\\\")\")\n",
    "                 time.sleep(wait_time)\n",
    "                 continue\n",
    "            else:\n",
    "                print(f\"HTTP Error fetching nutrition for '{ingredient_name}': {e}\")\n",
    "                return json.dumps({\"status\": \"unavailable\", \"reason\": f\"API request failed with HTTP error: {e}\"})\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (retry_delay * (2 ** attempt)) + random.uniform(0, 1)\n",
    "                print(f\"Request error for '{ingredient_name}': {e}. Retrying in {wait_time:.2f} seconds... (Attempt {attempt + 1}/{max_retries})\\\")\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error fetching nutrition for '{ingredient_name}' after {max_retries} attempts: {e}\")\n",
    "                return json.dumps({\"status\": \"unavailable\", \"reason\": f\"API request failed after retries: {e}\"})\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON response for '{ingredient_name}'\")\n",
    "            return json.dumps({\"status\": \"unavailable\", \"reason\": \"Invalid JSON response from API\"})\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"ERROR in fetch_nutrition_from_usda_fdc: {e}\")\n",
    "             import traceback\n",
    "             traceback.print_exc()\n",
    "             return json.dumps({\"error\": f\"Unexpected error fetching nutrition for {ingredient_name}: {e}\"})\n",
    "\n",
    "    print(f\"Max retries ({max_retries}) exceeded for API request for '{ingredient_name}'\")\n",
    "    return json.dumps({\"status\": \"unavailable\", \"reason\": f\"Max retries ({max_retries}) exceeded for API request for '{ingredient_name}'\"})\n",
    "\n",
    "\n",
    "# ---> REVISED: customize_recipe Tool (Slightly Improved Placeholder Logic) <---\\n\",\n",
    "import json\n",
    "import re # Make sure re is imported if not already done earlier in the cell\n",
    "from typing import Optional # Ensure Optional is imported\n",
    "\n",
    "@tool\n",
    "def customize_recipe(recipe_id: str, request: str, recipe_details_json: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Attempts to customize a recipe based on a user request (e.g., make vegetarian, substitute ingredient, make gluten-free, make low-fat/healthier).\n",
    "    Requires the recipe_id and the specific customization request string.\n",
    "    Optionally takes current recipe_details as JSON string to avoid re-fetching.\n",
    "    Returns a JSON string describing the suggested modifications or indicating inability.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG TOOL CALL: customize_recipe(recipe_id='{recipe_id}', request='{request}')\")\n",
    "    if not recipe_id or not request:\n",
    "        return json.dumps({\"status\": \"error\", \"message\": \"Missing recipe_id or customization request.\"})\n",
    "    if not isinstance(recipe_id, str) or not recipe_id.isdigit():\n",
    "         try:\n",
    "             recipe_id_int = int(recipe_id)\n",
    "             recipe_id = str(recipe_id_int)\n",
    "         except (ValueError, TypeError):\n",
    "              return json.dumps({\"status\": \"error\", \"message\": f\"Invalid recipe_id format: '{recipe_id}'. Must be numeric string.\"})\n",
    "\n",
    "    # --- Placeholder Logic (Slightly Enhanced) ---\n",
    "    # In a real implementation: Parse details, analyze request (NLP/LLM), lookup substitutions, generate specific steps.\n",
    "\n",
    "    request_lower = request.lower()\n",
    "    modifications = []\n",
    "    status = \"placeholder_success\"\n",
    "    message_lines = [f\"Suggestions for making recipe {recipe_id} '{request}':\"]\n",
    "\n",
    "    # Basic Keyword Checks\n",
    "    if \"vegan\" in request_lower:\n",
    "        modifications.append({\"action\": \"replace\", \"original\": \"dairy/meat/eggs\", \"suggestion\": \"plant-based alternatives (e.g., tofu, plant milk, oil instead of butter, nutritional yeast for cheese)\"})\n",
    "        message_lines.append(\"- Check all ingredients for animal products (stock, cheese, etc.) and replace with vegan versions.\")\n",
    "    elif \"gluten-free\" in request_lower or \"gluten free\" in request_lower:\n",
    "        modifications.append({\"action\": \"replace\", \"original\": \"wheat flour/pasta/soy sauce\", \"suggestion\": \"certified gluten-free alternatives (GF flour blend, GF pasta, tamari)\"})\n",
    "        message_lines.append(\"- Replace any gluten-containing grains (wheat, barley, rye) with gluten-free options.\")\n",
    "    elif \"low fat\" in request_lower or \"low-fat\" in request_lower or \"healthy\" in request_lower or \"healthier\" in request_lower:\n",
    "        modifications.append({\"action\": \"reduce/replace\", \"original\": \"high-fat ingredients (e.g., butter, cream, fatty meats, cheese spread)\", \"suggestion\": \"lower-fat options (e.g., olive oil sparingly, low-fat milk/yogurt, lean protein, reduced-fat cheese)\"})\n",
    "        message_lines.append(\"- Reduce added fats like butter or oil where possible.\")\n",
    "        message_lines.append(\"- Use low-fat dairy alternatives.\")\n",
    "        message_lines.append(\"- Consider adding more vegetables for volume and nutrients.\")\n",
    "        # Simple check if details provided and contain the ingredient\n",
    "        if recipe_details_json and \"american cheese spread\" in recipe_details_json:\n",
    "             message_lines.append(\"- Specifically, consider reducing or replacing the American cheese spread with a lower-fat option or nutritional yeast for cheesy flavor.\")\n",
    "    elif \"substitute\" in request_lower or \"replace\" in request_lower:\n",
    "        # Use regex to find \"substitute X for/with Y\" or \"replace X for/with Y\"\n",
    "        match = re.search(r\"(?:substitute|replace)\\s+(.*?)\\s+(?:for|with)\\s+(.*)\", request_lower)\n",
    "        original = \"ingredient_A\"\n",
    "        suggestion = \"ingredient_B\"\n",
    "        if match:\n",
    "            original = match.group(1).strip()\n",
    "            suggestion = match.group(2).strip()\n",
    "        modifications.append({\"action\": \"replace\", \"original\": original, \"suggestion\": suggestion})\n",
    "        message_lines.append(f\"- You could try replacing '{original}' with '{suggestion}'. Check quantities and cooking times.\")\n",
    "    else:\n",
    "        # If no specific keywords match, provide a generic placeholder or ask for clarification\n",
    "        status = \"placeholder_needs_clarification\" # Changed status\n",
    "        message_lines.append(f\"- I can provide general suggestions for '{request}'. Review ingredients and consider reducing fats/sugars or increasing vegetables.\")\n",
    "        message_lines.append(\"- For specific substitutions (like 'replace X with Y'), please state them clearly.\")\n",
    "        modifications.append({\"action\": \"general_advice\", \"original\": request, \"suggestion\": \"Review recipe based on request\"})\n",
    "\n",
    "    # Add a default message if no specific modifications were generated but status is success\n",
    "    if status == \"placeholder_success\" and not modifications:\n",
    "         message_lines.append(\"- No specific substitutions identified by this placeholder logic, but review ingredients based on your request.\")\n",
    "\n",
    "    return json.dumps({\n",
    "        \"status\": status,\n",
    "        \"message\": \"\\n\".join(message_lines), # Join lines for better formatting in the final message\n",
    "        \"recipe_id\": recipe_id,\n",
    "        \"original_request\": request,\n",
    "        \"suggested_modifications\": modifications\n",
    "    })\n",
    "# ---> END REVISION <---\\n\",\n",
    "\n",
    "# ---> ADDED: fetch_live_recipe_data Tool (Placeholder) <---\n",
    "@tool\n",
    "def fetch_live_recipe_data(recipe_id: str) -> str:\n",
    "    \"\"\"\n",
    "    (Placeholder) Attempts to fetch live recipe data (ingredients, steps, time)\n",
    "    from food.com using the recipe ID. Returns data as a JSON string or an error status.\n",
    "    Requires requests and beautifulsoup4 libraries.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG TOOL CALL: fetch_live_recipe_data(recipe_id='{recipe_id}')\")\n",
    "    if not isinstance(recipe_id, str) or not recipe_id.isdigit():\n",
    "        return json.dumps({\"status\": \"error\", \"message\": \"Invalid recipe_id format.\"})\n",
    "\n",
    "    url = f\"https://www.food.com/recipe/{recipe_id}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} # Basic user agent\n",
    "\n",
    "    try:\n",
    "        # --- Placeholder Scraping Logic ---\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Example: Extracting ingredients (CSS selectors WILL change)\n",
    "        ingredients_elements = soup.select('.recipe-ingredients__list .recipe-ingredients__item-label')\n",
    "        ingredients = [el.get_text(strip=True) for el in ingredients_elements]\n",
    "        \n",
    "        # Example: Extracting time (CSS selectors WILL change)\n",
    "        time_element = soup.select_one('.recipe-facts__details--total-time .value')\n",
    "        total_time_str = time_element.get_text(strip=True) if time_element else None\n",
    "        # Parse total_time_str into minutes (e.g., using regex)\n",
    "        total_minutes = None # Placeholder for parsed time\n",
    "        \n",
    "        # ... extract other fields like steps, description ...\n",
    "        \n",
    "        if not ingredients: # Basic check if scraping failed\n",
    "            raise ValueError(\"Could not extract ingredients from page.\")\n",
    "        \n",
    "        live_data = {\n",
    "            \"source\": \"food.com (live)\",\n",
    "            \"url\": url,\n",
    "            \"ingredients\": ingredients,\n",
    "            \"minutes\": total_minutes,\n",
    "            # \"steps\": extracted_steps,\n",
    "            # \"description\": extracted_description,\n",
    "        }\n",
    "        # return json.dumps({\"status\": \"live_success\", \"data\": live_data})\n",
    "        # --- End Placeholder ---\n",
    "\n",
    "        # Return placeholder success for now\n",
    "        print(f\"Placeholder: Would attempt to scrape {url}\")\n",
    "        return json.dumps({\n",
    "            \"status\": \"placeholder_success\",\n",
    "            \"message\": f\"Placeholder success for fetching live data for recipe {recipe_id}.\",\n",
    "            \"data\": {\n",
    "                \"source\": \"food.com (placeholder)\",\n",
    "                \"url\": url,\n",
    "                \"ingredients\": [\"Placeholder Ingredient 1\", \"Placeholder Ingredient 2\"],\n",
    "                \"minutes\": 45, # Placeholder value\n",
    "                \"steps\": [\"Placeholder Step 1\", \"Placeholder Step 2\"],\n",
    "                \"description\": \"Placeholder live description.\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in fetch_live_recipe_data for recipe {recipe_id}: {e}\")\n",
    "        return json.dumps({\"status\": \"error\", \"message\": f\"Failed to fetch or parse live data: {e}\"})\n",
    "# ---> END ADDITION <---\n",
    "\n",
    "\n",
    "# --- Nutrition Visualization Function ---\n",
    "# ---> MODIFIED: Add header_constant argument <---\n",
    "# In cell 5ead34ad-8b23-4018-a4d7-da4a854eebce\n",
    "# Find the --- Nutrition Visualization Function --- section\n",
    "# --- Nutrition Visualization Function ---\n",
    "# ---> MODIFIED: Add header_constant argument <---\\n\",\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "\n",
    "def extract_and_visualize_nutrition(response_text: str, header_constant: str):\n",
    "    \"\"\"Extracts nutrition data from text starting with header_constant and plots it.\"\"\"\n",
    "    print(\"Attempting to extract and visualize nutrition...\")\n",
    "\n",
    "    if not isinstance(response_text, str):\n",
    "        print(f\"Error: Expected string input for visualization, got {type(response_text)}\")\n",
    "        return # Cannot proceed if input isn't a string\n",
    "\n",
    "    if not header_constant:\n",
    "        print(\"ERROR: No header constant provided for visualization.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        header_pattern = re.escape(header_constant)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating regex pattern from header: {e}\")\n",
    "        return\n",
    "\n",
    "    nutrition_section_match = re.search(\n",
    "        rf\"{header_pattern}.*?:\\s*\\n(.*?)(?:$|\\n\\s*\\n|\\n\\(Note:|\\Z)\", # Slightly adjusted pattern end\n",
    "        response_text,\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    if not nutrition_section_match:\n",
    "        print(f\"Could not find the nutrition section starting with '{header_constant}' in the text.\")\n",
    "        # ---> ADDED: Print the text it searched in for debugging <---\n",
    "        print(f\"Searched text (first 300 chars): {response_text[:300]}...\")\n",
    "        return\n",
    "\n",
    "    nutrition_text = nutrition_section_match.group(1).strip()\n",
    "    print(f\"Extracted Nutrition Text Block:\\n---\\n{nutrition_text}\\n---\")\n",
    "\n",
    "    nutrient_pattern = re.compile(\n",
    "        r\"^\\s*-\\s*(?P<nutrient>[^:]+?)\\s*:\\s*(?P<value>[\\d.]+)\\s*(?P<unit>kcal|g|mg).*\",\n",
    "        re.MULTILINE | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # ... (rest of the key_map, extraction logic remains the same) ...\n",
    "    key_map = {\n",
    "        'calories': 'calories_100g',\n",
    "        'fat': 'fat_100g',\n",
    "        'saturated fat': 'saturated_fat_100g',\n",
    "        'carbohydrates': 'carbohydrates_100g',\n",
    "        'sugars': 'sugars_100g',\n",
    "        'fiber': 'fiber_100g',\n",
    "        'proteins': 'proteins_100g',\n",
    "        'sodium': 'sodium_100g'\n",
    "    }\n",
    "\n",
    "    extracted_values: Dict[str, float] = {}\n",
    "    extracted_units: Dict[str, str] = {}\n",
    "    processed_nutrients = 0\n",
    "\n",
    "    for match in nutrient_pattern.finditer(nutrition_text):\n",
    "        nutrient_name = match.group(\"nutrient\").strip().lower()\n",
    "        value_str = match.group(\"value\").strip()\n",
    "        unit = match.group(\"unit\").strip().lower()\n",
    "\n",
    "        if nutrient_name in key_map:\n",
    "            state_key = key_map[nutrient_name]\n",
    "            try:\n",
    "                value = float(value_str)\n",
    "                extracted_values[state_key] = value\n",
    "                extracted_units[state_key] = unit\n",
    "                processed_nutrients += 1\n",
    "                print(f\"Extracted: {state_key} = {value} {unit}\")\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not convert value '{value_str}' for '{nutrient_name}'.\")\n",
    "        else:\n",
    "            print(f\"Warning: Unrecognized nutrient '{nutrient_name}'.\")\n",
    "\n",
    "    if processed_nutrients == 0:\n",
    "        print(\"No valid nutrition data found to plot.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processed {processed_nutrients} nutrients.\")\n",
    "    print(\"Values:\", extracted_values)\n",
    "    print(\"Units:\", extracted_units)\n",
    "\n",
    "\n",
    "    # ... (DV calculation and plotting logic remains the same) ...\n",
    "    # ... (Make sure plt is imported correctly) ...\n",
    "    daily_values = {\n",
    "        \"calories_100g\": 2000, \"fat_100g\": 78, \"saturated_fat_100g\": 20,\n",
    "        \"carbohydrates_100g\": 275, \"sugars_100g\": 50, \"fiber_100g\": 28,\n",
    "        \"proteins_100g\": 50, \"sodium_100g\": 2300\n",
    "    }\n",
    "    percent_dv: Dict[str, float] = {}\n",
    "    actual_values_plot: Dict[str, float] = {}\n",
    "\n",
    "    for key, value in extracted_values.items():\n",
    "        dv = daily_values.get(key)\n",
    "        unit = extracted_units.get(key, 'g')\n",
    "        if dv is not None and dv > 0:\n",
    "            value_for_calc = value\n",
    "            if key == 'sodium_100g' and unit == 'g': value_for_calc *= 1000.0\n",
    "            elif unit == 'mg' and key != 'sodium_100g': value_for_calc /= 1000.0\n",
    "\n",
    "            percent_dv[key] = round((value_for_calc / dv) * 100, 1)\n",
    "            actual_values_plot[key] = round(value, 1)\n",
    "        else:\n",
    "            percent_dv[key] = 0.0\n",
    "            actual_values_plot[key] = round(value, 1)\n",
    "\n",
    "    calories_percent_dv = percent_dv.pop(\"calories_100g\", 0.0)\n",
    "    calories_actual = actual_values_plot.pop(\"calories_100g\", 0.0)\n",
    "    # ---> Filter only non-zero %DV AND ensure key exists in actual_values_plot <---\n",
    "    plot_data = {k: v for k, v in percent_dv.items() if k in actual_values_plot and v > 0}\n",
    "\n",
    "    if not plot_data:\n",
    "        print(\"No data with %DV > 0 to plot.\")\n",
    "        if calories_actual > 0:\n",
    "            print(f\"Avg Calories: {calories_actual:.0f} kcal\")\n",
    "        return\n",
    "\n",
    "    labels = list(plot_data.keys())\n",
    "    display_labels = [l.replace('_100g', '').replace('_', ' ').capitalize() for l in labels]\n",
    "    values = list(plot_data.values())\n",
    "    colors = ['forestgreen' if v <= 15 else ('orange' if v <= 40 else 'red') for v in values]\n",
    "\n",
    "    fig = None # Initialize fig to None\n",
    "    try:\n",
    "        # ---> Use subplots for better figure management <---\n",
    "        fig, ax = plt.subplots(figsize=(10, max(4, len(labels) * 0.6)))\n",
    "        bars = ax.barh(display_labels, values, color=colors, height=0.6)\n",
    "        ax.set_xlabel('% Daily Value (DV) - Based on average of 100g of each ingredient')\n",
    "        ax.set_title('Average Ingredient Nutrition (%DV)', fontsize=16)\n",
    "        ax.tick_params(axis='both', labelsize=10)\n",
    "        max_val = max(values + [100]) # Ensure max_val includes 100 for percentage scale\n",
    "        ax.set_xlim(right=max_val * 1.1) # Adjust xlim dynamically\n",
    "\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            nutrient_key = labels[i]\n",
    "            actual_val = actual_values_plot.get(nutrient_key, 0.0)\n",
    "            unit = extracted_units.get(nutrient_key, 'g')\n",
    "            if nutrient_key == 'sodium_100g': unit = 'mg'\n",
    "            display_actual = f\"{actual_val:.1f}\"\n",
    "            label_text = f'{width:.1f}% ({display_actual} {unit})'\n",
    "            # Dynamic text positioning\n",
    "            x_pos = width + max_val * 0.01 if width < max_val * 0.85 else width - max_val * 0.01\n",
    "            ha = 'left' if width < max_val * 0.85 else 'right'\n",
    "            color = 'black' if width < max_val * 0.85 else 'white'\n",
    "            ax.text(x_pos, bar.get_y() + bar.get_height() / 2., label_text,\n",
    "                    ha=ha, va='center', color=color, fontsize=9, fontweight='bold')\n",
    "\n",
    "        cal_color = 'forestgreen' if calories_percent_dv <= 15 else ('orange' if calories_percent_dv <= 40 else 'red')\n",
    "        calorie_text = f'Estimated Avg Calories per 100g Ingredient: {calories_actual:.0f} kcal ({calories_percent_dv:.1f}% DV)'\n",
    "        fig.text(0.5, 0.97, calorie_text, ha='center', va='bottom', fontsize=12, color=cal_color, fontweight='bold')\n",
    "\n",
    "        plt.gca().invert_yaxis() # Keep high %DV at top\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent overlap\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        # ---> Crucial for Kaggle/non-interactive: Ensure plot is shown <---\n",
    "        plt.show()\n",
    "        print(\"Nutrition visualization displayed.\")\n",
    "\n",
    "    except Exception as plot_error:\n",
    "        print(f\"Error during plotting: {plot_error}\")\n",
    "        # ---> ADDED: Close the figure if it was created but failed during plot generation <---\n",
    "        if fig:\n",
    "            plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "# --- Tool Lists & Executor ---\n",
    "stateless_tools = [ gemini_recipe_similarity_search, get_recipe_by_id, get_ratings_and_reviews_by_recipe_id, fetch_nutrition_from_usda_fdc, fetch_live_recipe_data, customize_recipe ]\n",
    "llm_callable_tools = stateless_tools + [customize_recipe]\n",
    "tool_executor_node = ToolNode(stateless_tools)\n",
    "\n",
    "# --- LLM Binding ---\n",
    "# llm should be defined in Step 2\n",
    "llm_with_callable_tools = llm.bind_tools(llm_callable_tools)\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ… LangGraph Step 3: Tools Defined and Bound (Revised)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Step 4: Define Core Nodes (Input Parser)\n",
    "\n",
    "Defines the `input_parser_node`. This node is crucial for interpreting the latest message (user input or tool result) and determining the next step. It includes revised logic to directly route to specific processing nodes (`AggregateNutritionNode`, `ReviewDashboardNode`, `ProcessCustomizationNode`) or the `ResponseFormatterNode` after certain tool calls (`get_recipe_by_id`, `fetch_live_recipe_data`) complete successfully, bypassing an unnecessary LLM call. Otherwise, it invokes the LLM to decide the next action (call a tool, chat, clarify, exit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:58:41.483130Z",
     "iopub.status.busy": "2025-04-18T14:58:41.482326Z",
     "iopub.status.idle": "2025-04-18T14:58:41.517228Z",
     "shell.execute_reply": "2025-04-18T14:58:41.516521Z",
     "shell.execute_reply.started": "2025-04-18T14:58:41.483104Z"
    }
   },
   "outputs": [],
   "source": [
    "# LangGraph Step 4: Core Node Definitions (Revised Input Parser for Recipe Details)\n",
    "\n",
    "# --- Assume KitchenState, KITCHEN_ASSISTANT_SYSINT, llm_with_callable_tools are defined ---\n",
    "\n",
    "def input_parser_node(state: KitchenState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parses user input or tool results. Routes directly after processing key tool results\n",
    "    (like recipe details, reviews, customization) to avoid unnecessary LLM calls.\n",
    "    Otherwise, uses the LLM to determine the next step.\n",
    "    \"\"\"\n",
    "    print(\"---NODE: InputParserNode---\")\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1] if messages else None\n",
    "    previous_ai_message: Optional[AIMessage] = None\n",
    "    for i in range(len(messages) - 2, -1, -1):\n",
    "        if isinstance(messages[i], AIMessage):\n",
    "            previous_ai_message = messages[i]\n",
    "            break\n",
    "\n",
    "    # --- Direct Routing based on incoming ToolMessage ---\n",
    "\n",
    "    # A. Nutrition Aggregation Check\n",
    "    needs_aggregation = False\n",
    "    last_message = messages[-1] if messages else None\n",
    "\n",
    "    # ---> Refined Check: Only trigger aggregation if the LAST message is a nutrition tool message <---\n",
    "    # ---> AND if we can trace back to an AI message that called multiple nutrition tools <---\n",
    "    if isinstance(last_message, ToolMessage) and last_message.name == \"fetch_nutrition_from_usda_fdc\":\n",
    "        print(\"DEBUG: Last message is a nutrition ToolMessage.\")\n",
    "        # Find the AIMessage that likely requested this batch\n",
    "        last_ai_request_index = -1\n",
    "        for i in range(len(messages) - 2, -1, -1): # Search backwards from message before last\n",
    "            msg = messages[i]\n",
    "            if isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "                nutrition_call_count = sum(1 for tc in msg.tool_calls if tc.get('name') == 'fetch_nutrition_from_usda_fdc')\n",
    "                if nutrition_call_count > 1: # Check if *multiple* calls were made\n",
    "                    print(f\"DEBUG: Found preceding AIMessage at index {i} with {nutrition_call_count} nutrition tool calls.\")\n",
    "                    last_ai_request_index = i\n",
    "                    break\n",
    "                elif nutrition_call_count == 1:\n",
    "                    print(f\"DEBUG: Found preceding AIMessage at index {i} with only 1 nutrition tool call. Not aggregating.\")\n",
    "                    break # Stop searching if only one was called\n",
    "            # Stop if we hit a human message before finding a relevant AI message\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                print(\"DEBUG: Hit HumanMessage before finding multi-nutrition AI request.\")\n",
    "                break\n",
    "\n",
    "        # Check if all messages after the AI request (up to the current one) are nutrition tool messages\n",
    "        if last_ai_request_index != -1:\n",
    "            all_nutrition_results_in_batch = True\n",
    "            # Only check messages between the AI request and the current message\n",
    "            for i in range(last_ai_request_index + 1, len(messages)):\n",
    "                msg = messages[i]\n",
    "                if not (isinstance(msg, ToolMessage) and msg.name == \"fetch_nutrition_from_usda_fdc\"):\n",
    "                    print(f\"DEBUG: Found non-nutrition ToolMessage ({type(msg).__name__}, name={getattr(msg, 'name', 'N/A')}) in batch. Not aggregating yet.\")\n",
    "                    all_nutrition_results_in_batch = False\n",
    "                    break\n",
    "            if all_nutrition_results_in_batch:\n",
    "                needs_aggregation = True\n",
    "                print(\"DEBUG: Detected likely end of nutrition tool results batch.\")\n",
    "        else:\n",
    "            print(\"DEBUG: No preceding AI message found requesting multiple nutrition lookups.\")\n",
    "\n",
    "\n",
    "    if needs_aggregation:\n",
    "        print(\"Routing to aggregation.\")\n",
    "        # Ensure necessary context is preserved for aggregation/formatting\n",
    "        updates = {\n",
    "            \"intent\": \"aggregate_nutrition\",\n",
    "            \"messages\": [], # Clear messages for the aggregation node step\n",
    "            \"selected_recipe_id\": state.get(\"selected_recipe_id\"),\n",
    "            \"current_recipe_details\": state.get(\"current_recipe_details\") # Pass details for recipe name\n",
    "            }\n",
    "        valid_keys = KitchenState.__annotations__.keys()\n",
    "        return {k: v for k, v in updates.items() if k in valid_keys}\n",
    "\n",
    "    # B. Review Processing Check\n",
    "    if isinstance(last_message, ToolMessage) and last_message.name == \"get_ratings_and_reviews_by_recipe_id\":\n",
    "         print(\"Detected review tool results, setting intent to process reviews.\")\n",
    "         try: review_content = json.loads(last_message.content)\n",
    "         except: review_content = {\"error\": \"Failed to parse review tool content\"}\n",
    "         updates = { \"intent\": \"process_reviews\", \"messages\": [], \"recipe_reviews\": review_content, \"selected_recipe_id\": state.get(\"selected_recipe_id\"), \"current_recipe_details\": state.get(\"current_recipe_details\") }\n",
    "         print(f\"Routing to review processing.\")\n",
    "         return {k: v for k, v in updates.items() if k in KitchenState.__annotations__}\n",
    "\n",
    "    # C. Customization Processing Check\n",
    "    if isinstance(last_message, ToolMessage) and last_message.name == \"customize_recipe\":\n",
    "         print(\"Detected customization tool results, setting intent to process customization.\")\n",
    "         updates = { \"intent\": \"process_customization\", \"messages\": [], \"selected_recipe_id\": state.get(\"selected_recipe_id\"), \"current_recipe_details\": state.get(\"current_recipe_details\") }\n",
    "         print(f\"Routing to customization processing.\")\n",
    "         return {k: v for k, v in updates.items() if k in KitchenState.__annotations__}\n",
    "\n",
    "    # D. Live Data Check (Route to Formatter)\n",
    "    if isinstance(last_message, ToolMessage) and last_message.name == \"fetch_live_recipe_data\":\n",
    "         print(\"Detected live recipe data results.\")\n",
    "         try: live_content = json.loads(last_message.content)\n",
    "         except: live_content = {\"error\": \"Failed to parse live data tool content\"}\n",
    "         updates = { \"intent\": \"live_data_fetched\", \"messages\": [], \"live_recipe_details\": live_content, \"selected_recipe_id\": state.get(\"selected_recipe_id\"), \"current_recipe_details\": state.get(\"current_recipe_details\") }\n",
    "         print(f\"Routing to formatter after fetching live data.\")\n",
    "         return {k: v for k, v in updates.items() if k in KitchenState.__annotations__}\n",
    "\n",
    "    # ---> ADDED: Recipe Details Check (Route to Formatter) <---\n",
    "    if isinstance(last_message, ToolMessage) and last_message.name == \"get_recipe_by_id\":\n",
    "        print(\"Detected recipe details tool results, updating state and routing to formatter.\")\n",
    "        try:\n",
    "            details_content = json.loads(last_message.content)\n",
    "            # Check if the tool returned an error or not found status\n",
    "            if isinstance(details_content, dict) and details_content.get(\"status\") in [\"error\", \"not_found\"]:\n",
    "                 # If tool failed, let LLM handle the error message\n",
    "                 print(\"Recipe details tool returned error/not_found, proceeding with LLM.\")\n",
    "                 pass # Fall through to LLM invocation below\n",
    "            elif isinstance(details_content, dict):\n",
    "                 # Success! Update state and route to formatter\n",
    "                 updates = {\n",
    "                     \"intent\": \"recipe_details_fetched\", # Signal for formatter\n",
    "                     \"messages\": [], # Prevent re-processing by LLM\n",
    "                     \"current_recipe_details\": details_content, # Store the fetched details\n",
    "                     \"selected_recipe_id\": state.get(\"selected_recipe_id\"), # Keep context\n",
    "                     # Clear potentially stale related data\n",
    "                     \"live_recipe_details\": None,\n",
    "                     \"processed_review_data\": None,\n",
    "                     \"customization_results\": None,\n",
    "                     \"recipe_reviews\": None,\n",
    "                 }\n",
    "                 print(f\"Routing to formatter after fetching recipe details.\")\n",
    "                 return {k: v for k, v in updates.items() if k in KitchenState.__annotations__}\n",
    "            else:\n",
    "                 # Unexpected content format, let LLM handle\n",
    "                 print(\"Unexpected format for recipe details tool result, proceeding with LLM.\")\n",
    "                 pass # Fall through\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing recipe details tool message: {e}. Proceeding with LLM.\")\n",
    "            pass # Fall through to LLM invocation on error\n",
    "\n",
    "    # ---> END ADDITION <---\n",
    "\n",
    "\n",
    "    # --- Normal LLM Invocation (If no direct routing occurred) ---\n",
    "    # This handles: Initial user input, interpreting other tool results, general chat, errors from previous steps\n",
    "    print(\"Proceeding with LLM invocation...\")\n",
    "    context_messages = [SystemMessage(content=KITCHEN_ASSISTANT_SYSINT[1])] + list(messages)\n",
    "    try:\n",
    "        ai_response: AIMessage = llm_with_callable_tools.invoke(context_messages)\n",
    "        print(f\"LLM Raw Response: {ai_response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Invocation Error: {e}\")\n",
    "        error_message = \"Sorry, I encountered an internal error trying to process that. Could you try rephrasing?\"\n",
    "        return { \"messages\": [AIMessage(content=error_message)], \"last_assistant_response\": error_message, \"intent\": \"error\", \"finished\": False, \"selected_recipe_id\": state.get(\"selected_recipe_id\"), \"current_recipe_details\": state.get(\"current_recipe_details\") }\n",
    "\n",
    "    # Prepare state updates based on LLM response\n",
    "    updates = {\n",
    "        \"messages\": [ai_response],\n",
    "        \"intent\": \"general_chat\", # Default intent\n",
    "        \"finished\": False,\n",
    "        \"last_assistant_response\": None,\n",
    "        \"needs_clarification\": False,\n",
    "        # Preserve context\n",
    "        \"selected_recipe_id\": state.get(\"selected_recipe_id\"),\n",
    "        \"current_recipe_details\": state.get(\"current_recipe_details\"),\n",
    "        \"recipe_reviews\": state.get(\"recipe_reviews\"),\n",
    "        \"live_recipe_details\": state.get(\"live_recipe_details\"),\n",
    "        # Clear transient fields\n",
    "        \"ingredient_nutrition_list\": None,\n",
    "        \"nutritional_info\": None,\n",
    "        \"processed_review_data\": None,\n",
    "        \"customization_results\": None,\n",
    "        \"grounding_results_formatted\": None,\n",
    "        \"customization_request\": None,\n",
    "    }\n",
    "\n",
    "    if ai_response.tool_calls:\n",
    "        updates[\"intent\"] = \"tool_call\" # General intent for routing to executor\n",
    "        print(f\"Intent: tool_call, Tool Calls: {ai_response.tool_calls}\\\")\")\n",
    "\n",
    "        # Context Management & Intent Setting based on Tool Calls\n",
    "        # ... (Keep the existing context management logic here) ...\n",
    "        new_search_initiated = any(tc.get('name') == 'gemini_recipe_similarity_search' for tc in ai_response.tool_calls)\n",
    "        if new_search_initiated:\n",
    "            print(\"New recipe search detected, clearing previous recipe context.\")\n",
    "            updates[\"current_recipe_details\"] = None; updates[\"selected_recipe_id\"] = None; updates[\"recipe_reviews\"] = None; updates[\"nutritional_info\"] = None; updates[\"live_recipe_details\"] = None; updates[\"processed_review_data\"] = None; updates[\"customization_results\"] = None\n",
    "\n",
    "        for tc in ai_response.tool_calls:\n",
    "            tool_name = tc.get('name'); tool_args = tc.get('args', {}); recipe_id_arg = tool_args.get('recipe_id')\n",
    "            if tool_name in ['get_recipe_by_id', 'get_ratings_and_reviews_by_recipe_id', 'customize_recipe', 'fetch_live_recipe_data'] and recipe_id_arg:\n",
    "                if recipe_id_arg != updates[\"selected_recipe_id\"]:\n",
    "                     print(f\"Tool call for new recipe ID '{recipe_id_arg}', updating context.\")\n",
    "                     updates[\"selected_recipe_id\"] = recipe_id_arg; updates[\"current_recipe_details\"] = None; updates[\"recipe_reviews\"] = None; updates[\"nutritional_info\"] = None; updates[\"live_recipe_details\"] = None; updates[\"processed_review_data\"] = None; updates[\"customization_results\"] = None\n",
    "                updates[\"selected_recipe_id\"] = recipe_id_arg # Ensure ID is set\n",
    "\n",
    "            # Set specific intents to guide routing *after* tool execution\n",
    "            # Note: get_recipe_by_id intent is now set above when processing the ToolMessage result\n",
    "            if tool_name == 'get_ratings_and_reviews_by_recipe_id': updates[\"intent\"] = \"reviews_fetched\"\n",
    "            elif tool_name == 'customize_recipe':\n",
    "                 updates[\"intent\"] = \"customization_requested\"\n",
    "                 updates[\"customization_request\"] = tool_args.get('request')\n",
    "                 if state.get(\"current_recipe_details\"):\n",
    "                     try: tool_args[\"recipe_details_json\"] = json.dumps(state[\"current_recipe_details\"])\n",
    "                     except Exception: print(\"Warning: Could not serialize details for customize_recipe tool.\")\n",
    "            elif tool_name == 'fetch_live_recipe_data': updates[\"intent\"] = \"live_data_requested\"\n",
    "            elif tool_name == 'fetch_nutrition_from_usda_fdc' and not any(t['name'] == 'fetch_nutrition_from_usda_fdc' for t in ai_response.tool_calls if t != tc): # Single nutrition call\n",
    "                 updates[\"intent\"] = \"single_nutrition_fetched\" # Let formatter handle simple display\n",
    "\n",
    "    elif ai_response.content:\n",
    "        # Handle direct text responses from LLM\n",
    "        updates[\"last_assistant_response\"] = ai_response.content\n",
    "        content_lower = ai_response.content.lower()\n",
    "        # ... (keep existing intent logic for text responses: clarification, exit, general_chat) ...\n",
    "        if \"need more details\" in content_lower or \"could you clarify\" in content_lower or \"which recipe\" in content_lower: updates[\"intent\"] = \"clarification_needed\"; updates[\"needs_clarification\"] = True\n",
    "        elif \"goodbye\" in content_lower or \"exit\" in content_lower or \"bye\" in content_lower: updates[\"intent\"] = \"exit\"; updates[\"finished\"] = True\n",
    "        elif state.get(\"user_input\", \"\").lower() in {\"q\", \"quit\", \"exit\", \"goodbye\"}: updates[\"intent\"] = \"exit\"; updates[\"finished\"] = True\n",
    "        else: updates[\"intent\"] = \"general_chat\"\n",
    "        print(f\"Intent: {updates['intent']}, Response: {updates['last_assistant_response'][:100]}...\")\n",
    "\n",
    "    else: # Handle LLM error or empty response\n",
    "        updates[\"intent\"] = \"error\"\n",
    "        error_message = \"Sorry, I had trouble processing that request. Can you please try again?\"\n",
    "        updates[\"last_assistant_response\"] = error_message\n",
    "        updates[\"messages\"] = [AIMessage(content=error_message)]\n",
    "        print(f\"Intent: error (Empty LLM response)\")\n",
    "\n",
    "    valid_keys = KitchenState.__annotations__.keys()\n",
    "    return {k: v for k, v in updates.items() if k in valid_keys and (k == 'messages' or state.get(k) != v)}\n",
    "\n",
    "\n",
    "print(\"âœ… LangGraph Step 4: Core Nodes Defined (Revised Input Parser for Recipe Details)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Step 5: Define Custom Action Nodes\n",
    "\n",
    "Defines the custom nodes responsible for processing the raw output from specific tools before the final response is formatted:\n",
    "*   `aggregate_nutrition_node`: Processes multiple `fetch_nutrition_from_usda_fdc` tool results, calculates averages, and updates the `nutritional_info` state.\n",
    "*   `visualize_nutrition_node`: Calls the plotting function if the final response contains nutrition data.\n",
    "*   `review_dashboard_node`: Processes raw review data, performs sentiment analysis, calculates rating breakdowns, and updates `processed_review_data`.\n",
    "*   `process_customization_node`: Parses the output from the `customize_recipe` tool and updates `customization_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:58:53.340276Z",
     "iopub.status.busy": "2025-04-18T14:58:53.339943Z",
     "iopub.status.idle": "2025-04-18T14:58:53.377284Z",
     "shell.execute_reply": "2025-04-18T14:58:53.376424Z",
     "shell.execute_reply.started": "2025-04-18T14:58:53.340252Z"
    }
   },
   "outputs": [],
   "source": [
    "# LangGraph Step 5: Specific Action Nodes (Revised - Restoring Missing Nodes)\n",
    "\n",
    "# --- Assume KitchenState, tools, constants, sentiment analyzer are defined ---\n",
    "# from step1_state import KitchenState (Revised)\n",
    "# from step3_tools import extract_and_visualize_nutrition, analyzer # (sentiment analyzer)\n",
    "# from step2_core import NUTRITION_RESPONSE_HEADER, REVIEW_DASHBOARD_HEADER, CUSTOMIZATION_HEADER (Revised)\n",
    "\n",
    "# --- Custom Action Nodes ---\n",
    "\n",
    "# ---> RESTORED: aggregate_nutrition_node (from original file) <---\n",
    "# Nutrition Aggregation Node (Revised for Robustness & Debugging in original file)\n",
    "def aggregate_nutrition_node(state: KitchenState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Aggregates nutrition data collected from fetch_nutrition_from_usda_fdc tool calls\n",
    "    since the last AI message that requested them. Calculates average values per 100g.\n",
    "    Updates the nutritional_info field in the state.\n",
    "    \"\"\"\n",
    "    print(\"---NODE: AggregateNutritionNode---\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    aggregated_sums: Dict[str, float] = defaultdict(float) # Use defaultdict\n",
    "    nutrient_counts: Dict[str, int] = defaultdict(int)\n",
    "    processed_ingredient_count = 0\n",
    "    unavailable_count = 0\n",
    "    error_count = 0 # Add error counter\n",
    "    relevant_tool_messages = []\n",
    "\n",
    "    # Find the last AI message that made nutrition tool calls\n",
    "    last_nutrition_request_index = -1\n",
    "    for i in range(len(messages) - 1, -1, -1):\n",
    "        msg = messages[i]\n",
    "        if isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "            if any(tc.get('name') == 'fetch_nutrition_from_usda_fdc' for tc in msg.tool_calls):\n",
    "                last_nutrition_request_index = i\n",
    "                break\n",
    "        # Stop searching if we hit the previous human message without finding an AI request\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "\n",
    "    # Collect all ToolMessages after that specific AI request\n",
    "    if last_nutrition_request_index != -1:\n",
    "        start_index = last_nutrition_request_index + 1\n",
    "        for i in range(start_index, len(messages)):\n",
    "            msg = messages[i]\n",
    "            if isinstance(msg, ToolMessage) and msg.name == \"fetch_nutrition_from_usda_fdc\":\n",
    "                relevant_tool_messages.append(msg)\n",
    "            # Stop collecting if we hit the next AI or Human message\n",
    "            elif isinstance(msg, (AIMessage, HumanMessage)):\n",
    "                 break\n",
    "    else:\n",
    "        # Fallback: Look for any nutrition tool messages in the current batch.\n",
    "        print(\"Warning: Could not find a preceding AI message requesting nutrition.\")\n",
    "        print(\"Scanning current message list for nutrition tool results...\")\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, ToolMessage) and msg.name == \"fetch_nutrition_from_usda_fdc\":\n",
    "                 relevant_tool_messages.append(msg)\n",
    "        if not relevant_tool_messages:\n",
    "             print(\"No nutrition tool messages found at all.\")\n",
    "             return {\"nutritional_info\": {\"processed_ingredient_count\": 0, \"nutrient_counts\": {}}}\n",
    "\n",
    "\n",
    "    print(f\"Found {len(relevant_tool_messages)} relevant nutrition ToolMessages to aggregate.\")\n",
    "    if not relevant_tool_messages:\n",
    "         print(\"No relevant tool messages found to aggregate.\")\n",
    "         return {\"nutritional_info\": {\"processed_ingredient_count\": 0, \"nutrient_counts\": {}}}\n",
    "\n",
    "\n",
    "    # Process the relevant messages\n",
    "    for msg in relevant_tool_messages:\n",
    "        print(f\"\\nDEBUG Aggregation: Processing ToolMessage ID {getattr(msg, 'tool_call_id', 'N/A')}\")\n",
    "        print(f\"DEBUG Aggregation: Raw Content: {msg.content}\")\n",
    "        try:\n",
    "            content_str = msg.content\n",
    "            if not isinstance(content_str, str):\n",
    "                 content_str = json.dumps(content_str)\n",
    "\n",
    "            content_data = json.loads(content_str)\n",
    "            ingredient_name = content_data.get(\"food_normalized\", \"Unknown Ingredient\")\n",
    "\n",
    "            if content_data.get(\"status\") == \"unavailable\":\n",
    "                unavailable_count += 1\n",
    "                print(f\"--> Skipping unavailable result for '{ingredient_name}': {content_data.get('reason', 'No reason provided')}\")\n",
    "                continue\n",
    "            elif \"error\" in content_data:\n",
    "                error_count += 1\n",
    "                print(f\"--> Skipping error result for '{ingredient_name}': {content_data.get('error', 'Unknown error')}\")\n",
    "                continue\n",
    "\n",
    "            core_nutrients = [\"calories_100g\", \"fat_100g\", \"proteins_100g\", \"carbohydrates_100g\"]\n",
    "            has_core_data = False\n",
    "            numeric_values_found = {}\n",
    "\n",
    "            for key in content_data.keys():\n",
    "                if key.endswith(\"_100g\") and key not in [\"food_normalized\", \"source\", \"product_name\", \"status\", \"reason\", \"error\", \"fdc_id\", \"data_type\"]:\n",
    "                    value = content_data.get(key)\n",
    "                    if value is not None:\n",
    "                        try:\n",
    "                            num_value = float(value)\n",
    "                            if num_value >= 0:\n",
    "                                aggregated_sums[key] += num_value\n",
    "                                nutrient_counts[key] += 1\n",
    "                                numeric_values_found[key] = num_value\n",
    "                                if key in core_nutrients:\n",
    "                                    has_core_data = True\n",
    "                            else:\n",
    "                                print(f\"--> Warning: Ignoring negative value '{num_value}' for key '{key}' in '{ingredient_name}'.\")\n",
    "                        except (ValueError, TypeError):\n",
    "                            print(f\"--> Warning: Could not convert value '{value}' for key '{key}' in '{ingredient_name}' to float.\")\n",
    "\n",
    "            if not has_core_data:\n",
    "                unavailable_count += 1\n",
    "                print(f\"--> Skipping result for '{ingredient_name}': Parsed OK, but no core numeric nutrition data found. Found: {numeric_values_found}\")\n",
    "                continue\n",
    "            else:\n",
    "                processed_ingredient_count += 1\n",
    "                print(f\"--> Successfully processed nutrition for: {ingredient_name}\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            error_count += 1\n",
    "            print(f\"--> ERROR: Could not parse ToolMessage content as JSON: {str(msg.content)[:100]}...\")\n",
    "        except Exception as e:\n",
    "             error_count += 1\n",
    "             print(f\"--> ERROR: Unexpected error processing ToolMessage ({getattr(msg, 'tool_call_id', 'N/A')}): {e}\")\n",
    "             import traceback\n",
    "             traceback.print_exc()\n",
    "\n",
    "    # Calculate averages\n",
    "    average_nutrition = {}\n",
    "    for key, total_sum in aggregated_sums.items():\n",
    "        count = nutrient_counts[key]\n",
    "        average_nutrition[key] = round(total_sum / count, 2) if count > 0 else 0.0\n",
    "\n",
    "    average_nutrition[\"processed_ingredient_count\"] = processed_ingredient_count\n",
    "    average_nutrition[\"unavailable_ingredient_count\"] = unavailable_count\n",
    "    average_nutrition[\"error_ingredient_count\"] = error_count\n",
    "    average_nutrition[\"nutrient_counts\"] = dict(nutrient_counts)\n",
    "\n",
    "    print(f\"\\nAggregation Complete. Processed: {processed_ingredient_count}, Unavailable/NoData: {unavailable_count}, Errors: {error_count}\")\n",
    "    print(f\"Aggregated Nutrition (Avg per 100g): {average_nutrition}\")\n",
    "\n",
    "    # Update state\n",
    "    return {\"nutritional_info\": average_nutrition, \"ingredient_nutrition_list\": None} # Clear temp list\n",
    "# ---> END RESTORED <---\n",
    "\n",
    "\n",
    "# ---> RESTORED: visualize_nutrition_node (from original file) <---\n",
    "# Visualization Node (Revised to CALL the function in original file)\n",
    "# ---> MODIFIED: visualize_nutrition_node to pass the header <---\n",
    "def visualize_nutrition_node(state: KitchenState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calls the nutrition visualization function using the final assistant response\n",
    "    if it contains the expected nutrition information header. Passes the header constant.\n",
    "    \"\"\"\n",
    "    print(\"---NODE: VisualizeNutritionNode---\")\n",
    "    final_response = state.get(\"last_assistant_response\")\n",
    "\n",
    "    # Ensure NUTRITION_RESPONSE_HEADER is accessible (defined in Step 2)\n",
    "    try:\n",
    "        header_check = NUTRITION_RESPONSE_HEADER\n",
    "    except NameError:\n",
    "        print(\"Warning: NUTRITION_RESPONSE_HEADER not found for visualization check.\")\n",
    "        header_check = None # Set to None if not found\n",
    "\n",
    "    if final_response and header_check and header_check in final_response: # Check header exists and is in response\n",
    "        print(\"Detected nutrition info in final response. Calling visualization function.\")\n",
    "        try:\n",
    "            # Call the visualization function defined/imported (e.g., from Step 3)\n",
    "            # Ensure extract_and_visualize_nutrition is defined or imported\n",
    "            # ---> Pass the header constant <---\n",
    "            extract_and_visualize_nutrition(final_response, header_check)\n",
    "            print(\"Visualization function executed.\")\n",
    "        except NameError:\n",
    "             print(\"ERROR: extract_and_visualize_nutrition function not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during visualization call: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # Print full traceback for plotting errors\n",
    "    else:\n",
    "        if not header_check:\n",
    "             print(\"Skipping visualization because NUTRITION_RESPONSE_HEADER is not defined.\")\n",
    "        else:\n",
    "             print(f\"No nutrition section header ('{header_check}') found in the final response, skipping visualization.\")\n",
    "\n",
    "    return {} # This node primarily performs a side effect\n",
    "# ---> END MODIFICATION <---\n",
    "\n",
    "\n",
    "\n",
    "# ---> ADDED: ReviewDashboardNode (from previous response) <---\n",
    "def review_dashboard_node(state: KitchenState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Processes raw review data from the state ('recipe_reviews'), performs sentiment analysis (if available),\n",
    "    calculates rating breakdown, selects reviews to display, and stores the processed data\n",
    "    in 'processed_review_data' for the formatter node.\n",
    "    \"\"\"\n",
    "    print(\"---NODE: ReviewDashboardNode---\")\n",
    "    raw_review_data = state.get(\"recipe_reviews\") # This should come from get_ratings_and_reviews tool output\n",
    "\n",
    "    if not raw_review_data or not isinstance(raw_review_data, dict):\n",
    "        print(\"No raw review data found in state.\")\n",
    "        return {\"processed_review_data\": None, \"intent\": \"reviews_processed_nodata\"} # Signal no data\n",
    "\n",
    "    processed_data = {\n",
    "        \"recipe_id\": raw_review_data.get(\"recipe_id\"),\n",
    "        \"overall_rating\": raw_review_data.get(\"overall_rating\"),\n",
    "        \"rating_counts\": {},\n",
    "        \"sentiment_scores\": {'positive': 0, 'negative': 0, 'neutral': 0},\n",
    "        \"reviews_for_display\": []\n",
    "    }\n",
    "\n",
    "    # Calculate rating breakdown from the sample\n",
    "    all_ratings = raw_review_data.get(\"all_ratings_sample\", [])\n",
    "    if all_ratings:\n",
    "        processed_data[\"rating_counts\"] = dict(Counter(all_ratings))\n",
    "\n",
    "    # Process recent reviews for sentiment and selection\n",
    "    recent_reviews = raw_review_data.get(\"recent_reviews\", [])\n",
    "    positive_reviews = []\n",
    "    negative_reviews = []\n",
    "    neutral_reviews = []\n",
    "\n",
    "    for review in recent_reviews:\n",
    "        text = review.get(\"review\", \"\")\n",
    "        rating = review.get(\"rating\")\n",
    "        sentiment = \"neutral\" # Default\n",
    "        sentiment_score = 0.0\n",
    "\n",
    "        if analyzer and text: # Check if analyzer was imported successfully\n",
    "            try:\n",
    "                vs = analyzer.polarity_scores(text)\n",
    "                sentiment_score = vs['compound']\n",
    "                if sentiment_score >= 0.05:\n",
    "                    sentiment = \"positive\"\n",
    "                    processed_data[\"sentiment_scores\"][\"positive\"] += 1\n",
    "                elif sentiment_score <= -0.05:\n",
    "                    sentiment = \"negative\"\n",
    "                    processed_data[\"sentiment_scores\"][\"negative\"] += 1\n",
    "                else:\n",
    "                    sentiment = \"neutral\"\n",
    "                    processed_data[\"sentiment_scores\"][\"neutral\"] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Sentiment analysis failed for review: {e}\")\n",
    "        elif rating is not None: # Fallback sentiment based on rating\n",
    "             if rating >= 4:\n",
    "                 sentiment = \"positive\"\n",
    "                 processed_data[\"sentiment_scores\"][\"positive\"] += 1\n",
    "             elif rating <= 2:\n",
    "                 sentiment = \"negative\"\n",
    "                 processed_data[\"sentiment_scores\"][\"negative\"] += 1\n",
    "             else:\n",
    "                 processed_data[\"sentiment_scores\"][\"neutral\"] += 1\n",
    "\n",
    "\n",
    "        review_details = {\n",
    "            \"review\": text,\n",
    "            \"rating\": rating,\n",
    "            \"date\": review.get(\"date\"),\n",
    "            \"sentiment\": sentiment,\n",
    "            \"sentiment_score\": sentiment_score # Store score for potential sorting\n",
    "        }\n",
    "\n",
    "        if sentiment == \"positive\":\n",
    "            positive_reviews.append(review_details)\n",
    "        elif sentiment == \"negative\":\n",
    "            negative_reviews.append(review_details)\n",
    "        else:\n",
    "            neutral_reviews.append(review_details)\n",
    "\n",
    "    # Select reviews: 3 positive, 2 negative (prioritize highest/lowest scores if available)\n",
    "    positive_reviews.sort(key=lambda x: x.get('sentiment_score', 0), reverse=True)\n",
    "    negative_reviews.sort(key=lambda x: x.get('sentiment_score', 0))\n",
    "\n",
    "    processed_data[\"reviews_for_display\"].extend(positive_reviews[:3])\n",
    "    processed_data[\"reviews_for_display\"].extend(negative_reviews[:2])\n",
    "\n",
    "    # If we don't have 5 reviews yet, add neutrals or remaining ones\n",
    "    needed = 5 - len(processed_data[\"reviews_for_display\"])\n",
    "    if needed > 0:\n",
    "        remaining_reviews = neutral_reviews + positive_reviews[3:] + negative_reviews[2:]\n",
    "        # Ensure no duplicates if a review was both neutral and in remaining pos/neg\n",
    "        seen_reviews = {r['review'] for r in processed_data[\"reviews_for_display\"]}\n",
    "        for r in remaining_reviews:\n",
    "            if len(processed_data[\"reviews_for_display\"]) >= 5: break\n",
    "            if r['review'] not in seen_reviews:\n",
    "                processed_data[\"reviews_for_display\"].append(r)\n",
    "                seen_reviews.add(r['review'])\n",
    "\n",
    "\n",
    "    print(f\"Processed review data: {len(processed_data['reviews_for_display'])} reviews selected.\")\n",
    "    # Update state\n",
    "    return {\n",
    "        \"processed_review_data\": processed_data,\n",
    "        \"intent\": \"reviews_processed\" # Signal for formatter\n",
    "        }\n",
    "# ---> END ADDITION <---\n",
    "\n",
    "\n",
    "# ---> ADDED: ProcessCustomizationNode (from previous response) <---\n",
    "def process_customization_node(state: KitchenState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Processes the JSON output from the 'customize_recipe' tool, storing the results\n",
    "    in 'customization_results' for the formatter node.\n",
    "    \"\"\"\n",
    "    print(\"---NODE: ProcessCustomizationNode---\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    customization_tool_result = None\n",
    "\n",
    "    # Find the last ToolMessage from customize_recipe\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, ToolMessage) and msg.name == \"customize_recipe\":\n",
    "            customization_tool_result = msg\n",
    "            break\n",
    "\n",
    "    if not customization_tool_result or not customization_tool_result.content:\n",
    "        print(\"No customization tool result found in messages.\")\n",
    "        return {\"customization_results\": None, \"intent\": \"customization_processed_nodata\"}\n",
    "\n",
    "    try:\n",
    "        # Ensure content is a string before loading JSON\n",
    "        content_str = customization_tool_result.content\n",
    "        if not isinstance(content_str, str):\n",
    "             content_str = json.dumps(content_str)\n",
    "\n",
    "        parsed_result = json.loads(content_str)\n",
    "        print(f\"Parsed customization result: {parsed_result.get('status')}\")\n",
    "\n",
    "        # Store the parsed result for the formatter\n",
    "        return {\n",
    "            \"customization_results\": parsed_result,\n",
    "            \"intent\": \"customization_processed\" # Signal for formatter\n",
    "        }\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ERROR: Could not parse customize_recipe tool result as JSON: {content_str[:100]}...\")\n",
    "        return {\"customization_results\": {\"status\": \"error\", \"message\": \"Failed to parse customization tool output.\"}, \"intent\": \"customization_error\"}\n",
    "    except Exception as e:\n",
    "         print(f\"ERROR: Unexpected error processing customization result: {e}\")\n",
    "         return {\"customization_results\": {\"status\": \"error\", \"message\": f\"Unexpected error: {e}\"}, \"intent\": \"customization_error\"}\n",
    "# ---> END ADDITION <---\n",
    "\n",
    "\n",
    "print(\"âœ… LangGraph Step 5: Custom Action Nodes Defined (Revised - Nodes Restored)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Step 6: Define Conditional Edge Functions\n",
    "\n",
    "Defines the functions that control the flow of the graph based on the agent's state:\n",
    "*   `route_after_parsing`: Determines the next node after the `InputParserNode`, routing to tool execution, specific action nodes, the formatter, or ending the turn based on the parsed intent or presence of tool calls.\n",
    "*   `route_after_formatting`: Decides whether to proceed to the `VisualizeNutritionNode` or end the turn after the `ResponseFormatterNode` has prepared the output, based on whether the output contains the nutrition header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:58:57.060320Z",
     "iopub.status.busy": "2025-04-18T14:58:57.059359Z",
     "iopub.status.idle": "2025-04-18T14:58:57.070669Z",
     "shell.execute_reply": "2025-04-18T14:58:57.069851Z",
     "shell.execute_reply.started": "2025-04-18T14:58:57.060266Z"
    }
   },
   "outputs": [],
   "source": [
    "# LangGraph Step 6: Conditional Edge Functions (Revised)\n",
    "\n",
    "# --- Assume KitchenState, constants are defined ---\n",
    "# from step1_state import KitchenState\n",
    "# from step2_core import NUTRITION_RESPONSE_HEADER\n",
    "\n",
    "# --- Conditional Edge Functions ---\n",
    "\n",
    "def route_after_parsing(state: KitchenState) -> Literal[\n",
    "    \"ToolExecutorNode\", \"AggregateNutritionNode\",\n",
    "    \"ResponseFormatterNode\", \"ReviewDashboardNode\", # ---> ADDED ReviewDashboardNode\n",
    "    \"ProcessCustomizationNode\", # ---> ADDED ProcessCustomizationNode\n",
    "    END\n",
    "]:\n",
    "    \"\"\"\n",
    "    Routes after the InputParserNode based on intent or presence of tool calls.\n",
    "    - Tool calls ('tool_call' intent) -> ToolExecutorNode\n",
    "    - Specific intents ('aggregate_nutrition', 'process_reviews', 'process_customization') -> Corresponding Node\n",
    "    - 'exit' intent -> END\n",
    "    - Otherwise (general chat, clarification, error, fetched data ready for formatter) -> ResponseFormatterNode\n",
    "    \"\"\"\n",
    "    print(\"---ROUTING (After Parsing)---\")\n",
    "    intent = state.get(\"intent\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    last_message = messages[-1] if messages else None\n",
    "    # Check for tool calls generated by the *parser* node itself\n",
    "    has_parser_tool_calls = isinstance(last_message, AIMessage) and bool(last_message.tool_calls)\n",
    "\n",
    "    print(f\"Routing based on: Intent='{intent}', HasParserToolCalls={has_parser_tool_calls}\")\n",
    "\n",
    "    if intent == \"aggregate_nutrition\":\n",
    "        print(\"Routing to: AggregateNutritionNode\")\n",
    "        return \"AggregateNutritionNode\"\n",
    "    # ---> ADDED Routing for review/customization processing <---\n",
    "    elif intent == \"process_reviews\":\n",
    "         print(\"Routing to: ReviewDashboardNode\")\n",
    "         return \"ReviewDashboardNode\"\n",
    "    elif intent == \"process_customization\":\n",
    "         print(\"Routing to: ProcessCustomizationNode\")\n",
    "         return \"ProcessCustomizationNode\"\n",
    "    # ---> END ADDITION <---\n",
    "    elif has_parser_tool_calls or intent == \"tool_call\": # If parser generated calls OR intent is explicitly tool_call\n",
    "        print(\"Routing to: ToolExecutorNode\")\n",
    "        return \"ToolExecutorNode\"\n",
    "    elif intent == \"exit\" or state.get(\"finished\"):\n",
    "        print(\"Routing to: END\")\n",
    "        return END\n",
    "    else: # general_chat, clarification_needed, error, or data fetched and ready for formatter (e.g., recipe_details_fetched, live_data_fetched)\n",
    "        print(\"Routing to: ResponseFormatterNode\")\n",
    "        return \"ResponseFormatterNode\"\n",
    "\n",
    "\n",
    "# ---> REMOVED route_after_action - Replaced by more specific routing from parser/nodes <---\n",
    "# The parser now directly routes ToolMessages to the appropriate processing node or back to the LLM if needed.\n",
    "# Action nodes (Aggregate, ReviewDashboard, ProcessCustomization) will route directly to the Formatter.\n",
    "\n",
    "def route_after_formatting(state: KitchenState) -> Literal[\"VisualizeNutritionNode\", END]:\n",
    "    \"\"\"\n",
    "    Decides whether to visualize nutrition data after formatting the response.\n",
    "    Checks if the final response STARTS WITH the specific nutrition header.\n",
    "    \"\"\"\n",
    "    print(\"---ROUTING (After Formatting)---\")\n",
    "    final_response = state.get(\"last_assistant_response\")\n",
    "\n",
    "    try:\n",
    "        header_check = NUTRITION_RESPONSE_HEADER\n",
    "    except NameError:\n",
    "        print(\"Warning: NUTRITION_RESPONSE_HEADER not found for routing check.\")\n",
    "        header_check = \"Here's the approximate average nutrition\" # Fallback\n",
    "\n",
    "    if final_response and final_response.strip().startswith(header_check):\n",
    "        print(\"Routing to: VisualizeNutritionNode\")\n",
    "        return \"VisualizeNutritionNode\"\n",
    "    else:\n",
    "        print(\"Routing to: END (No visualization needed)\")\n",
    "        return END\n",
    "\n",
    "\n",
    "print(\"âœ… LangGraph Step 6: Conditional Edge Functions Defined (Revised)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Step 7: Assemble and Compile Graph\n",
    "\n",
    "Builds the `StateGraph` by adding all the defined nodes (parser, executor, action nodes, formatter, visualizer) and connecting them with edges based on the defined routing functions (entry point, conditional edges after parsing and formatting, edges after tool execution and action nodes). Finally, it compiles the graph into a runnable `kitchen_assistant_graph` object and attempts to display its structure using Mermaid/Graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:59:01.107407Z",
     "iopub.status.busy": "2025-04-18T14:59:01.106793Z",
     "iopub.status.idle": "2025-04-18T14:59:01.215471Z",
     "shell.execute_reply": "2025-04-18T14:59:01.214516Z",
     "shell.execute_reply.started": "2025-04-18T14:59:01.107379Z"
    }
   },
   "outputs": [],
   "source": [
    "# LangGraph Step 7: Graph Assembly & Compilation (Revised)\n",
    "\n",
    "# --- Assume KitchenState, Nodes, Edges are defined ---\n",
    "# from step1_state import KitchenState (Revised)\n",
    "# from step3_5_nodes import input_parser_node, response_formatter_node # (Revised)\n",
    "# from step3_tools import tool_executor_node\n",
    "# from step4_actions import aggregate_nutrition_node, visualize_nutrition_node, review_dashboard_node, process_customization_node # (Revised/Added)\n",
    "# from step5_routing import route_after_parsing, route_after_formatting # (Revised)\n",
    "\n",
    "# --- Graph Assembly ---\n",
    "graph_builder = StateGraph(KitchenState)\n",
    "\n",
    "# Add Nodes (Ensure names match function names)\n",
    "graph_builder.add_node(\"InputParserNode\", input_parser_node)\n",
    "graph_builder.add_node(\"ToolExecutorNode\", tool_executor_node)\n",
    "graph_builder.add_node(\"AggregateNutritionNode\", aggregate_nutrition_node)\n",
    "graph_builder.add_node(\"ResponseFormatterNode\", response_formatter_node)\n",
    "graph_builder.add_node(\"VisualizeNutritionNode\", visualize_nutrition_node)\n",
    "# ---> ADDED New Nodes <---\n",
    "graph_builder.add_node(\"ReviewDashboardNode\", review_dashboard_node)\n",
    "graph_builder.add_node(\"ProcessCustomizationNode\", process_customization_node)\n",
    "# ---> END ADDITION <---\n",
    "\n",
    "# Define Entry Point\n",
    "graph_builder.add_edge(START, \"InputParserNode\")\n",
    "\n",
    "# Define Conditional Edges from Parser\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"InputParserNode\",\n",
    "    route_after_parsing,\n",
    "    {\n",
    "        \"ToolExecutorNode\": \"ToolExecutorNode\",\n",
    "        \"AggregateNutritionNode\": \"AggregateNutritionNode\",\n",
    "        \"ReviewDashboardNode\": \"ReviewDashboardNode\", # ---> ADDED <---\n",
    "        \"ProcessCustomizationNode\": \"ProcessCustomizationNode\", # ---> ADDED <---\n",
    "        \"ResponseFormatterNode\": \"ResponseFormatterNode\", # Handles chat, errors, fetched data\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# ---> REVISED: Edges After Tool Execution <---\n",
    "# ToolExecutorNode output (ToolMessages) goes BACK to InputParserNode\n",
    "# The parser will then route based on the tool name to the appropriate processing node\n",
    "# or back to the LLM if the tool result needs interpretation.\n",
    "graph_builder.add_edge(\"ToolExecutorNode\", \"InputParserNode\")\n",
    "# ---> END REVISION <---\n",
    "\n",
    "\n",
    "# ---> ADDED: Edges After Processing Nodes <---\n",
    "# After aggregation, review processing, or customization processing, format the response\n",
    "graph_builder.add_edge(\"AggregateNutritionNode\", \"ResponseFormatterNode\")\n",
    "graph_builder.add_edge(\"ReviewDashboardNode\", \"ResponseFormatterNode\")\n",
    "graph_builder.add_edge(\"ProcessCustomizationNode\", \"ResponseFormatterNode\")\n",
    "# ---> END ADDITION <---\n",
    "\n",
    "# After formatting, decide whether to visualize or end the turn\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"ResponseFormatterNode\",\n",
    "    route_after_formatting,\n",
    "    {\n",
    "        \"VisualizeNutritionNode\": \"VisualizeNutritionNode\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# After visualization, the graph run ends for this turn\n",
    "graph_builder.add_edge(\"VisualizeNutritionNode\", END)\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "kitchen_assistant_graph = graph_builder.compile()\n",
    "\n",
    "print(\"\\nâœ… LangGraph Step 7: Graph Compiled Successfully! (Revised Flow)\")\n",
    "\n",
    "# Visualize the updated graph\n",
    "try:\n",
    "    png_data = kitchen_assistant_graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(png_data))\n",
    "    print(\"Graph visualization displayed.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nGraph visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Step 8: Setup UI Integration\n",
    "\n",
    "Sets up the interactive user interface simulation using `ipywidgets`. It initializes a global `conversation_state` dictionary, creates widgets for text input, voice selection (dropdown with placeholder paths), submit buttons, and separate output areas for chat history and debug logging. It defines the `run_graph_and_display` function which handles taking user input (text or transcribed voice), invoking the compiled LangGraph agent (`kitchen_assistant_graph`), capturing the final state, and displaying the user/AI interaction in the chat history output area while logging graph steps in the debug output area. Event handlers (`on_text_submit`, `on_voice_submit`) are defined and linked to the buttons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:59:07.880003Z",
     "iopub.status.busy": "2025-04-18T14:59:07.879171Z",
     "iopub.status.idle": "2025-04-18T14:59:07.923418Z",
     "shell.execute_reply": "2025-04-18T14:59:07.922323Z",
     "shell.execute_reply.started": "2025-04-18T14:59:07.879975Z"
    }
   },
   "outputs": [],
   "source": [
    "# LangGraph Step 8: User Interface Integration (Revised)\n",
    "\n",
    "# --- Assume KitchenState, Graph, transcribe_audio are defined ---\n",
    "# from step1_state import KitchenState (Revised)\n",
    "# from step6_graph import kitchen_assistant_graph (Revised)\n",
    "# from step0_utils import transcribe_audio # Or wherever it's defined\n",
    "\n",
    "# --- UI Simulation using ipywidgets ---\n",
    "\n",
    "# Conversation state (global for this simple example)\n",
    "# Reset state for UI interaction\n",
    "conversation_state: KitchenState = {\n",
    "    \"messages\": [], \"user_input\": None, \"audio_file_path\": None, \"intent\": None,\n",
    "    \"selected_recipe_id\": None, \"customization_request\": None, \"nutrition_query\": None,\n",
    "    \"grounding_query\": None, \"current_recipe_details\": None, \"recipe_reviews\": None,\n",
    "    \"ingredient_nutrition_list\": None, \"live_recipe_details\": None, \"nutritional_info\": None,\n",
    "    \"processed_review_data\": None, \"customization_results\": None, \"grounding_results_formatted\": None,\n",
    "    \"user_ingredients\": [], \"dietary_preferences\": [],\n",
    "    \"needs_clarification\": False, \"finished\": False, \"last_assistant_response\": None,\n",
    "}\n",
    "\n",
    "# Widgets\n",
    "text_input = widgets.Textarea(description=\"You:\", layout={'width': '90%', 'height': '80px'}) # Adjusted height\n",
    "text_submit_button = widgets.Button(description=\"Send Text\")\n",
    "# Define voice options (use actual paths accessible to your environment)\n",
    "default_voice_path = \"/kaggle/input/some-audio-dataset/audio.ogg\" # Example placeholder\n",
    "voice_options = [\n",
    "    (\"Select Voice...\", None),\n",
    "    # Add paths to your actual .ogg or .wav files here\n",
    "    # (\"Pizza Search (Simulated)\", \"path/to/your/pizza_search.ogg\"),\n",
    "]\n",
    "# Filter options if needed\n",
    "# valid_voice_options = [(name, path) for name, path in voice_options if path is None or os.path.exists(path)]\n",
    "# voice_dropdown = widgets.Dropdown(options=valid_voice_options, description=\"Voice:\")\n",
    "voice_dropdown = widgets.Dropdown(options=voice_options, description=\"Voice:\") # Using original for now\n",
    "voice_submit_button = widgets.Button(description=\"Process Voice\")\n",
    "\n",
    "# ---> REVISED: Output Area for Chat History <---\n",
    "chat_history_output = widgets.Output(layout={\n",
    "    'border': '1px solid black',\n",
    "    'height': '400px',\n",
    "    'overflow_y': 'scroll',\n",
    "    'width': '90%'\n",
    "})\n",
    "\n",
    "# ---> REVISED: Separate Output for Debug Info <---\n",
    "debug_output = widgets.Output(layout={\n",
    "    'border': '1px solid blue',\n",
    "    'height': '150px', # Increased height slightly\n",
    "    'overflow_y': 'scroll',\n",
    "    'width': '90%'\n",
    "})\n",
    "\n",
    "# Display initial welcome message in the chat history\n",
    "with chat_history_output:\n",
    "    display(Markdown(\"**Assistant:** Welcome! Ask me about recipes, ingredients, or nutrition.\"))\n",
    "    # Optionally add to state if needed for graph context on first turn\n",
    "    # conversation_state[\"messages\"].append(AIMessage(content=\"Welcome! Ask me about recipes, ingredients, or nutrition.\"))\n",
    "    # conversation_state[\"last_assistant_response\"] = \"Welcome! Ask me about recipes, ingredients, or nutrition.\"\n",
    "\n",
    "\n",
    "# --- Interaction Logic (Revised for better display and Markdown) ---\n",
    "def run_graph_and_display(initial_state_update: Dict[str, Any]):\n",
    "    global conversation_state\n",
    "\n",
    "    # 1. Prepare input for the graph\n",
    "    current_messages = list(conversation_state.get(\"messages\", []))\n",
    "    if \"messages\" in initial_state_update:\n",
    "        # Don't add the human message here yet, add it to the display first\n",
    "        pass\n",
    "\n",
    "    input_for_graph = conversation_state.copy()\n",
    "    input_for_graph.update(initial_state_update)\n",
    "    # Pass the *existing* history plus the *new* human message to the graph\n",
    "    input_for_graph[\"messages\"] = current_messages + initial_state_update.get(\"messages\", [])\n",
    "    input_for_graph[\"intent\"] = None\n",
    "    input_for_graph[\"last_assistant_response\"] = None\n",
    "    input_for_graph[\"nutritional_info\"] = None # Clear previous results\n",
    "    input_for_graph[\"processed_review_data\"] = None\n",
    "    input_for_graph[\"customization_results\"] = None\n",
    "    input_for_graph[\"live_recipe_details\"] = None\n",
    "    input_for_graph[\"needs_clarification\"] = False\n",
    "    # Keep context like selected_recipe_id\n",
    "\n",
    "    # ---> REVISED: Display Logic - Append latest interaction <---\n",
    "    user_input_text = initial_state_update.get(\"user_input\", \"\")\n",
    "    with chat_history_output:\n",
    "        # Display the user's input for this turn\n",
    "        display(Markdown(f\"**You:** {user_input_text}\"))\n",
    "        # Display a thinking indicator\n",
    "        thinking_output = widgets.Output()\n",
    "        with thinking_output:\n",
    "             display(Markdown(\"**Assistant:** Thinking...\"))\n",
    "        display(thinking_output)\n",
    "    # ---> END REVISION <---\n",
    "\n",
    "    # 2. Stream graph execution (or use invoke)\n",
    "    final_state_after_run = None\n",
    "    assistant_response_to_display = \"...\" # Default thinking message\n",
    "    error_occurred = None\n",
    "\n",
    "    # Clear previous debug info\n",
    "    with debug_output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"--- Running Graph ---\")\n",
    "\n",
    "    try:\n",
    "        # Use stream to observe intermediate steps in debug output\n",
    "        for step_state in kitchen_assistant_graph.stream(input_for_graph, {\"recursion_limit\": 25}):\n",
    "            node_name = list(step_state.keys())[0]\n",
    "            current_state_snapshot = step_state[node_name]\n",
    "\n",
    "            # --- Debugging Output ---\n",
    "            with debug_output:\n",
    "                 # print(f\"\\n--- Step: {node_name} ---\") # Keep adding steps\n",
    "                 # Simple print of node name is often enough\n",
    "                 print(f\"-> {node_name}\")\n",
    "                 # Optionally print more details like intent, last message etc.\n",
    "                 # print(f\"  Intent: {current_state_snapshot.get('intent')}\")\n",
    "                 # last_msg = current_state_snapshot.get('messages', [])[-1] if current_state_snapshot.get('messages') else None\n",
    "                 # if last_msg: print(f\"  Last Msg: {type(last_msg).__name__}\")\n",
    "            # --- End Debugging ---\n",
    "\n",
    "            # Update global state progressively\n",
    "            conversation_state.update(current_state_snapshot)\n",
    "            final_state_after_run = conversation_state # Keep track of the latest full state\n",
    "\n",
    "            if conversation_state.get(\"finished\", False):\n",
    "                with debug_output: print(\"--- Finished flag set, ending stream early. ---\")\n",
    "                break\n",
    "\n",
    "        # After the stream finishes (or breaks)\n",
    "        if final_state_after_run:\n",
    "            conversation_state.update(final_state_after_run) # Ensure final state is captured\n",
    "            assistant_response_to_display = conversation_state.get(\"last_assistant_response\", \"Okay, what next?\")\n",
    "            if conversation_state.get(\"finished\"):\n",
    "                 assistant_response_to_display = conversation_state.get(\"last_assistant_response\", \"Goodbye!\")\n",
    "        else:\n",
    "             assistant_response_to_display = \"Something went wrong during processing (no final state).\"\n",
    "             error_occurred = \"No final state returned from graph.\"\n",
    "             conversation_state[\"last_assistant_response\"] = assistant_response_to_display\n",
    "             if \"messages\" not in conversation_state: conversation_state[\"messages\"] = []\n",
    "             conversation_state[\"messages\"].append(AIMessage(content=assistant_response_to_display))\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        assistant_response_to_display = f\"An error occurred: {e}\"\n",
    "        error_occurred = str(e)\n",
    "        print(f\"ERROR during graph execution: {e}\")\n",
    "        import traceback\n",
    "        with debug_output: # Print traceback to debug area\n",
    "            traceback.print_exc()\n",
    "        conversation_state[\"last_assistant_response\"] = assistant_response_to_display\n",
    "        if \"messages\" not in conversation_state: conversation_state[\"messages\"] = []\n",
    "        # Add error as AI message to history\n",
    "        conversation_state[\"messages\"].append(AIMessage(content=f\"Error: {e}\"))\n",
    "\n",
    "    # 3. Display the final AI response for this turn\n",
    "    # ---> REVISED: Display Logic - Update thinking message with final response <---\n",
    "    with chat_history_output:\n",
    "        thinking_output.clear_output() # Remove \"Thinking...\"\n",
    "        if assistant_response_to_display:\n",
    "            display(Markdown(f\"**Assistant:**\\n\\n{assistant_response_to_display}\"))\n",
    "        elif error_occurred:\n",
    "             display(Markdown(f\"**Assistant:** Sorry, an error occurred: {error_occurred}\"))\n",
    "        else:\n",
    "             display(Markdown(f\"**Assistant:** (No response content generated)\"))\n",
    "        # Add a separator for clarity\n",
    "        display(Markdown(\"---\"))\n",
    "    # ---> END REVISION <---\n",
    "\n",
    "    # 4. Visualization (Matplotlib) is handled INSIDE the graph by VisualizeNutritionNode\n",
    "    # and should appear in the standard cell output area below the widgets.\n",
    "\n",
    "# --- Event Handlers (No changes needed here, they call the revised run_graph_and_display) ---\n",
    "def on_text_submit(b):\n",
    "    user_text = text_input.value\n",
    "    if not user_text: return\n",
    "    initial_update = {\n",
    "        \"user_input\": user_text,\n",
    "        \"messages\": [HumanMessage(content=user_text)], # Pass the new message\n",
    "        \"finished\": False\n",
    "        }\n",
    "    text_input.value = \"\" # Clear input\n",
    "    run_graph_and_display(initial_update)\n",
    "\n",
    "def on_voice_submit(b):\n",
    "     selected_file = voice_dropdown.value\n",
    "     if not selected_file:\n",
    "         with chat_history_output: display(Markdown(\"**Assistant:** Please select a voice file.\\n\\n---\")); return\n",
    "     if not os.path.exists(selected_file):\n",
    "          with chat_history_output: display(Markdown(f\"**Assistant:** Error - Voice file not found: {selected_file}\\n\\n---\")); return\n",
    "\n",
    "     # Display processing message in chat\n",
    "     with chat_history_output:\n",
    "         display(Markdown(f\"*(Processing voice file: {os.path.basename(selected_file)}...)*\"))\n",
    "         thinking_output = widgets.Output()\n",
    "         with thinking_output: display(Markdown(\"**Assistant:** Transcribing...\"))\n",
    "         display(thinking_output)\n",
    "\n",
    "     transcribed_text = \"Error: Transcription setup failed.\"\n",
    "     # ... (Keep the transcription logic from your original Step 7) ...\n",
    "     try:\n",
    "         google_creds = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "         openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "         if google_creds and os.path.exists(google_creds):\n",
    "              try:\n",
    "                  import io\n",
    "                  from google.cloud import speech\n",
    "                  print(\"Attempting transcription with Google Cloud Speech...\")\n",
    "                  # Ensure transcribe_audio function is defined/imported\n",
    "                  transcribed_text = transcribe_audio(service=\"google\", file_path=selected_file, credentials_path=google_creds)\n",
    "              except ImportError:\n",
    "                   print(\"Google Cloud Speech library not installed.\")\n",
    "                   transcribed_text = \"Error: Google Speech library missing.\"\n",
    "              except Exception as google_err:\n",
    "                   print(f\"Google transcription failed: {google_err}\")\n",
    "                   transcribed_text = f\"Error: Google transcription failed - {google_err}\"\n",
    "         elif openai_key:\n",
    "              try:\n",
    "                  from openai import OpenAI\n",
    "                  print(\"Attempting transcription with OpenAI Whisper...\")\n",
    "                  # Ensure transcribe_audio function is defined/imported\n",
    "                  transcribed_text = transcribe_audio(service=\"openai\", file_path=selected_file, api_key=openai_key)\n",
    "              except ImportError:\n",
    "                   print(\"OpenAI library not installed.\")\n",
    "                   transcribed_text = \"Error: OpenAI library missing.\"\n",
    "              except Exception as openai_err:\n",
    "                   print(f\"OpenAI transcription failed: {openai_err}\")\n",
    "                   transcribed_text = f\"Error: OpenAI transcription failed - {openai_err}\"\n",
    "         else:\n",
    "              print(\"Neither Google Credentials nor OpenAI API Key found.\")\n",
    "              transcribed_text = \"Error: No transcription service configured.\"\n",
    "     except NameError:\n",
    "          print(\"ERROR: transcribe_audio function not defined.\")\n",
    "          transcribed_text = \"Error: Transcription function missing.\"\n",
    "     except Exception as e:\n",
    "          print(f\"Unexpected error during transcription setup: {e}\")\n",
    "          transcribed_text = f\"Error: Transcription failed unexpectedly - {e}\"\n",
    "\n",
    "\n",
    "     # Clear transcribing message\n",
    "     with chat_history_output:\n",
    "         thinking_output.clear_output()\n",
    "\n",
    "     if \"Error:\" in transcribed_text:\n",
    "          with chat_history_output: display(Markdown(f\"**Assistant:** Transcription failed - {transcribed_text}\\n\\n---\"))\n",
    "          return\n",
    "\n",
    "     # Display transcribed text as user input before running graph\n",
    "     with chat_history_output:\n",
    "         display(Markdown(f\"**You (from voice):** {transcribed_text}\"))\n",
    "\n",
    "     initial_update = {\n",
    "         \"user_input\": transcribed_text,\n",
    "         \"messages\": [HumanMessage(content=transcribed_text)], # Pass the new message\n",
    "         \"audio_file_path\": selected_file,\n",
    "         \"finished\": False\n",
    "     }\n",
    "     voice_dropdown.value = None # Reset dropdown\n",
    "     run_graph_and_display(initial_update) # Run graph with transcribed text\n",
    "\n",
    "# Assign callbacks\n",
    "text_submit_button.on_click(on_text_submit)\n",
    "voice_submit_button.on_click(on_voice_submit)\n",
    "\n",
    "# Display Widgets\n",
    "print(\"--- Kitchen Assistant Interface ---\")\n",
    "\n",
    "# ---> REVISED: Display layout <---\n",
    "# display(widgets.VBox([\n",
    "#     widgets.HTML(\"<b>Enter request via text or select voice file:</b>\"),\n",
    "#     widgets.HBox([text_input, text_submit_button]), # Text input and button side-by-side\n",
    "#     widgets.HBox([voice_dropdown, voice_submit_button]), # Voice input and button side-by-side\n",
    "#     widgets.HTML(\"<hr><b>Conversation:</b>\"),\n",
    "#     chat_history_output, # Use the dedicated chat history output\n",
    "#     widgets.HTML(\"<hr><b>Debug Log (Graph Steps):</b>\"),\n",
    "#     debug_output\n",
    "# ]))\n",
    "\n",
    "print(\"âœ… LangGraph Step 8: UI Integration Setup Complete (Revised)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Step 9: Define Chat Helper Functions\n",
    "\n",
    "Defines helper functions for interacting with the compiled graph, primarily for testing and potentially for backend integration:\n",
    "1.  `suppress_stdout`: A context manager to temporarily hide print statements (used to make chat output cleaner).\n",
    "2.  `chat_with_assistant`: Takes user input and the current state, runs one turn of the graph using `invoke`, suppresses internal node prints, displays only the user input and final AI response using Markdown, and returns the updated state.\n",
    "3.  `get_assistant_response_json`: Similar to `chat_with_assistant` but returns the key interaction details (input, response, error, state summary) as a JSON string, suppressing internal prints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:59:12.541875Z",
     "iopub.status.busy": "2025-04-18T14:59:12.541133Z",
     "iopub.status.idle": "2025-04-18T14:59:12.565927Z",
     "shell.execute_reply": "2025-04-18T14:59:12.564615Z",
     "shell.execute_reply.started": "2025-04-18T14:59:12.541808Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- LangGraph Step 9: Chat Helper Function & JSON Output (Revised Display) ---\n",
    "\n",
    "\n",
    "# Utility to suppress stdout (keep as is)\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    # ... (keep existing code) ...\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = StringIO()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "# --- Chat Helper with Cleaner Markdown Output ---\n",
    "def chat_with_assistant(user_msg: str, state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Runs a turn of the conversation with the LangGraph agent.\n",
    "    Displays ONLY the user's input and the AI's final response for this turn,\n",
    "    formatted as Markdown.\n",
    "    Returns the updated state dictionary containing the full message history.\n",
    "    \"\"\"\n",
    "    if not state:\n",
    "        state = {\"messages\": []}\n",
    "    elif \"messages\" not in state:\n",
    "        state[\"messages\"] = []\n",
    "\n",
    "    # Prepare input state for the graph\n",
    "    current_messages = state.get(\"messages\", [])\n",
    "    # ---> Add the human message to the history that goes INTO the graph <---\n",
    "    input_messages = current_messages + [HumanMessage(content=user_msg)]\n",
    "\n",
    "    input_for_graph = state.copy()\n",
    "    input_for_graph[\"messages\"] = input_messages\n",
    "    input_for_graph[\"user_input\"] = user_msg\n",
    "    input_for_graph[\"finished\"] = False\n",
    "    # Clear fields that the graph should determine\n",
    "    input_for_graph[\"intent\"] = None\n",
    "    input_for_graph[\"last_assistant_response\"] = None\n",
    "    input_for_graph[\"nutritional_info\"] = None\n",
    "    input_for_graph[\"processed_review_data\"] = None\n",
    "    input_for_graph[\"customization_results\"] = None\n",
    "    input_for_graph[\"live_recipe_details\"] = None\n",
    "    input_for_graph[\"needs_clarification\"] = False\n",
    "    # Keep context (like selected_recipe_id, current_recipe_details) from previous state\n",
    "\n",
    "    final_state = {}\n",
    "    print(f\"\\n--- Invoking Graph for: '{user_msg[:50]}...' ---\")\n",
    "\n",
    "    # ---> Keep suppression for cleaner notebook output during run <---\n",
    "    # But ensure errors are still printed outside\n",
    "    error_message = None\n",
    "    with suppress_stdout():\n",
    "        try:\n",
    "            # Invoke the graph\n",
    "            final_state = kitchen_assistant_graph.invoke(input_for_graph, {\"recursion_limit\": 25})\n",
    "        except Exception as e:\n",
    "            error_message = f\"ERROR during graph invocation: {e}\"\n",
    "            print(error_message) # Print error outside suppressed block\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Create minimal error state to return\n",
    "            final_state = input_for_graph.copy() # Start with input state\n",
    "            error_content = f\"Sorry, an error occurred: {e}\"\n",
    "            # Add the error message to the history\n",
    "            final_state[\"messages\"] = final_state.get(\"messages\", []) + [AIMessage(content=error_content)]\n",
    "            final_state[\"last_assistant_response\"] = error_content\n",
    "            final_state[\"intent\"] = \"error\"\n",
    "\n",
    "    print(\"--- Graph Invocation Complete ---\")\n",
    "\n",
    "    # --- Display ONLY the final User/AI interaction for this turn using Markdown ---\n",
    "\n",
    "    # ---> GET the final AI response reliably from the returned state <---\n",
    "    # It should have been set correctly by the revised response_formatter_node\n",
    "    ai_response_text = final_state.get(\"last_assistant_response\", None)\n",
    "\n",
    "    # Fallback: If last_assistant_response is missing, try getting content from the last AIMessage\n",
    "    if not ai_response_text and final_state.get(\"messages\"):\n",
    "         last_ai = next((msg for msg in reversed(final_state[\"messages\"]) if isinstance(msg, AIMessage) and getattr(msg, 'content', None)), None)\n",
    "         if last_ai:\n",
    "             ai_response_text = last_ai.content\n",
    "             print(\"Warning: Using content from last AIMessage as last_assistant_response was missing.\") # Debug info\n",
    "\n",
    "    # Display the user message for this turn\n",
    "    display(Markdown(f\"**ðŸ§‘ You:**\\n\\n{user_msg}\"))\n",
    "\n",
    "    # Display the AI response for this turn\n",
    "    if ai_response_text:\n",
    "        # ---> Ensure response is treated as a string before displaying <---\n",
    "        display(Markdown(f\"**ðŸ¤– AI:**\\n\\n{str(ai_response_text)}\"))\n",
    "    elif error_message: # If invoke failed\n",
    "         display(Markdown(f\"**ðŸ¤– AI:**\\n\\n(Graph execution error: {error_message})\"))\n",
    "    else:\n",
    "        display(Markdown(\"**ðŸ¤– AI:**\\n\\n(No response content generated or an error occurred)\"))\n",
    "\n",
    "    # ---> Return the final state containing the full history for the next turn <---\n",
    "    # Make sure the final state has the correct message history\n",
    "    # The response_formatter_node should now return a state with messages = [final_AIMessage]\n",
    "    # We need to add this final AIMessage back to the history carried over from the input state\n",
    "    if \"messages\" in final_state and isinstance(final_state[\"messages\"], list) and len(final_state[\"messages\"]) == 1 and isinstance(final_state[\"messages\"][0], AIMessage):\n",
    "         # Correct: formatter returned only the last AI message. Reconstruct history.\n",
    "         final_state[\"messages\"] = input_messages + final_state[\"messages\"]\n",
    "    elif error_message and \"messages\" in final_state:\n",
    "        # Correct: error message was already added to input_messages history\n",
    "        pass\n",
    "    else:\n",
    "         # Fallback/Warning: If message history is not as expected, log it\n",
    "         #print(f\"Warning: Message history structure from graph might be unexpected. Type: {type(final_state.get('messages'))}, Length: {len(final_state.get('messages', []))}\")\n",
    "         # Attempt to preserve input history + add the text response if possible\n",
    "         if ai_response_text and not error_message:\n",
    "             final_state[\"messages\"] = input_messages + [AIMessage(content=str(ai_response_text))]\n",
    "         else: # Preserve input history at least\n",
    "             final_state[\"messages\"] = input_messages\n",
    "\n",
    "\n",
    "    return final_state\n",
    "\n",
    "# --- JSON Output Function (Add similar safety checks) ---\n",
    "def get_assistant_response_json(user_msg: str, state: dict) -> str:\n",
    "    \"\"\"\n",
    "    Runs a turn of the conversation and returns the key information\n",
    "    (user input, AI response, final state summary) as a JSON string.\n",
    "    \"\"\"\n",
    "    # ... (Input state preparation - same as chat_with_assistant) ...\n",
    "    if not state: state = {\"messages\": []}\n",
    "    elif \"messages\" not in state: state[\"messages\"] = []\n",
    "    current_messages = state.get(\"messages\", [])\n",
    "    input_messages = current_messages + [HumanMessage(content=user_msg)]\n",
    "    input_for_graph = state.copy(); input_for_graph[\"messages\"] = input_messages\n",
    "    input_for_graph[\"user_input\"] = user_msg; input_for_graph[\"finished\"] = False\n",
    "    input_for_graph[\"intent\"] = None; input_for_graph[\"last_assistant_response\"] = None # etc.\n",
    "\n",
    "    final_state = {}\n",
    "    error_message = None\n",
    "    with suppress_stdout(): # Suppress internal prints\n",
    "        try:\n",
    "            final_state = kitchen_assistant_graph.invoke(input_for_graph, {\"recursion_limit\": 50})\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            final_state = input_for_graph.copy()\n",
    "            final_state[\"intent\"] = \"error\"\n",
    "            final_state[\"last_assistant_response\"] = f\"Error: {e}\"\n",
    "            # Add error to message history for completeness if needed for JSON output\n",
    "            final_state[\"messages\"] = final_state.get(\"messages\", []) + [AIMessage(content=f\"Error: {e}\")]\n",
    "\n",
    "\n",
    "    # Extract relevant info\n",
    "    ai_response_text = final_state.get(\"last_assistant_response\", None)\n",
    "    if not ai_response_text and not error_message and final_state.get(\"messages\"):\n",
    "         last_ai = next((msg for msg in reversed(final_state[\"messages\"]) if isinstance(msg, AIMessage) and getattr(msg, 'content', None)), None)\n",
    "         if last_ai: ai_response_text = last_ai.content\n",
    "\n",
    "    # ---> Ensure state_summary accesses fields safely using .get() <---\n",
    "    state_summary = {\n",
    "        \"intent\": final_state.get(\"intent\"),\n",
    "        \"selected_recipe_id\": final_state.get(\"selected_recipe_id\"),\n",
    "        \"needs_clarification\": final_state.get(\"needs_clarification\", False),\n",
    "        \"finished\": final_state.get(\"finished\", False),\n",
    "        \"has_recipe_details\": bool(final_state.get(\"current_recipe_details\")),\n",
    "        \"has_live_details\": bool(final_state.get(\"live_recipe_details\")),\n",
    "        \"has_reviews\": bool(final_state.get(\"recipe_reviews\")), # Raw reviews might still be present\n",
    "        \"has_processed_reviews\": bool(final_state.get(\"processed_review_data\")),\n",
    "        \"has_nutrition\": bool(final_state.get(\"nutritional_info\")),\n",
    "        \"has_customization\": bool(final_state.get(\"customization_results\")),\n",
    "    }\n",
    "\n",
    "    # Construct final output JSON data\n",
    "    output_data = {\n",
    "        \"user_input\": user_msg,\n",
    "        \"ai_response\": str(ai_response_text) if not error_message and ai_response_text else None,\n",
    "        \"error\": error_message,\n",
    "        \"state_summary\": state_summary,\n",
    "        # Optionally include message history, converting BaseMessage objects\n",
    "        # \"message_history\": [msg.dict() for msg in final_state.get(\"messages\", [])] # Be careful with serialization\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Use default=str for better serialization safety\n",
    "        return json.dumps(output_data, indent=2, default=str)\n",
    "    except TypeError as e:\n",
    "        return json.dumps({\"error\": \"Failed to serialize final state to JSON.\", \"details\": str(e)}, indent=2)\n",
    "\n",
    "\n",
    "print(\"âœ… LangGraph Step 9: Chat Helper Function & JSON Output Defined (Revised Display)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Conversation State\n",
    "\n",
    "Initializes an empty dictionary `state` to hold the conversation state before starting the chat interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:59:16.698692Z",
     "iopub.status.busy": "2025-04-18T14:59:16.697658Z",
     "iopub.status.idle": "2025-04-18T14:59:16.702570Z",
     "shell.execute_reply": "2025-04-18T14:59:16.701766Z",
     "shell.execute_reply.started": "2025-04-18T14:59:16.698664Z"
    }
   },
   "outputs": [],
   "source": [
    "state = {}  # Initialize empty state first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Scenario 1: Initial Greeting\n",
    "\n",
    "Runs the first turn of the conversation using the `chat_with_assistant` helper. Sends a greeting (\"Hello, what can you do?\") to the agent and displays the interaction. Updates the `state` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:59:18.815391Z",
     "iopub.status.busy": "2025-04-18T14:59:18.814768Z",
     "iopub.status.idle": "2025-04-18T14:59:19.853059Z",
     "shell.execute_reply": "2025-04-18T14:59:19.852361Z",
     "shell.execute_reply.started": "2025-04-18T14:59:18.815364Z"
    }
   },
   "outputs": [],
   "source": [
    "state = chat_with_assistant(\"Hello, what can you do?\", state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Scenario 2: Recipe Search (Vegetarian Soup)\n",
    "\n",
    "Runs the second turn. Asks the agent to find vegetarian soup recipes using the `gemini_recipe_similarity_search` tool (implicitly). Displays the interaction and updates the `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:59:23.639925Z",
     "iopub.status.busy": "2025-04-18T14:59:23.639595Z",
     "iopub.status.idle": "2025-04-18T15:00:46.366727Z",
     "shell.execute_reply": "2025-04-18T15:00:46.366015Z",
     "shell.execute_reply.started": "2025-04-18T14:59:23.639900Z"
    }
   },
   "outputs": [],
   "source": [
    "state = chat_with_assistant(\"Find me vegeterian soup recipes and tag the recipes. 5 recipes\", state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Scenario 3: Get Recipe Details\n",
    "\n",
    "Runs the third turn. Asks for details about the third recipe from the previous list, triggering the `get_recipe_by_id` tool based on context. Displays the interaction (Recipe Dashboard) and updates the `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:45:23.897648Z",
     "iopub.status.busy": "2025-04-18T14:45:23.897226Z",
     "iopub.status.idle": "2025-04-18T14:45:25.043667Z",
     "shell.execute_reply": "2025-04-18T14:45:25.042938Z",
     "shell.execute_reply.started": "2025-04-18T14:45:23.897620Z"
    }
   },
   "outputs": [],
   "source": [
    "state = chat_with_assistant(\"I like to know about third recipe in the list but not review\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Scenario 4: Get Reviews\n",
    "\n",
    "Runs the fourth turn. Asks for reviews of the current recipe (Greek Barley Soup), triggering the `get_ratings_and_reviews_by_recipe_id` tool and the `ReviewDashboardNode`. Displays the interaction (Review Dashboard) and updates the `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:45:30.033095Z",
     "iopub.status.busy": "2025-04-18T14:45:30.032797Z",
     "iopub.status.idle": "2025-04-18T14:45:31.638213Z",
     "shell.execute_reply": "2025-04-18T14:45:31.637352Z",
     "shell.execute_reply.started": "2025-04-18T14:45:30.033077Z"
    }
   },
   "outputs": [],
   "source": [
    "state = chat_with_assistant(\"Show the recipe reviews\", state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Scenario 5: Get Recipe Nutrition\n",
    "\n",
    "Runs the fifth turn. Asks for nutrition information for the current recipe, triggering multiple calls to `fetch_nutrition_from_usda_fdc` followed by the `AggregateNutritionNode` and `VisualizeNutritionNode`. Displays the interaction (Nutrition Summary and plot) and updates the `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:45:45.001467Z",
     "iopub.status.busy": "2025-04-18T14:45:45.001127Z",
     "iopub.status.idle": "2025-04-18T14:45:49.892189Z",
     "shell.execute_reply": "2025-04-18T14:45:49.891239Z",
     "shell.execute_reply.started": "2025-04-18T14:45:45.001444Z"
    }
   },
   "outputs": [],
   "source": [
    "state = chat_with_assistant(\"Get nutriotion information for this recipe\", state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:46:04.998043Z",
     "iopub.status.busy": "2025-04-18T14:46:04.996966Z",
     "iopub.status.idle": "2025-04-18T14:46:06.348149Z",
     "shell.execute_reply": "2025-04-18T14:46:06.347443Z",
     "shell.execute_reply.started": "2025-04-18T14:46:04.998009Z"
    }
   },
   "outputs": [],
   "source": [
    "state = chat_with_assistant(\"Run the nutrition analysis for the recipe we just discussed.\", state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:29:20.440280Z",
     "iopub.status.busy": "2025-04-18T13:29:20.440007Z",
     "iopub.status.idle": "2025-04-18T13:29:20.444356Z",
     "shell.execute_reply": "2025-04-18T13:29:20.443533Z",
     "shell.execute_reply.started": "2025-04-18T13:29:20.440261Z"
    }
   },
   "outputs": [],
   "source": [
    "# state = chat_with_assistant(\"yes it is that same recipe\", state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Scenario 6: Recipe Customization\n",
    "\n",
    "Runs the sixth turn. Asks to make the current recipe healthier/low-fat, triggering the `customize_recipe` tool and the `ProcessCustomizationNode`. Displays the interaction (Customization Suggestions) and updates the `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:46:21.613018Z",
     "iopub.status.busy": "2025-04-18T14:46:21.612361Z",
     "iopub.status.idle": "2025-04-18T14:46:22.739150Z",
     "shell.execute_reply": "2025-04-18T14:46:22.738361Z",
     "shell.execute_reply.started": "2025-04-18T14:46:21.612994Z"
    }
   },
   "outputs": [],
   "source": [
    "state = chat_with_assistant(\"make this recipe more healthy for low fat diet\", state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Scenario 7: General Question (Grounding)\n",
    "\n",
    "Runs the seventh turn. Asks a general question about egg yolk substitutes, triggering the LLM's internal knowledge and potentially the built-in Google Search grounding. Displays the interaction and updates the `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T14:46:26.310308Z",
     "iopub.status.busy": "2025-04-18T14:46:26.309963Z",
     "iopub.status.idle": "2025-04-18T14:46:27.860737Z",
     "shell.execute_reply": "2025-04-18T14:46:27.859940Z",
     "shell.execute_reply.started": "2025-04-18T14:46:26.310269Z"
    }
   },
   "outputs": [],
   "source": [
    "state = chat_with_assistant(\"What's a good substitute for egg yolks\", state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Final Message History\n",
    "\n",
    "Prints the final list of messages stored in the `state` dictionary after the test conversation sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:22.369857Z",
     "iopub.status.busy": "2025-04-18T13:54:22.368886Z",
     "iopub.status.idle": "2025-04-18T13:54:22.376389Z",
     "shell.execute_reply": "2025-04-18T13:54:22.375364Z",
     "shell.execute_reply.started": "2025-04-18T13:54:22.369825Z"
    }
   },
   "outputs": [],
   "source": [
    "state[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:29:23.273671Z",
     "iopub.status.busy": "2025-04-18T13:29:23.273285Z",
     "iopub.status.idle": "2025-04-18T13:29:23.291745Z",
     "shell.execute_reply": "2025-04-18T13:29:23.290665Z",
     "shell.execute_reply.started": "2025-04-18T13:29:23.273642Z"
    }
   },
   "outputs": [],
   "source": [
    "# LangGraph Step 10: Testing Voice Input with JSON Output\n",
    "\n",
    "\n",
    "\n",
    "# --- Assume transcribe_audio and get_assistant_response_json are defined ---\n",
    "# Make sure these functions are defined in previous cells or imported\n",
    "\n",
    "# --- Voice-to-JSON Helper Function ---\n",
    "def chat_with_voice_and_get_json(\n",
    "    audio_file_path: str,\n",
    "    state: dict,\n",
    "    service: str = \"google\", # Default to google as used in your test\n",
    "    language: str = \"en\",\n",
    "    api_key: Optional[str] = None, # For OpenAI\n",
    "    credentials_path: Optional[str] = None, # For Google Path\n",
    "    credentials_json: Optional[str] = None # For Google JSON String (if used)\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Transcribes an audio file, runs the transcription through the agent,\n",
    "    and returns key interaction details as a JSON string.\n",
    "    Uses the provided 'state' for context during the graph run for this turn,\n",
    "    but does NOT return the updated state dictionary.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Voice-to-JSON process for: {audio_file_path} ---\")\n",
    "\n",
    "    # 1. Validate Audio File Path\n",
    "    if not audio_file_path:\n",
    "        print(\"ERROR: Audio file path is required.\")\n",
    "        return json.dumps({\"error\": \"Audio file path is required.\"}, indent=2)\n",
    "    if not os.path.exists(audio_file_path):\n",
    "        print(f\"ERROR: Audio file not found at: {audio_file_path}\")\n",
    "        return json.dumps({\"error\": f\"Audio file not found at: {audio_file_path}\"}, indent=2)\n",
    "\n",
    "    # 2. Transcribe Audio\n",
    "    print(f\"--- Transcribing audio file using {service}... ---\")\n",
    "    transcribed_text = transcribe_audio(\n",
    "        service=service,\n",
    "        file_path=audio_file_path,\n",
    "        language=language,\n",
    "        api_key=api_key,\n",
    "        credentials_path=credentials_path,\n",
    "        credentials_json=credentials_json\n",
    "    )\n",
    "    print(f\"--- Transcription complete ---\")\n",
    "\n",
    "    # 3. Handle Transcription Errors\n",
    "    if not transcribed_text or transcribed_text.startswith(\"Error:\"):\n",
    "        print(f\"Transcription Error/Empty: {transcribed_text}\")\n",
    "        return json.dumps({\n",
    "            \"user_input_source\": \"voice\",\n",
    "            \"audio_file\": audio_file_path,\n",
    "            \"transcription_error\": transcribed_text,\n",
    "            \"ai_response\": None,\n",
    "            \"error\": \"Transcription failed\",\n",
    "            \"state_summary\": None\n",
    "            }, indent=2)\n",
    "\n",
    "    print(f\"Transcribed Text: {transcribed_text}\")\n",
    "\n",
    "    # 4. Invoke Graph via get_assistant_response_json\n",
    "    # This function internally invokes the graph using the provided 'state' for context.\n",
    "    print(f\"--- Invoking Graph with transcribed text ('{transcribed_text[:50]}...') for JSON output ---\")\n",
    "    json_output = get_assistant_response_json(user_msg=transcribed_text, state=state)\n",
    "    print(f\"--- Graph invocation for JSON complete ---\")\n",
    "\n",
    "    # 5. Add transcribed text to the output JSON and return\n",
    "    try:\n",
    "        output_data = json.loads(json_output)\n",
    "        output_data[\"transcribed_input\"] = transcribed_text # Add the transcription\n",
    "        return json.dumps(output_data, indent=2, default=str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Could not parse the JSON output from get_assistant_response_json\")\n",
    "        return json.dumps({\n",
    "            \"error\": \"Failed to parse internal JSON response\",\n",
    "            \"raw_output\": json_output,\n",
    "            \"transcribed_input\": transcribed_text\n",
    "            }, indent=2)\n",
    "    except Exception as e:\n",
    "         print(f\"Unexpected error modifying JSON output: {e}\")\n",
    "         return json.dumps({\n",
    "            \"error\": f\"Unexpected error processing JSON: {e}\",\n",
    "            \"raw_output\": json_output,\n",
    "            \"transcribed_input\": transcribed_text\n",
    "            }, indent=2)\n",
    "\n",
    "print(\"âœ… LangGraph Step 10: Voice-to-JSON Helper Function Defined\")\n",
    "\n",
    "# --- Test Scenario: Voice Input to JSON Output ---\n",
    "\n",
    "# Initialize or use the existing state from previous text interactions\n",
    "# If you ran the text scenarios in Step 9, 'state' will contain that history.\n",
    "# state = {} # Uncomment to start with a fresh, empty state\n",
    "\n",
    "# Define the path to your audio file\n",
    "# Make sure this path is correct for your environment\n",
    "audio_file = \"/home/snowholt/coding/python/google_capstone/voices/Nariman_1.ogg\"\n",
    "\n",
    "# Define credentials/keys (ensure these are correctly set in your environment/secrets)\n",
    "\n",
    "\n",
    "# Choose the service ('google' or 'openai')\n",
    "transcription_service = \"google\" # Or \"openai\"\n",
    "\n",
    "# Call the new function\n",
    "# if transcription_service == \"google\":\n",
    "#     if not google_creds_path or not os.path.exists(google_creds_path):\n",
    "#          print(\"ERROR: GOOGLE_APPLICATION_CREDENTIALS path not set or invalid.\")\n",
    "#          json_result = json.dumps({\"error\": \"Google credentials path missing or invalid.\"}, indent=2)\n",
    "#     else:\n",
    "#         json_result = chat_with_voice_and_get_json(\n",
    "#             audio_file_path=audio_file,\n",
    "#             state=state, # Pass the current state for context\n",
    "#             service=\"google\",\n",
    "#             language=\"en-US\", # Use appropriate code for Google\n",
    "#             credentials_path=google_creds_path\n",
    "#         )\n",
    "# elif transcription_service == \"openai\":\n",
    "#      if not openai_api_key:\n",
    "#          print(\"ERROR: OPENAI_API_KEY not set.\")\n",
    "#          json_result = json.dumps({\"error\": \"OpenAI API key missing.\"}, indent=2)\n",
    "#      else:\n",
    "#          json_result = chat_with_voice_and_get_json(\n",
    "#             audio_file_path=audio_file,\n",
    "#             state=state, # Pass the current state for context\n",
    "#             service=\"openai\",\n",
    "#             language=\"en\", # Use appropriate code for OpenAI\n",
    "#             api_key=openai_api_key\n",
    "#         )\n",
    "# else:\n",
    "#     json_result = json.dumps({\"error\": \"Invalid transcription service selected\"}, indent=2)\n",
    "\n",
    "\n",
    "# # Print the resulting JSON\n",
    "# print(\"\\n--- JSON Output from Voice Interaction ---\")\n",
    "# print(json_result)\n",
    "\n",
    "# # --- Note on State Update ---\n",
    "# # The 'state' variable in this notebook cell *has not* been updated by the\n",
    "# # chat_with_voice_and_get_json call because it returns a JSON string, not the state dict.\n",
    "# # To continue the conversation programmatically after this voice turn, you would need to:\n",
    "# # 1. Modify chat_with_voice_and_get_json to return the full state dict.\n",
    "# # OR\n",
    "# # 2. Manually parse 'json_result' and update the 'state' dictionary with the new messages.\n",
    "# # OR\n",
    "# # 3. Use the alternative approach: transcribe first, then call chat_with_assistant.\n",
    "# print(\"\\n--- Note: Notebook 'state' variable is not updated by this function call. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 311962,
     "sourceId": 783630,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7053270,
     "sourceId": 11281977,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7086722,
     "sourceId": 11328936,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7110204,
     "sourceId": 11360382,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
