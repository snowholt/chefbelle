{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57806c9c",
   "metadata": {},
   "source": [
    "# Interactive Recipe & Kitchen Management Assistant\n",
    "\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The Interactive Recipe & Kitchen Management Assistant helps users:\n",
    "1. Discover recipes based on available ingredients\n",
    "2. Customize recipes according to dietary needs\n",
    "3. Receive step-by-step cooking guidance\n",
    "\n",
    "This assistant will use multiple Gen AI capabilities including:\n",
    "- Audio understanding (for voice input)\n",
    "- Few-shot prompting (for recipe customization)\n",
    "- Function calling (for specific recipe operations)\n",
    "- RAG (Retrieval Augmented Generation for recipe knowledge)\n",
    "- Grounding (using web search for supplemental information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93275b-5c3b-4781-8316-363181657b45",
   "metadata": {},
   "source": [
    "## Step 1: Data Source & Setup\n",
    "\n",
    "This notebook implements the first step of our Interactive Recipe & Kitchen Management Assistant capstone project for the Google Gen AI Intensive Course. We'll acquire, explore, and prepare the recipe dataset that will serve as the foundation for our recipe retrieval and customization system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e715f",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "Let's start by installing and importing the necessary libraries for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a33b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and install compatible versions\n",
    "#!pip uninstall -y tensorflow protobuf google-api-core google-cloud-automl google-generativeai google-cloud-translate chromadb\n",
    "!pip uninstall -qqy kfp > /dev/null 2>&1\n",
    "\n",
    "# Install chromadb with compatible versions\n",
    "!pip install -qU --no-warn-conflicts \"google-genai==1.7.0\" chromadb==0.6.3 \n",
    "# #!pip install -U google-api-core==2.16.0\n",
    "\n",
    "!pip install -q --no-warn-conflicts google-cloud-speech\n",
    "\n",
    "# Install base packages with minimal dependencies\n",
    "!pip install -q --no-warn-conflicts pandas matplotlib seaborn \n",
    "!pip install -q --no-warn-conflicts kagglehub[pandas-datasets]\n",
    "!pip install -q --no-warn-conflicts soundfile pydub ipywidgets openai\n",
    "\n",
    "# Install compatible versions\n",
    "#!pip install -q google-generativeai  # Latest version instead of 1.7.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14276a53-d819-401e-ad49-7067f742df8c",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Now let's import the libraries we'll need for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18c5abd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Audio libraries could not be imported: No module named 'sounddevice'\n",
      "Google Cloud Speech-to-Text is imported successfully!\n",
      "Environment setup complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "from openai import OpenAI\n",
    "import io\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, clear_output, display, HTML, clear_output, Markdown\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "\n",
    "# Set warnings filter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing libraries with error handling\n",
    "try:\n",
    "    import soundfile as sf\n",
    "    import sounddevice as sd\n",
    "    from IPython.display import Audio, display\n",
    "    AUDIO_LIBRARIES_AVAILABLE = True\n",
    "    print(\"Audio libraries imported successfully!\")\n",
    "except (ImportError, OSError) as e:\n",
    "    print(f\"Warning: Audio libraries could not be imported: {e}\")\n",
    "\n",
    "# Google Cloud Speech-to-Text (with error handling)\n",
    "try:\n",
    "    from google.cloud import speech\n",
    "    GOOGLE_SPEECH_AVAILABLE = True\n",
    "    print(\"Google Cloud Speech-to-Text is imported successfully!\")\n",
    "except ImportError:\n",
    "    GOOGLE_SPEECH_AVAILABLE = False\n",
    "    print(\"Google Cloud Speech-to-Text not available. Will use simulation for speech recognition.\")\n",
    "\n",
    "# Google Gemini API for natural language understanding\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import retry\n",
    "\n",
    "# Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e802dadf-774b-4095-9a9a-096b3aaa6040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path:\n",
      "/usr/lib/python312.zip\n",
      "/usr/lib/python3.12\n",
      "/usr/lib/python3.12/lib-dynload\n",
      "\n",
      "/home/snowholt/venv/analytic/lib/python3.12/site-packages\n",
      "\n",
      "chromadb imported as: <class 'module'>\n",
      "chromadb location: /home/snowholt/venv/analytic/lib/python3.12/site-packages/chromadb/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check Python paths\n",
    "print(\"Python path:\")\n",
    "for path in sys.path:\n",
    "    print(path)\n",
    "\n",
    "# Try to find chromadb\n",
    "try:\n",
    "    import chromadb\n",
    "    print(f\"\\nchromadb imported as: {type(chromadb)}\")\n",
    "    print(f\"chromadb location: {chromadb.__file__}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError importing chromadb: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b5d70-ba95-4a88-b08f-c4cf0e1fc39c",
   "metadata": {},
   "source": [
    "### Set up your API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`, `GOOGLE_APPLICATION_CREDENTIALS`, `OPENAI_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.\n",
    "\n",
    "Furthermore, for the Google Cloud Client Libraries (like the google-cloud-speech Python library you're using), you generally cannot authenticate using only an API Key. ðŸš«ðŸ”‘, So you need to provide and import Service Account Credentials (JSON Key File)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb15654-b70f-4895-bb39-99a4b23e7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "# OPENAI_API_KEY = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n",
    "# SecretValueJson = UserSecretsClient().get_secret(\"GOOGLE_APPLICATION_CREDENTIALS\") # Use the label you gave the secret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cee7b89-aa37-4ea1-acb0-28ca5089ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists: True\n",
      "OpenAI API Key exists: True\n",
      "SecretValueJson API Key exists: True\n"
     ]
    }
   ],
   "source": [
    "# Import the os module to access environment variables\n",
    "\n",
    "# Access environment variables\n",
    "def get_api_key(key_name):\n",
    "    \"\"\"\n",
    "    Retrieve an API key from environment variables.\n",
    "    \n",
    "    Args:\n",
    "        key_name (str): The name of the environment variable containing the API key\n",
    "        \n",
    "    Returns:\n",
    "        str: The API key or None if it doesn't exist\n",
    "    \"\"\"\n",
    "    api_key = os.environ.get(key_name)\n",
    "    \n",
    "    if api_key is None:\n",
    "        print(f\"Warning: {key_name} environment variable not found.\")\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "# Example usage\n",
    "GOOGLE_API_KEY = get_api_key(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY = get_api_key(\"OPENAI_API_KEY\")\n",
    "SecretValueJson=get_api_key(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "# Check if keys exist\n",
    "print(f\"Google API Key exists: {GOOGLE_API_KEY is not None}\")\n",
    "print(f\"OpenAI API Key exists: {OPENAI_API_KEY is not None}\")\n",
    "print(f\"SecretValueJson API Key exists: {SecretValueJson is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8aaa08",
   "metadata": {},
   "source": [
    "\n",
    "## Data Loading\n",
    "\n",
    "### Importing the Dataset in Kaggle\n",
    "\n",
    "Since you're using Kaggle, you can easily import the Food.com Recipes dataset directly:\n",
    "\n",
    "1. Search for \"Food.com Recipes and User Interactions\" in the Kaggle datasets section\n",
    "2. Or use this direct link: https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions\n",
    "\n",
    "In Kaggle, you can either:\n",
    "- Add the dataset to your notebook directly from the \"Add data\" button in the right sidebar\n",
    "- Use the Kaggle datasets API as shown below\n",
    "\n",
    "\n",
    "We'll use the Food.com Recipes and Interactions dataset. This contains recipe information including ingredients, steps, and user interactions.\n",
    "\n",
    "If you've downloaded the dataset using the Kaggle API, uncomment and use the data loading code below. Otherwise, we'll use a direct URL to access the data.\n",
    "\n",
    "loading both the vectorized and raw data and nutritional breakdown dataset that will be used in subsequent steps, particularly for the few-shot prompting recipe customization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f483204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 231637 recipes\n",
      "Successfully loaded 1132367 interactions\n",
      "Successfully loaded nutritional dataset with 3454 records\n",
      "Successfully parsed ingredients column\n",
      "Successfully parsed steps column\n",
      "Successfully parsed tags column\n",
      "\n",
      "Dataset successfully processed\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Direct Kaggle dataset import\n",
    "# This is the easiest way to import datasets in Kaggle notebooks\n",
    "\n",
    "try:\n",
    "    # If the dataset is added via the \"Add data\" button, it will be available at /kaggle/input/\n",
    "    recipes_df = pd.read_csv('/home/snowholt/coding/python/google_capstone/datasets/RAW_recipes.csv')\n",
    "    interactions_df = pd.read_csv('/home/snowholt/coding/python/google_capstone/datasets/RAW_interactions.csv')\n",
    "    nutrition_df = pd.read_csv('/home/snowholt/coding/python/google_capstone/datasets/cleaned_nutrition_dataset.csv')\n",
    "    print(f\"Successfully loaded {len(recipes_df)} recipes\")\n",
    "    print(f\"Successfully loaded {len(interactions_df)} interactions\")\n",
    "    print(f\"Successfully loaded nutritional dataset with {len(nutrition_df)} records\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset files not found. Please make sure you've added the dataset to your Kaggle notebook.\")\n",
    "    print(\"You can add it by clicking the 'Add data' button in the right sidebar.\")\n",
    "    print(\"Alternatively, you can use direct URLs if available.\")\n",
    "\n",
    "# Let's parse the JSON strings in the columns that contain lists\n",
    "if 'recipes_df' in locals():\n",
    "    # Check the actual structure of the dataframe\n",
    "    \n",
    "    # For Food.com dataset, ingredients, steps, and tags are stored as strings that represent lists\n",
    "    # We need to convert them from string representation to actual Python lists\n",
    "    try:\n",
    "        if 'ingredients' in recipes_df.columns:\n",
    "            recipes_df['ingredients'] = recipes_df['ingredients'].apply(eval)\n",
    "            print(\"Successfully parsed ingredients column\")\n",
    "        \n",
    "        if 'steps' in recipes_df.columns:\n",
    "            recipes_df['steps'] = recipes_df['steps'].apply(eval)\n",
    "            print(\"Successfully parsed steps column\")\n",
    "        \n",
    "        if 'tags' in recipes_df.columns:\n",
    "            recipes_df['tags'] = recipes_df['tags'].apply(eval)\n",
    "            print(\"Successfully parsed tags column\")\n",
    "            \n",
    "            # Add cuisine type based on tags\n",
    "            recipes_df['cuisine_type'] = recipes_df['tags'].apply(\n",
    "                lambda x: next((tag for tag in x if tag in ['italian', 'persian', 'mexican', 'chinese', 'indian', 'french', 'thai']), 'other')\n",
    "            )\n",
    "        \n",
    "      \n",
    "        # Count number of ingredients\n",
    "        recipes_df['n_ingredients'] = recipes_df['ingredients'].apply(len)\n",
    "            \n",
    "        print(\"\\nDataset successfully processed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {e}\")\n",
    "        print(\"Column sample values:\")\n",
    "        for col in recipes_df.columns:\n",
    "            print(f\"{col}: {recipes_df[col].iloc[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e892e82",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's explore the dataset to understand its structure and content. This will help us plan our cleaning and preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a440e3-ba61-491b-b254-83e24baf611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA ANALYSIS FOR ALL DATAFRAMES ===\n",
      "\n",
      "------------------------------\n",
      "Analysis for Recipes:\n",
      "------------------------------\n",
      "\n",
      "Data types:\n",
      "name: object\n",
      "id: int64\n",
      "minutes: int64\n",
      "contributor_id: int64\n",
      "submitted: object\n",
      "tags: object\n",
      "nutrition: object\n",
      "n_steps: int64\n",
      "steps: object\n",
      "description: object\n",
      "ingredients: object\n",
      "n_ingredients: int64\n",
      "cuisine_type: object\n",
      "\n",
      "Missing values per column:\n",
      "name: 1 missing values (0.00%)\n",
      "description: 4979 missing values (2.15%)\n",
      "\n",
      "Numeric columns summary:\n",
      "                   count          mean   min           max\n",
      "id              231637.0  2.220147e+05  38.0  5.377160e+05\n",
      "minutes         231637.0  9.398546e+03   0.0  2.147484e+09\n",
      "contributor_id  231637.0  5.534885e+06  27.0  2.002290e+09\n",
      "n_steps         231637.0  9.765499e+00   0.0  1.450000e+02\n",
      "n_ingredients   231637.0  9.051153e+00   1.0  4.300000e+01\n",
      "\n",
      "------------------------------\n",
      "Analysis for Interactions:\n",
      "------------------------------\n",
      "\n",
      "Data types:\n",
      "user_id: int64\n",
      "recipe_id: int64\n",
      "date: object\n",
      "rating: int64\n",
      "review: object\n",
      "\n",
      "Missing values per column:\n",
      "review: 169 missing values (0.01%)\n",
      "\n",
      "Numeric columns summary:\n",
      "               count          mean     min           max\n",
      "user_id    1132367.0  1.384291e+08  1533.0  2.002373e+09\n",
      "recipe_id  1132367.0  1.608972e+05    38.0  5.377160e+05\n",
      "rating     1132367.0  4.411016e+00     0.0  5.000000e+00\n",
      "\n",
      "------------------------------\n",
      "Analysis for Nutrition:\n",
      "------------------------------\n",
      "\n",
      "Data types:\n",
      "Vitamin C: float64\n",
      "Vitamin B11: float64\n",
      "Sodium: float64\n",
      "Calcium: float64\n",
      "Carbohydrates: float64\n",
      "food: object\n",
      "Iron: float64\n",
      "Caloric Value: float64\n",
      "Sugars: float64\n",
      "Dietary Fiber: float64\n",
      "Fat: float64\n",
      "Protein: float64\n",
      "food_normalized: object\n",
      "\n",
      "Missing values per column:\n",
      "\n",
      "Numeric columns summary:\n",
      "                count        mean  min       max\n",
      "Vitamin C      3454.0    9.501021  0.0   3872.00\n",
      "Vitamin B11    3454.0   11.447140  0.0    550.52\n",
      "Sodium         3454.0  141.437806  0.0  14174.59\n",
      "Calcium        3454.0   53.953960  0.0   1283.50\n",
      "Carbohydrates  3454.0   18.724570  0.0    390.20\n",
      "Iron           3454.0    1.627954  0.0    121.20\n",
      "Caloric Value  3454.0  227.591546  0.0   6077.00\n",
      "Sugars         3454.0    5.693513  0.0    291.50\n",
      "Dietary Fiber  3454.0    2.157695  0.0     76.50\n",
      "Fat            3454.0   12.006807  0.0    550.70\n",
      "Protein        3454.0   10.819314  0.0    560.30\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vectorized_recipes_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m analyze_dataframe(interactions_df, \u001b[33m\"\u001b[39m\u001b[33mInteractions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m analyze_dataframe(nutrition_df, \u001b[33m\"\u001b[39m\u001b[33mNutrition\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m analyze_dataframe(\u001b[43mvectorized_recipes_df\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mVectorized Recipes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m analyze_dataframe(vectorized_users_df, \u001b[33m\"\u001b[39m\u001b[33mVectorized Users\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorized_recipes_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to analyze dataframe properties\n",
    "def analyze_dataframe(df, df_name):\n",
    "    print(f\"\\n{'-' * 30}\")\n",
    "    print(f\"Analysis for {df_name}:\")\n",
    "    print(f\"{'-' * 30}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(\"\\nData types:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].dtype}\")\n",
    "    \n",
    "    # Check missing values\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    for col, missing in zip(missing_values.index, missing_values.values):\n",
    "        if missing > 0:\n",
    "            print(f\"{col}: {missing} missing values ({missing/len(df):.2%})\")\n",
    "    \n",
    "    # Summary statistics for numeric columns\n",
    "    print(\"\\nNumeric columns summary:\")\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    if numeric_cols:\n",
    "        # Show basic stats for numeric columns only\n",
    "        print(df[numeric_cols].describe().T[['count', 'mean', 'min', 'max']])\n",
    "    else:\n",
    "        print(\"No numeric columns found\")\n",
    "\n",
    "# Analyze all dataframes\n",
    "print(\"\\n=== DATA ANALYSIS FOR ALL DATAFRAMES ===\")\n",
    "analyze_dataframe(recipes_df, \"Recipes\")\n",
    "analyze_dataframe(interactions_df, \"Interactions\")\n",
    "analyze_dataframe(nutrition_df, \"Nutrition\")\n",
    "analyze_dataframe(vectorized_recipes_df, \"Vectorized Recipes\")\n",
    "analyze_dataframe(vectorized_users_df, \"Vectorized Users\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09174682-b8e2-4b0f-b712-072496eaf5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a few rows instead of full stats\n",
    "print(\"\\nSample rows:\")\n",
    "print(recipes_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266570ee-5772-4c3b-afd3-71121cb561fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of cuisine types\n",
    "plt.figure(figsize=(12, 6))\n",
    "if 'cuisine_type' in recipes_df.columns:\n",
    "    # Limit to top 15 cuisines to avoid cluttered plot\n",
    "    recipes_df['cuisine_type'].value_counts().nlargest(15).plot(kind='bar')\n",
    "    plt.title('Top 15 Cuisine Types')\n",
    "    plt.xlabel('Cuisine')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273ccae-3406-4d97-b10c-ab523eb83253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Distribution of cooking time - use smaller bins\n",
    "# if 'cooking_time' in recipes_df.columns:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     # Use log scale for better visualization if the range is large\n",
    "#     if recipes_df['cooking_time'].max() > 5 * recipes_df['cooking_time'].median():\n",
    "#         sns.histplot(recipes_df['cooking_time'].clip(upper=recipes_df['cooking_time'].quantile(0.95)), bins=20)\n",
    "#         plt.title('Distribution of Cooking Time (minutes) - Clipped at 95th percentile')\n",
    "#     else:\n",
    "#         sns.histplot(recipes_df['cooking_time'], bins=20)\n",
    "#         plt.title('Distribution of Cooking Time (minutes)')\n",
    "#     plt.xlabel('Cooking Time (minutes)')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218adf5-d481-4f8e-983b-d2d3bd010344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of ingredients distribution\n",
    "if 'n_ingredients' in recipes_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(recipes_df['n_ingredients'], bins=range(1, min(30, recipes_df['n_ingredients'].max()+1)))\n",
    "    plt.title('Distribution of Number of Ingredients')\n",
    "    plt.xlabel('Number of Ingredients')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3802d",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Now we'll clean the data by:\n",
    "1. Removing duplicate recipes\n",
    "2. Normalizing ingredient names\n",
    "3. Standardizing measurements\n",
    "4. Handling missing values\n",
    "5. Creating dietary tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2753a4ec-f6dd-4df0-8e77-08e449181994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to check and remove duplicates in dataframes\n",
    "def check_remove_duplicates(df, df_name, subset_cols=None):\n",
    "    \"\"\"\n",
    "    Check and remove duplicates from a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: The dataframe to process\n",
    "        df_name: Name of the dataframe for printing\n",
    "        subset_cols: List of columns to consider for duplicates. If None, all columns are used.\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe with duplicates removed\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'-' * 30}\")\n",
    "    print(f\"Duplicate analysis for {df_name}:\")\n",
    "    print(f\"{'-' * 30}\")\n",
    "    \n",
    "    # If subset not specified, identify potential key columns\n",
    "    if subset_cols is None:\n",
    "        # Try to find ID-like columns first\n",
    "        id_cols = [col for col in df.columns if 'id' in col.lower()]\n",
    "        name_cols = [col for col in df.columns if 'name' in col.lower()]\n",
    "        \n",
    "        if id_cols:\n",
    "            subset_cols = id_cols\n",
    "            print(f\"Using ID columns for duplicate check: {subset_cols}\")\n",
    "        elif name_cols:\n",
    "            subset_cols = name_cols\n",
    "            print(f\"Using name columns for duplicate check: {subset_cols}\")\n",
    "        else:\n",
    "            # Use all columns if no suitable identifiers found\n",
    "            subset_cols = df.columns.tolist()\n",
    "            print(\"Using all columns for duplicate check\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    dup_count = df.duplicated(subset=subset_cols).sum()\n",
    "    print(f\"Number of duplicates in {df_name}: {dup_count} ({dup_count/len(df):.2%} of data)\")\n",
    "    \n",
    "    if dup_count > 0:\n",
    "        # Remove duplicates\n",
    "        df_cleaned = df.drop_duplicates(subset=subset_cols).reset_index(drop=True)\n",
    "        print(f\"Number of records after removing duplicates: {len(df_cleaned)}\")\n",
    "        return df_cleaned\n",
    "    else:\n",
    "        print(\"No duplicates found\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5627d2-5db1-4448-8bda-df4d5b284c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and remove duplicates from all dataframes\n",
    "print(\"\\n=== DUPLICATE ANALYSIS FOR ALL DATAFRAMES ===\")\n",
    "recipes_df = check_remove_duplicates(recipes_df, \"Recipes\", subset_cols=['name'])\n",
    "interactions_df = check_remove_duplicates(interactions_df, \"Interactions\")\n",
    "nutrition_df = check_remove_duplicates(nutrition_df, \"Nutrition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to normalize ingredient names\n",
    "def normalize_ingredients(ingredient_list):\n",
    "    \"\"\"\n",
    "    Normalize ingredient names by removing quantities and standardizing format\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    # If ingredient_list is already a list of strings\n",
    "    if isinstance(ingredient_list, list):\n",
    "        for ingredient in ingredient_list:\n",
    "            # Skip empty ingredients\n",
    "            if not ingredient or not isinstance(ingredient, str):\n",
    "                continue\n",
    "            \n",
    "            # Remove quantities (simplified for demonstration)\n",
    "            cleaned = re.sub(r'^\\d+\\s+\\d+/\\d+\\s+', '', ingredient)\n",
    "            cleaned = re.sub(r'^\\d+/\\d+\\s+', '', cleaned)\n",
    "            cleaned = re.sub(r'^\\d+\\s+', '', cleaned)\n",
    "            \n",
    "            # Convert to lowercase and strip whitespace\n",
    "            cleaned = cleaned.lower().strip()\n",
    "            \n",
    "            normalized.append(cleaned)\n",
    "    else:\n",
    "        # Handle the case where ingredient_list might be a string or another format\n",
    "        print(\"Warning: Expected ingredient_list to be a list, but got:\", type(ingredient_list))\n",
    "        if isinstance(ingredient_list, str):\n",
    "            # Try to interpret as a string representation of a list\n",
    "            try:\n",
    "                actual_list = eval(ingredient_list) if ingredient_list.startswith('[') else [ingredient_list]\n",
    "                return normalize_ingredients(actual_list)\n",
    "            except:\n",
    "                normalized = [ingredient_list.lower().strip()]\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Apply normalization to ingredients - with error handling\n",
    "recipes_df['normalized_ingredients'] = recipes_df['ingredients'].apply(\n",
    "    lambda x: normalize_ingredients(x) if isinstance(x, list) or isinstance(x, str) else []\n",
    ")\n",
    "\n",
    "# Show a sample recipe with normalized ingredients\n",
    "if len(recipes_df) > 0:\n",
    "    sample_idx = 0\n",
    "    print(f\"Original ingredients: {recipes_df.iloc[sample_idx]['ingredients']}\")\n",
    "    print(f\"Normalized ingredients: {recipes_df.iloc[sample_idx]['normalized_ingredients']}\")\n",
    "else:\n",
    "    print(\"No recipes found in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify dietary tags based on ingredients\n",
    "def identify_dietary_tags(ingredients):\n",
    "    \"\"\"\n",
    "    Identify dietary preferences based on ingredients\n",
    "    \"\"\"\n",
    "    # Handle empty ingredients list\n",
    "    if not ingredients or not isinstance(ingredients, (list, str)):\n",
    "        return []\n",
    "        \n",
    "    # Convert list of ingredients to a single string for easier checking\n",
    "    ingredients_str = ' '.join(ingredients).lower()\n",
    "    \n",
    "    tags = []\n",
    "    \n",
    "    # Vegetarian check (simplified)\n",
    "    meat_ingredients = ['chicken', 'beef', 'pork', 'lamb', 'turkey', 'veal', 'bacon']\n",
    "    if not any(meat in ingredients_str for meat in meat_ingredients):\n",
    "        tags.append('vegetarian')\n",
    "        \n",
    "        # Vegan check (simplified)\n",
    "        animal_products = ['cheese', 'milk', 'cream', 'yogurt', 'butter', 'egg', 'honey']\n",
    "        if not any(product in ingredients_str for product in animal_products):\n",
    "            tags.append('vegan')\n",
    "    \n",
    "    # Gluten-free check (simplified)\n",
    "    gluten_ingredients = ['flour', 'wheat', 'barley', 'rye', 'pasta', 'bread']\n",
    "    if not any(gluten in ingredients_str for gluten in gluten_ingredients):\n",
    "        tags.append('gluten-free')\n",
    "    \n",
    "    # Low-carb check (simplified)\n",
    "    high_carb_ingredients = ['sugar', 'pasta', 'rice', 'potato', 'bread', 'flour']\n",
    "    if not any(carb in ingredients_str for carb in high_carb_ingredients):\n",
    "        tags.append('low-carb')\n",
    "    \n",
    "    # Dairy-free check\n",
    "    dairy_ingredients = ['milk', 'cheese', 'cream', 'yogurt', 'butter']\n",
    "    if not any(dairy in ingredients_str for dairy in dairy_ingredients):\n",
    "        tags.append('dairy-free')\n",
    "    \n",
    "    return tags\n",
    "\n",
    "# Apply dietary tagging\n",
    "recipes_df['dietary_tags'] = recipes_df['normalized_ingredients'].apply(identify_dietary_tags)\n",
    "\n",
    "# Show the distribution of dietary tags\n",
    "diet_counts = {}\n",
    "for tags in recipes_df['dietary_tags']:\n",
    "    for tag in tags:\n",
    "        diet_counts[tag] = diet_counts.get(tag, 0) + 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "diet_df = pd.Series(diet_counts).sort_values(ascending=False)\n",
    "diet_df.plot(kind='bar')\n",
    "plt.title('Distribution of Dietary Tags')\n",
    "plt.xlabel('Dietary Tag')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show sample recipes with their dietary tags\n",
    "print(\"\\nSample recipes with dietary tags:\")\n",
    "sample_recipes = recipes_df[['name', 'normalized_ingredients', 'dietary_tags']].sample(5)\n",
    "for _, recipe in sample_recipes.iterrows():\n",
    "    print(f\"\\nRecipe: {recipe['name']}\")\n",
    "    print(f\"Ingredients: {', '.join(recipe['normalized_ingredients'])}\")\n",
    "    print(f\"Dietary Tags: {', '.join(recipe['dietary_tags']) if recipe['dietary_tags'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009b822-9917-4cb8-87d7-19dacb33711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic dataset information\n",
    "print(\"Raw Datasets information:\")\n",
    "print(f\"Number of recipes: {len(recipes_df)}\")\n",
    "print(\"\\nrecipes_df columns:\")\n",
    "print(recipes_df.columns.tolist())\n",
    "print(15 * \"-\")\n",
    "print(f\"Number of interactions: {len(interactions_df)}\")\n",
    "print(\"\\ninteractions_df columns:\")\n",
    "print(interactions_df.columns.tolist())\n",
    "print(15 * \"-\")\n",
    "print(f\"Number of nutritions: {len(nutrition_df)}\")\n",
    "print(\"\\nnutrition_df columns:\")\n",
    "print(nutrition_df.columns.tolist())\n",
    "print(15 * \"-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63edc2",
   "metadata": {},
   "source": [
    "## Final Data Structure and Storage\n",
    "\n",
    "### Save Datasets in JSON Format for RAG Implementation\n",
    "\n",
    "Let's save each dataset in JSON format to facilitate their use in our Retrieval Augmented Generation (RAG) system. JSON format is highly compatible with various RAG implementations and will make it easier to load the data in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c64b0-c406-40a0-afd4-518f4c3f8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "print(chromadb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141d898-0427-48f6-a49f-b599eaf0156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define paths for ChromaDB and SQL database\n",
    "VECTOR_DB_PATH = \"final/vector_db\"\n",
    "DB_PATH = \"final/kitchen_db.sqlite\"\n",
    "\n",
    "#####################\n",
    "# SQL Database Setup\n",
    "#####################\n",
    "def safe_convert(x):\n",
    "    \"\"\"\n",
    "    Safely converts a value to a string:\n",
    "      - If x is a list or numpy array, join its elements into a space-separated string.\n",
    "      - If x is not a list/array and is not null, convert to string.\n",
    "      - Otherwise, return an empty string.\n",
    "    \"\"\"\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        return \" \".join([str(item) for item in x])\n",
    "    return str(x) if pd.notna(x) else \"\"\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess DataFrame columns to be SQLite-compatible.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = df[column].apply(safe_convert)\n",
    "    return df\n",
    "\n",
    "def setup_sql_database(\n",
    "    recipes_df: pd.DataFrame, \n",
    "    interactions_df: pd.DataFrame, \n",
    "    nutrition_df: Optional[pd.DataFrame] = None,\n",
    "    db_path: str = DB_PATH\n",
    ") -> sqlite3.Connection:\n",
    "    \"\"\"\n",
    "    Set up SQLite database with raw dataframes.\n",
    "    \"\"\"\n",
    "    recipes_df = preprocess_dataframe(recipes_df)\n",
    "    interactions_df = preprocess_dataframe(interactions_df)\n",
    "    if nutrition_df is not None:\n",
    "        nutrition_df = preprocess_dataframe(nutrition_df)\n",
    "\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    print(f\"Creating SQLite database at {db_path}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    print(f\"Storing {len(recipes_df)} recipes in the database\")\n",
    "    recipes_df.to_sql('recipes', conn, if_exists='replace', index=False)\n",
    "    print(f\"Storing {len(interactions_df)} interactions in the database\")\n",
    "    interactions_df.to_sql('interactions', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    if nutrition_df is not None:\n",
    "        print(f\"Storing {len(nutrition_df)} nutrition entries in the database\")\n",
    "        nutrition_df.to_sql('nutrition', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    print(\"SQL database setup complete\")\n",
    "    return conn\n",
    "\n",
    "#############################\n",
    "# Vector Database Setup (ChromaDB)\n",
    "#############################\n",
    "def setup_vector_database(\n",
    "    vectorized_recipes_df: pd.DataFrame,\n",
    "    vectorized_interactions_df: Optional[pd.DataFrame] = None,\n",
    "    vector_db_path: str = VECTOR_DB_PATH\n",
    ") -> Tuple[Any, Any, Optional[Any]]:\n",
    "    \"\"\"\n",
    "    Set up ChromaDB using the precomputed dataframes for recipes and interactions.\n",
    "    \n",
    "    Arguments:\n",
    "        vectorized_recipes_df: DataFrame with your recipe data.\n",
    "        vectorized_interactions_df: DataFrame with your interaction data.\n",
    "        vector_db_path: Directory where ChromaDB will store its data.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing the ChromaDB client, the recipe collection, and \n",
    "        the interactions collection (if interactions_df is provided).\n",
    "    \"\"\"\n",
    "    os.makedirs(vector_db_path, exist_ok=True)\n",
    "    print(f\"Creating ChromaDB at {vector_db_path}\")\n",
    "    client = chromadb.PersistentClient(path=vector_db_path)\n",
    "    \n",
    "    #########################\n",
    "    # Load recipes into ChromaDB\n",
    "    #########################\n",
    "    print(f\"Setting up recipe collection with {len(vectorized_recipes_df)} recipes\")\n",
    "    recipe_collection = client.get_or_create_collection(name=\"recipes\")\n",
    "    \n",
    "    recipe_documents = []\n",
    "    recipe_metadatas = []\n",
    "    recipe_ids = []\n",
    "    \n",
    "    # Define which recipe columns to include as metadata\n",
    "    metadata_fields = ['name', 'minutes', 'contributor_id', 'submitted',\n",
    "                       'tags', 'nutrition', 'n_steps', 'cuisine_type',\n",
    "                       'n_ingredients', 'dietary_tags']\n",
    "    \n",
    "    for i, row in vectorized_recipes_df.iterrows():\n",
    "        # Determine a unique recipe ID. Use 'id' column if available.\n",
    "        recipe_id = row.get('id')\n",
    "        if recipe_id is None or (isinstance(recipe_id, float) and pd.isna(recipe_id)) or recipe_id == \"\":\n",
    "            recipe_id = str(i)\n",
    "        else:\n",
    "            recipe_id = str(recipe_id)\n",
    "        recipe_ids.append(recipe_id)\n",
    "        \n",
    "        # Build a document string by concatenating key text fields.\n",
    "        # You may adjust the fields below to better capture recipe information.\n",
    "        doc_text = \" \".join([\n",
    "            safe_convert(row.get('name', '')),\n",
    "            safe_convert(row.get('ingredients', '')),\n",
    "            safe_convert(row.get('steps', '')),\n",
    "            safe_convert(row.get('description', ''))\n",
    "        ])\n",
    "        recipe_documents.append(doc_text)\n",
    "        \n",
    "        # Build richer metadata from the chosen fields.\n",
    "        metadata = {key: safe_convert(row.get(key, \"\")) for key in metadata_fields}\n",
    "        metadata['recipe_id'] = recipe_id\n",
    "        recipe_metadatas.append(metadata)\n",
    "    \n",
    "    batch_size = 1000\n",
    "    for j in range(0, len(recipe_documents), batch_size):\n",
    "        end_idx = min(j + batch_size, len(recipe_documents))\n",
    "        recipe_collection.add(\n",
    "            documents=recipe_documents[j:end_idx],\n",
    "            metadatas=recipe_metadatas[j:end_idx],\n",
    "            ids=recipe_ids[j:end_idx]\n",
    "        )\n",
    "    \n",
    "    #########################\n",
    "    # Load interactions into ChromaDB (if provided)\n",
    "    #########################\n",
    "    interactions_collection = None\n",
    "    if vectorized_interactions_df is not None and not vectorized_interactions_df.empty:\n",
    "        print(f\"Setting up interactions collection with {len(vectorized_interactions_df)} interactions\")\n",
    "        interactions_collection = client.get_or_create_collection(name=\"interactions\")\n",
    "        \n",
    "        interaction_documents = []\n",
    "        interaction_metadatas = []\n",
    "        interaction_ids = []\n",
    "        \n",
    "        for i, row in vectorized_interactions_df.iterrows():\n",
    "            # Create a unique interaction ID from user_id, recipe_id, and index.\n",
    "            user_id = safe_convert(row.get('user_id', ''))\n",
    "            recipe_id = safe_convert(row.get('recipe_id', ''))\n",
    "            interaction_id = f\"{user_id}_{recipe_id}_{i}\"\n",
    "            interaction_ids.append(interaction_id)\n",
    "            \n",
    "            # Use the review text as the primary document.\n",
    "            review_text = safe_convert(row.get('review', ''))\n",
    "            if not review_text:\n",
    "                review_text = \"No review provided.\"\n",
    "            interaction_documents.append(review_text)\n",
    "            \n",
    "            # Build metadata for this interaction.\n",
    "            int_metadata = {\n",
    "                'interaction_id': interaction_id,\n",
    "                'user_id': user_id,\n",
    "                'recipe_id': recipe_id,\n",
    "                'date': safe_convert(row.get('date', '')),\n",
    "                'rating': safe_convert(row.get('rating', ''))\n",
    "            }\n",
    "            interaction_metadatas.append(int_metadata)\n",
    "        \n",
    "        for j in range(0, len(interaction_documents), batch_size):\n",
    "            end_idx = min(j + batch_size, len(interaction_documents))\n",
    "            interactions_collection.add(\n",
    "                documents=interaction_documents[j:end_idx],\n",
    "                metadatas=interaction_metadatas[j:end_idx],\n",
    "                ids=interaction_ids[j:end_idx]\n",
    "            )\n",
    "    \n",
    "    print(\"Vector database setup complete\")\n",
    "    return client, recipe_collection, interactions_collection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714843bc-e051-4166-b452-b53863e6b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Main Execution\n",
    "##############################\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume recipes_df and interactions_df have been loaded previously.\n",
    "    # For example:\n",
    "    # recipes_df = pd.read_pickle(\"your_recipes.pkl\")\n",
    "    # interactions_df = pd.read_pickle(\"your_interactions.pkl\")\n",
    "\n",
    "    # Set up the SQL database\n",
    "    # sqlite_conn = setup_sql_database(\n",
    "    #     recipes_df=recipes_df,\n",
    "    #     interactions_df=interactions_df,\n",
    "    #     nutrition_df=nutrition_df,  # Modify if you have nutrition data.\n",
    "    #     db_path=DB_PATH\n",
    "    # )\n",
    "    \n",
    "    # Set up ChromaDB with recipes and interactions\n",
    "    # chroma_client, recipe_collection, interactions_collection = setup_vector_database(\n",
    "    #     vectorized_recipes_df=recipes_df,\n",
    "    #     vectorized_interactions_df=interactions_df,\n",
    "    #     vector_db_path=VECTOR_DB_PATH\n",
    "    # )\n",
    "    \n",
    "    print(\"ChromaDB is ready for similarity search!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5026324-782f-4ed0-b1d7-7f7f08d71e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to SQL database\n",
    "DB_PATH = \"final/kitchen_db.sqlite\"\n",
    "# Path to Vectorized database\n",
    "VECTOR_DB_PATH = \"final/vector_db\"\n",
    "\n",
    "\n",
    "def view_schema_info(collection_name: str, db_path: str = VECTOR_DB_PATH):\n",
    "    \"\"\"\n",
    "    View schema information for a collection (metadata fields and their data types).\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to analyze\n",
    "        db_path: Path to the ChromaDB database\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    \n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"Collection '{collection_name}' not found. Error: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Get a sample of records to analyze schema\n",
    "    try:\n",
    "        results = collection.get(\n",
    "            limit=100,\n",
    "            include=['metadatas']\n",
    "        )\n",
    "        \n",
    "        if not results['metadatas']:\n",
    "            print(f\"Collection '{collection_name}' is empty or has no metadata.\")\n",
    "            return None\n",
    "        \n",
    "        # Analyze metadata fields\n",
    "        print(f\"\\n=== Schema for '{collection_name}' collection ===\\n\")\n",
    "        print(\"Metadata fields:\")\n",
    "        \n",
    "        # Collect all possible keys and their types\n",
    "        all_keys = set()\n",
    "        key_types = {}\n",
    "        key_examples = {}\n",
    "        \n",
    "        for metadata in results['metadatas']:\n",
    "            for key, value in metadata.items():\n",
    "                all_keys.add(key)\n",
    "                \n",
    "                # Track the data type\n",
    "                value_type = type(value).__name__\n",
    "                if key not in key_types:\n",
    "                    key_types[key] = set()\n",
    "                key_types[key].add(value_type)\n",
    "                \n",
    "                # Store an example value\n",
    "                if key not in key_examples and value:\n",
    "                    example = str(value)\n",
    "                    if len(example) > 50:\n",
    "                        example = example[:50] + \"...\"\n",
    "                    key_examples[key] = example\n",
    "        \n",
    "        # Display the schema information\n",
    "        for key in sorted(all_keys):\n",
    "            types_str = \", \".join(key_types[key])\n",
    "            example = key_examples.get(key, \"N/A\")\n",
    "            print(f\"  - {key}: {types_str}\")\n",
    "            print(f\"    Example: {example}\")\n",
    "        \n",
    "        return key_types\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting schema info: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def collection_info(db_path: str = VECTOR_DB_PATH):\n",
    "    \"\"\"\n",
    "    A simple function to display basic information about all collections.\n",
    "    More robust against API changes than the other functions.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the ChromaDB database\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    \n",
    "    try:\n",
    "        collection_names = client.list_collections()\n",
    "        print(f\"Found {len(collection_names)} collections in {db_path}:\")\n",
    "        \n",
    "        for name in collection_names:\n",
    "            print(f\"\\nCollection: {name}\")\n",
    "            \n",
    "            try:\n",
    "                collection = client.get_collection(name=str(name))\n",
    "                \n",
    "                # Try to get count\n",
    "                try:\n",
    "                    count = collection.count(where={})\n",
    "                    print(f\"  Records: {count}\")\n",
    "                except:\n",
    "                    print(\"  Count: Could not retrieve\")\n",
    "                \n",
    "                # Try to get the first few items\n",
    "                try:\n",
    "                    first_items = collection.get(limit=3, include=[\"metadatas\"])\n",
    "                    print(f\"  Sample IDs: {first_items['ids']}\")\n",
    "                    \n",
    "                    # Show first item metadata as example\n",
    "                    if first_items['metadatas'] and len(first_items['metadatas']) > 0:\n",
    "                        print(\"  Sample metadata keys:\", list(first_items['metadatas'][0].keys()))\n",
    "                except:\n",
    "                    print(\"  Sample: Could not retrieve\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error accessing collection: {str(e)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing collections: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a023d6-c2c8-431b-951e-65646ae21651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interactions', 'recipes']\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path=VECTOR_DB_PATH)\n",
    "print(client.list_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f75046-637e-4bce-a227-d8663139b013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 collections in final/vector_db:\n",
      "\n",
      "Collection: interactions\n",
      "  Count: Could not retrieve\n",
      "  Sample IDs: ['38094_40893_0', '1293707_40893_1', '8937_44394_2']\n",
      "  Sample metadata keys: ['date', 'interaction_id', 'rating', 'recipe_id', 'user_id']\n",
      "\n",
      "Collection: recipes\n",
      "  Count: Could not retrieve\n",
      "  Sample IDs: ['137739', '31490', '112140']\n",
      "  Sample metadata keys: ['contributor_id', 'cuisine_type', 'dietary_tags', 'minutes', 'n_ingredients', 'n_steps', 'name', 'nutrition', 'recipe_id', 'submitted', 'tags']\n"
     ]
    }
   ],
   "source": [
    "collection_info(VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664231b4-0097-47aa-b5f4-b198e9aa09a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Schema for 'recipes' collection ===\n",
      "\n",
      "Metadata fields:\n",
      "  - contributor_id: str\n",
      "    Example: 47892\n",
      "  - cuisine_type: str\n",
      "    Example: mexican\n",
      "  - dietary_tags: str\n",
      "    Example: vegetarian gluten-free low-carb\n",
      "  - minutes: str\n",
      "    Example: 55\n",
      "  - n_ingredients: str\n",
      "    Example: 7\n",
      "  - n_steps: str\n",
      "    Example: 11\n",
      "  - name: str\n",
      "    Example: arriba   baked winter squash mexican style\n",
      "  - nutrition: str\n",
      "    Example: [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]\n",
      "  - recipe_id: str\n",
      "    Example: 137739\n",
      "  - submitted: str\n",
      "    Example: 2005-09-16\n",
      "  - tags: str\n",
      "    Example: 60-minutes-or-less time-to-make course main-ingred...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'contributor_id': {'str'},\n",
       " 'cuisine_type': {'str'},\n",
       " 'dietary_tags': {'str'},\n",
       " 'minutes': {'str'},\n",
       " 'n_ingredients': {'str'},\n",
       " 'n_steps': {'str'},\n",
       " 'name': {'str'},\n",
       " 'nutrition': {'str'},\n",
       " 'recipe_id': {'str'},\n",
       " 'submitted': {'str'},\n",
       " 'tags': {'str'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_schema_info(\"recipes\", VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a588583-186b-4a0c-b16a-525e5139c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Schema for 'interactions' collection ===\n",
      "\n",
      "Metadata fields:\n",
      "  - date: str\n",
      "    Example: 2003-02-17\n",
      "  - interaction_id: str\n",
      "    Example: 38094_40893_0\n",
      "  - rating: str\n",
      "    Example: 4\n",
      "  - recipe_id: str\n",
      "    Example: 40893\n",
      "  - user_id: str\n",
      "    Example: 38094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'date': {'str'},\n",
       " 'interaction_id': {'str'},\n",
       " 'rating': {'str'},\n",
       " 'recipe_id': {'str'},\n",
       " 'user_id': {'str'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_schema_info(\"interactions\", VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf16547e-c641-47f1-a6f0-7306520b5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your vector database (keep this as a global constant)\n",
    "VECTOR_DB_PATH = \"final/vector_db\"\n",
    "\n",
    "def gemini_recipe_similarity_search(query_text: str, n_results: int, cuisine: Optional[str] = None, dietary_tag: Optional[str] = None, max_minutes: Optional[int] = None) -> str:\n",
    "    \"\"\"\n",
    "    Searches for similar recipes based on a query, with optional filters and returns full metadata.\n",
    "\n",
    "    Args:\n",
    "        query_text: The text to search for in recipes.\n",
    "        n_results: The number of top similar recipes to return.\n",
    "        cuisine: (Optional) Filter by cuisine type (e.g., 'mexican', 'italian').\n",
    "        dietary_tag: (Optional) Filter by dietary tag (e.g., 'vegetarian', 'gluten-free').\n",
    "        max_minutes: (Optional) Filter recipes with a cooking time less than or equal to this value.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing the full metadata of the top similar recipes with similarity scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=VECTOR_DB_PATH)\n",
    "        recipe_collection = client.get_collection(name=\"recipes\")\n",
    "\n",
    "        where_clause = {}\n",
    "        if cuisine is not None:\n",
    "            where_clause[\"cuisine_type\"] = cuisine\n",
    "        if dietary_tag is not None:\n",
    "            where_clause[\"dietary_tags\"] = {\"$contains\": dietary_tag}\n",
    "        if max_minutes is not None:\n",
    "            where_clause[\"minutes\"] = {\"$lte\": str(max_minutes)} # Store as string in metadata\n",
    "\n",
    "        results = recipe_collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        if not results['ids'][0]:\n",
    "            return f\"No similar recipes found for the query: '{query_text}' with the specified criteria.\"\n",
    "\n",
    "        output = f\"Found {len(results['ids'][0])} similar recipes for query: '{query_text}'.\\n\"\n",
    "        output += \"-\" * 80 + \"\\n\"\n",
    "        for i, (doc_id, doc, metadata, distance) in enumerate(zip(\n",
    "            results['ids'][0],\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            similarity_score = (1 - distance) * 100\n",
    "            output += f\"\\n{i+1}. Recipe Name: {metadata.get('name', 'Unnamed')}\\n\"\n",
    "            output += f\"   Similarity: {similarity_score:.2f}%\\n\"\n",
    "            output += f\"   Recipe ID: {doc_id}\\n\"\n",
    "            for key, value in metadata.items():\n",
    "                output += f\"   {key.replace('_', ' ').title()}: {value}\\n\"\n",
    "            output += f\"   Ingredients: {doc}\\n\"  # Include the full document (ingredients/steps)\n",
    "            output += \"-\" * 80 + \"\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during recipe similarity search: {e}\"\n",
    "\n",
    "# Updated `gemini_interaction_similarity_search` Function:\n",
    "\n",
    "def gemini_interaction_similarity_search(query_text: str, n_results: int) -> str:\n",
    "    \"\"\"\n",
    "    Searches for similar user interactions (reviews) based on a query and returns full metadata.\n",
    "\n",
    "    Args:\n",
    "        query_text: The text to search for in user reviews.\n",
    "        n_results: The number of top similar interactions to return.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing the full metadata of the top similar interactions with similarity scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=VECTOR_DB_PATH)\n",
    "        interactions_collection = client.get_collection(name=\"interactions\")\n",
    "        results = interactions_collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        if not results['ids'][0]:\n",
    "            return f\"No similar reviews found for the query: '{query_text}'.\"\n",
    "\n",
    "        output = f\"Found {len(results['ids'][0])} similar reviews for query: '{query_text}'.\\n\"\n",
    "        output += \"-\" * 80 + \"\\n\"\n",
    "        for i, (doc_id, doc, metadata, distance) in enumerate(zip(\n",
    "            results['ids'][0],\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            similarity_score = (1 - distance) * 100\n",
    "            output += f\"\\n{i+1}. Review ID: {doc_id}\\n\"\n",
    "            output += f\"   Similarity: {similarity_score:.2f}%\\n\"\n",
    "            for key, value in metadata.items():\n",
    "                output += f\"   {key.replace('_', ' ').title()}: {value}\\n\"\n",
    "            output += f\"   Review Text: {doc}\\n\"  # Include the full document (review text)\n",
    "            output += \"-\" * 80 + \"\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    except ValueError:\n",
    "        return \"Interactions collection not found. Make sure you have interaction data loaded.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during interaction similarity search: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c5d599-bfb0-4fa1-b594-9c8fb572ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 similar recipes for query: 'check for making an italian pizza '.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Recipe Name: gluten free and low fat vegetarian pizza\n",
      "   Similarity: 39.87%\n",
      "   Recipe ID: 455493\n",
      "   Contributor Id: 286566\n",
      "   Cuisine Type: other\n",
      "   Dietary Tags: vegetarian vegan dairy-free\n",
      "   Minutes: 30\n",
      "   N Ingredients: 11\n",
      "   N Steps: 9\n",
      "   Name: gluten free and low fat vegetarian pizza\n",
      "   Nutrition: [234.7, 21.0, 30.0, 28.0, 25.0, 33.0, 5.0]\n",
      "   Recipe Id: 455493\n",
      "   Submitted: 2011-05-09\n",
      "   Tags: 30-minutes-or-less time-to-make course main-ingredient preparation main-dish vegetables pizza dietary gluten-free mushrooms free-of-something peppers artichoke\n",
      "   Ingredients: gluten free and low fat vegetarian pizza bread mix cold water garlic cloves fresh rosemary pasta sauce red capsicum artichoke heart button mushrooms kalamata olive bocconcini rocket preheat oven to 240c lightly grease two 18cm pizza trays place bread mix in a bowl and combine 100ml cold water , garlic and rosemary in a jug add water mix to bread mix and using an electric mixer , beat on low speed for 30 seconds or until just combined , scrape down side of bowl and beat for a further 2 to 3 minutes or until thick spoon half the mix onto 1 prepared tray and using the back of a spoon , spread mix to form an 18cm round repeat with remaining dough bake for 10 to 12 minutes or until bases are light golden and cooked through remove from oven and top with capsicum , artichoke , mushrooms , olives and bocconcini ad bake for 10 to 12 minutes or until cheese has melted cut into wedges and serve with rocket from super food ideas and their special diets italian feature.  times are estimated.\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_text = \"check for making an italian pizza \"\n",
    "result = gemini_recipe_similarity_search(query_text, n_results = 1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24868b13-54e0-4321-98f0-e22807324374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 similar reviews for query: 'best italian pizza'.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Review ID: 362880_165360_692272\n",
      "   Similarity: 46.85%\n",
      "   Date: 2006-12-15\n",
      "   Interaction Id: 362880_165360_692272\n",
      "   Rating: 5\n",
      "   Recipe Id: 165360\n",
      "   User Id: 362880\n",
      "   Review Text: I made this for my Mom's birthday today--I wanted the pizza to be as Italian as possible (my Dad and I spent three weeks in Italy this Jan.) so I did a search here for thin-crust pizza and this was at the top of the list--BRAVO!\n",
      "\n",
      "I made my own tomato sauce and topped it w/ a sprinkling of mozzarella cheese (unlike here in Canada and the States, the Italians only put enough cheese to lightly cover it--unless it's 4-cheese) and fresh basil--deeeeeelish!\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_text = \"best italian pizza\"\n",
    "result = gemini_interaction_similarity_search(query_text, n_results = 1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cf7dc8a-87f2-42c3-b2bf-1c75e921a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not parse JSON for field 'steps' in recipe ID 71373. Kept as string.\n",
      "Info: Field 'ingredients' in recipe ID 71373 was treated as space-separated string.\n",
      "Info: Field 'tags' in recipe ID 71373 was treated as space-separated string.\n",
      "Info: Field 'dietary_tags' in recipe ID 71373 was treated as space-separated string.\n",
      "Info: Field 'normalized_ingredients' in recipe ID 71373 was treated as space-separated string.\n",
      "Error fetching nutrition for 'water': HTTPSConnectionPool(host='world.openfoodfacts.org', port=443): Read timed out. (read timeout=10)\n",
      "Error fetching nutrition for 'oil': HTTPSConnectionPool(host='world.openfoodfacts.org', port=443): Read timed out. (read timeout=10)\n",
      "Error fetching nutrition for 'bread': HTTPSConnectionPool(host='world.openfoodfacts.org', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Recipe Details: Country White Bread or Dinner Rolls (Bread Machine) - Recipe ID: 71373\n",
       "\n",
       "### Recipe Information:\n",
       "*   **Name:** Country White Bread or Dinner Rolls bread machine\n",
       "*   **Description:** This is considered the best white bread recipe ever! It is a tender bread with a slightly sweet taste. Perfect for dinner rolls.\n",
       "*   **Contributor ID:** 60260\n",
       "*   **Cuisine Type:** Other\n",
       "*   **Dietary Tags:** Vegetarian\n",
       "*   **Ingredients:** water, egg, vegetable, oil, bread, flour, sugar, salt, instant, yeast, butter, shortening\n",
       "*   **Number of Ingredients:** 9\n",
       "*   **Number of Steps:** 14\n",
       "*   **Total Time:** 45 minutes\n",
       "*   **Steps:**\n",
       "    1.  In bread machine pan, place all ingredients from flour through yeast in order as recommended by the manufacturer.\n",
       "    2.  Select the basic bread setting.\n",
       "    3.  Check dough after 5 minutes of mixing. If needed add 1-2t of water or flour.\n",
       "    4.  Bake as normal.\n",
       "    5.  For dinner rolls, mix in bread machine but use only the dough option.\n",
       "    6.  Lightly grease a 9 x 13 baking pan.\n",
       "    7.  When dough is finished shape into 15 uniformly sized balls.\n",
       "    8.  Grease hands with shortening to smooth the dough out and keep the dough from drying out.\n",
       "    9.  Cover and let the rolls rise in a warm, draft free place until risen to the desired size. Let them about double in size.\n",
       "    10. Bake in a 350 oven about 12-15 minutes until golden brown.\n",
       "    11. After removing from the oven, brush the tops of the rolls with melted butter.\n",
       "    12. Take out of pan.\n",
       "    13. Let completely cool before storing.\n",
       "\n",
       "### Ingredient Nutrition (per 100g, from Open Food Facts):\n",
       "\n",
       "*   **water:** API request failed\n",
       "*   **egg:** calories_100g: 725, carbohydrates_100g: 1.4, fat_100g: 79, proteins_100g: 1.1, saturated_fat_100g: 6.3, sodium_100g: 0.6, sugars_100g: 1.3\n",
       "*   **vegetable:** calories_100g: 675, carbohydrates_100g: 0.2, fat_100g: 75, fiber_100g: 0, proteins_100g: 0.1, saturated_fat_100g: 34, sodium_100g: 0.2, sugars_100g: 0\n",
       "*   **oil:** API request failed\n",
       "*   **bread:** API request failed\n",
       "*   **flour:** calories_100g: 116, carbohydrates_100g: 16.5, fat_100g: 5.1, fiber_100g: 0.1, proteins_100g: 1, saturated_fat_100g: 2.5, sodium_100g: 0.02, sugars_100g: 9.2\n",
       "*   **sugar:** calories_100g: 116, carbohydrates_100g: 16.5, fat_100g: 5.1, fiber_100g: 0.1, proteins_100g: 1, saturated_fat_100g: 2.5, sodium_100g: 0.02, sugars_100g: 9.2\n",
       "*   **salt:** carbohydrates_100g: 0, fat_100g: 0, fiber_100g: 0, proteins_100g: 0, saturated_fat_100g: 0, sodium_100g: 39.6, sugars_100g: 0\n",
       "*   **instant:** calories_100g: 515, carbohydrates_100g: 63, fat_100g: 26, fiber_100g: 1, proteins_100g: 7.1, saturated_fat_100g: 7.74, sodium_100g: 0.098, sugars_100g: 61\n",
       "*   **yeast:** calories_100g: 260, carbohydrates_100g: 30, fat_100g: 0.5, fiber_100g: 0, proteins_100g: 34, saturated_fat_100g: 0.1, sodium_100g: 4.32, sugars_100g: 1.2\n",
       "*   **butter:** calories_100g: 675, carbohydrates_100g: 0.2, fat_100g: 75, fiber_100g: 0, proteins_100g: 0.1, saturated_fat_100g: 34, sodium_100g: 0.2, sugars_100g: 0\n",
       "*   **shortening:** calories_100g: 368, carbohydrates_100g: 49.12, fat_100g: 17.54, fiber_100g: 1.8, proteins_100g: 5.26, saturated_fat_100g: 5.26, sodium_100g: 0.333, sugars_100g: 17.54\n",
       "\n",
       "### User Interaction Data:\n",
       "\n",
       "*   **Overall Rating:** 4.60\n",
       "*   **Recent Reviews:**\n",
       "    *   **Date:** 2018-09-03, **Rating:** 5, **Review:** Light fluffy and so pretty to look at, and to top it off so easy to make. Everyone at our get together raved about them. I'm planning on following reviewer Viclynn's freezing directions, so that I'll always have some on hand\n",
       "    *   **Date:** 2018-03-05, **Rating:** 5, **Review:** Best buns ever! Followed the recipe exactly, got awesome soft golden melt-in-your-mouth buns. My kid calls them clouds. Used them for sandwiches, with butter and jam, ate them plain or with just butter or margarine... They would probably make cute little sliders but they don't last that long. Brought two batches at work today, made everyone's Monday better. :) I'm planning to try mixing the ingredients in the Kitchenaid and then letting the dough rise before shaping the rolls and letting them rise a second time - it would be nice if I could make more than 15 at once.\n",
       "    *   **Date:** 2017-12-24, **Rating:** 5, **Review:** I don't use instant yeast, only active dry, so it's hit and miss with bread machine recipes, if my conversions turn out. For this recipe, I used milk instead of water, warmed, and let 3 slightly rounded teaspoons active dry yeast and the sugar proof in the milk, then added all the rest of the ingredients, except I used all purpose unbleached white flour, and pure olive oil for the vegetable oil. The dough rose wonderfully and baked to beautiful and very tasty rolls. Instantly bookmarked and will make often. Thank you!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# Path to SQL database\n",
    "DB_PATH = \"final/kitchen_db.sqlite\"\n",
    "\n",
    "def list_tables() -> List[str]:\n",
    "    \"\"\"List all tables in the SQLite database.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return [table[0] for table in tables]\n",
    "\n",
    "def describe_table(table_name: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Describe the schema of a specified table.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    schema = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return [(col[1], col[2]) for col in schema]\n",
    "\n",
    "def execute_query(sql: str) -> List[Tuple]:\n",
    "    \"\"\"Execute an SQL query and return the results.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        results = cursor.fetchall()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except sqlite3.Error as e:\n",
    "        conn.close()\n",
    "        # Return error message instead of the full error object for better handling downstream\n",
    "        return [(\"Error executing SQL query:\", str(e))] # Modified error return\n",
    "\n",
    "# New function to fetch nutrition data from Open Food Facts API\n",
    "def fetch_nutrition_from_openfoodfacts(ingredient_name: str) -> Dict:\n",
    "    \"\"\"Fetch nutrition data for an ingredient from Open Food Facts API.\"\"\"\n",
    "    api_key = os.getenv('OPENFOODFACTS_API_KEY')\n",
    "    # It's good practice to check if the key exists, though Open Food Facts search might work without one for basic queries.\n",
    "    # if not api_key:\n",
    "    #     print(\"Warning: OPENFOODFACTS_API_KEY environment variable not set.\")\n",
    "    #     # Depending on strictness, you might return unavailable here or proceed without auth\n",
    "    #     # return {\"status\": \"unavailable\", \"reason\": \"API key not configured\"}\n",
    "\n",
    "    search_url = f\"https://world.openfoodfacts.org/cgi/search.pl\"\n",
    "    params = {\n",
    "        \"search_terms\": ingredient_name,\n",
    "        \"search_simple\": 1,\n",
    "        \"action\": \"process\",\n",
    "        \"json\": 1,\n",
    "        \"page_size\": 1 # We only need the top result\n",
    "    }\n",
    "    headers = {'User-Agent': 'CapstoneProject/1.0 (Language Model Integration)'} # Good practice to set a User-Agent\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url, params=params, headers=headers, timeout=10) # Added timeout\n",
    "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get('products') and len(data['products']) > 0:\n",
    "            product = data['products'][0]\n",
    "            nutriments = product.get('nutriments', {})\n",
    "\n",
    "            # Extract desired nutrients, providing default None if not found\n",
    "            # Using .get() avoids KeyError if a nutrient is missing\n",
    "            nutrition_info = {\n",
    "                \"food_normalized\": ingredient_name, # Keep track of what was searched\n",
    "                \"source\": \"Open Food Facts\",\n",
    "                \"product_name\": product.get('product_name', 'N/A'),\n",
    "                \"calories_100g\": nutriments.get('energy-kcal_100g'),\n",
    "                \"fat_100g\": nutriments.get('fat_100g'),\n",
    "                \"saturated_fat_100g\": nutriments.get('saturated-fat_100g'),\n",
    "                \"carbohydrates_100g\": nutriments.get('carbohydrates_100g'),\n",
    "                \"sugars_100g\": nutriments.get('sugars_100g'),\n",
    "                \"fiber_100g\": nutriments.get('fiber_100g'),\n",
    "                \"proteins_100g\": nutriments.get('proteins_100g'),\n",
    "                \"sodium_100g\": nutriments.get('sodium_100g'),\n",
    "                # Add other relevant fields if needed, e.g., vitamins\n",
    "            }\n",
    "            # Filter out keys with None values for cleaner output\n",
    "            return {k: v for k, v in nutrition_info.items() if v is not None}\n",
    "        else:\n",
    "            return {\"status\": \"unavailable\", \"reason\": \"No product found on Open Food Facts\"}\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching nutrition for '{ingredient_name}': {e}\")\n",
    "        return {\"status\": \"unavailable\", \"reason\": f\"API request failed: {e}\"}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON response for '{ingredient_name}'\")\n",
    "        return {\"status\": \"unavailable\", \"reason\": \"Invalid JSON response from API\"}\n",
    "\n",
    "\n",
    "def get_recipe_by_id(recipe_id: str) -> Optional[dict]:\n",
    "    \"\"\"Get a recipe by its ID with all details, including live nutrition data for ingredients.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    # Use try-finally to ensure connection is closed\n",
    "    try:\n",
    "        cursor.execute(\"SELECT * FROM recipes WHERE id = ?\", (recipe_id,))\n",
    "        recipe_data = cursor.fetchone()\n",
    "\n",
    "        if not recipe_data:\n",
    "            return None\n",
    "\n",
    "        columns = [col[0] for col in cursor.description]\n",
    "        recipe = dict(zip(columns, recipe_data))\n",
    "\n",
    "        # Standardize field parsing - attempt JSON, fallback for simple strings\n",
    "        for field in [\"steps\", \"ingredients\", \"nutrition\", \"tags\", \"dietary_tags\", \"normalized_ingredients\"]:\n",
    "            value = recipe.get(field)\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    recipe[field] = json.loads(value)\n",
    "                except json.JSONDecodeError:\n",
    "                    # If JSON fails, and it's one of the list-like fields, try splitting by space\n",
    "                    if field in [\"ingredients\", \"tags\", \"dietary_tags\", \"normalized_ingredients\"]:\n",
    "                         recipe[field] = value.split() # Split space-separated string into list\n",
    "                         print(f\"Info: Field '{field}' in recipe ID {recipe_id} was treated as space-separated string.\")\n",
    "                    # else: # Keep as string if it's not expected to be a list (like 'steps' or 'nutrition' if not JSON)\n",
    "                    #    print(f\"Warning: Could not parse JSON for field '{field}' in recipe ID {recipe_id}. Kept as string. Value: {value[:100]}...\")\n",
    "                    #    pass # Keep as string if parsing fails and it's not a simple list field\n",
    "                    # Simplified: Just split the known list-like fields if JSON fails\n",
    "                    elif field == \"steps\": # Keep steps as string if not JSON\n",
    "                         print(f\"Warning: Could not parse JSON for field 'steps' in recipe ID {recipe_id}. Kept as string.\")\n",
    "                         pass # Keep as string\n",
    "                    else: # Handle nutrition or other fields if necessary\n",
    "                         print(f\"Warning: Could not parse JSON for field '{field}' in recipe ID {recipe_id}. Value: {value[:100]}...\")\n",
    "                         pass # Keep as string or handle differently if needed\n",
    "\n",
    "\n",
    "        # Fetch nutrition for normalized ingredients\n",
    "        ingredient_nutrition_list = []\n",
    "        normalized_ingredients = recipe.get(\"normalized_ingredients\")\n",
    "\n",
    "        # Ensure normalized_ingredients is now a list before iterating\n",
    "        if isinstance(normalized_ingredients, list):\n",
    "            for ingredient in normalized_ingredients:\n",
    "                if isinstance(ingredient, str): # Ensure it's a string before fetching\n",
    "                     # Check for empty strings that might result from splitting\n",
    "                     if ingredient.strip():\n",
    "                         nutrition_data = fetch_nutrition_from_openfoodfacts(ingredient)\n",
    "                         ingredient_nutrition_list.append(nutrition_data)\n",
    "                else:\n",
    "                     # Handle cases where items in the list aren't strings\n",
    "                     ingredient_nutrition_list.append({\"status\": \"unavailable\", \"reason\": f\"Invalid ingredient format: {type(ingredient)}\"})\n",
    "        # Removed the 'elif normalized_ingredients is not None:' block as the splitting logic above handles the string case.\n",
    "        # If it's still not a list after the processing above, something else is wrong.\n",
    "        elif normalized_ingredients is not None:\n",
    "             print(f\"Error: 'normalized_ingredients' field in recipe ID {recipe_id} is not a list after processing. Type: {type(normalized_ingredients)}\")\n",
    "             ingredient_nutrition_list.append({\"status\": \"unavailable\", \"reason\": \"normalized_ingredients field could not be processed into a list\"})\n",
    "\n",
    "\n",
    "        recipe['ingredient_nutrition'] = ingredient_nutrition_list # Add the fetched nutrition data\n",
    "\n",
    "        return recipe\n",
    "    finally:\n",
    "        conn.close() # Ensure connection is closed even if errors occur\n",
    "\n",
    "\n",
    "def get_ratings_and_reviews_by_recipe_id(recipe_id: str, limit: int) -> Optional[dict]: # Removed default for limit\n",
    "    \"\"\"Get ratings and the most recent reviews for a recipe ID.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    # Use try-finally for connection closing\n",
    "    try:\n",
    "        # Get overall rating\n",
    "        cursor.execute(\"SELECT AVG(rating) FROM interactions WHERE recipe_id = ?\", (recipe_id,))\n",
    "        overall_rating_result = cursor.fetchone()\n",
    "        # Handle case where there are no ratings yet\n",
    "        overall_rating = overall_rating_result[0] if overall_rating_result else None\n",
    "\n",
    "        # Get most recent reviews\n",
    "        cursor.execute(\n",
    "            \"SELECT date, rating, review FROM interactions WHERE recipe_id = ? ORDER BY date DESC LIMIT ?\",\n",
    "            (recipe_id, limit),\n",
    "        )\n",
    "        recent_reviews = cursor.fetchall()\n",
    "        columns = [\"date\", \"rating\", \"review\"]\n",
    "        reviews_list = [dict(zip(columns, review)) for review in recent_reviews]\n",
    "\n",
    "        return {\"overall_rating\": overall_rating, \"recent_reviews\": reviews_list}\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# These are the Python functions defined above.\n",
    "db_tools = [list_tables, describe_table, execute_query, get_ratings_and_reviews_by_recipe_id, get_recipe_by_id, fetch_nutrition_from_openfoodfacts]\n",
    "\n",
    "instruction = \"\"\"You are a helpful chatbot that can interact with an SQL database for a Kitchen Assistant.\n",
    "You can retrieve information about recipes and user interactions (ratings and reviews).\n",
    "Use the available tools to understand the database schema and execute SQL queries to answer user questions.\n",
    "**Note:** Ingredient nutrition information is fetched live from the Open Food Facts API when you request recipe details using `get_recipe_by_id`.\n",
    "\n",
    "Here are the available tools:\n",
    "\n",
    "- list_tables(): Lists all tables in the database.\n",
    "- describe_table(table_name: str): Describes the schema (columns and their types) of a specified table.\n",
    "- execute_query(sql: str): Executes an SQL query and returns the results. Use this for complex data retrieval. Be careful with the SQL syntax.\n",
    "- get_recipe_by_id(recipe_id: str): Retrieves all details for a specific recipe given its ID. This includes fetching live nutrition data for each normalized ingredient from Open Food Facts.\n",
    "- get_ratings_and_reviews_by_recipe_id(recipe_id: str, limit: int): Retrieves the overall rating and the 'limit' most recent reviews for a given recipe ID. You MUST provide a value for 'limit'. If the user doesn't specify, use 3.\n",
    "\n",
    "When a user asks a question:\n",
    "\n",
    "1. Understand the user's intent and identify which tool(s) can best answer the question.\n",
    "2. If the question requires specific information about a recipe (like ingredients, steps, nutrition), use `get_recipe_by_id`. This will automatically include nutrition details fetched from Open Food Facts.\n",
    "3. If the question asks for ratings or reviews for a recipe, use `get_ratings_and_reviews_by_recipe_id`. **Always specify a value for the 'limit' parameter. If the user doesn't say how many reviews, use 3.**\n",
    "4. For more complex queries involving filtering, joining tables, or specific data selection, construct and use the `execute_query` tool. First, use `list_tables` and `describe_table` to understand the database structure if needed.\n",
    "5. Present the information clearly and concisely to the user, ideally in a readable format like Markdown.\n",
    "\n",
    "For example:\n",
    "\n",
    "- To get details about recipe ID 71373 (including ingredient nutrition), use `get_recipe_by_id(recipe_id='71373')`.\n",
    "- To get the overall rating and 3 recent reviews for recipe ID 71373, use `get_ratings_and_reviews_by_recipe_id(recipe_id='71373', limit=3)`.\n",
    "- For a complex query like \"Find recipes with 'chicken' and a cooking time less than 30 minutes\", you might need to use `execute_query` after understanding the 'recipes' table schema.\n",
    "\n",
    "Be mindful of potential SQL injection if using `execute_query` with user-provided input, although in this setup, the queries are constructed by you based on the user's intent.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# These are the Python functions defined above.\n",
    "\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Start a chat with automatic function calling enabled.\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=instruction,\n",
    "        tools=db_tools,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "resp = chat.send_message(\"\"\"check recipe id=71373 in both tables (recipes and interactions) , \n",
    "check both tables please, and return full info, start with recipes table,\n",
    "then check the nutritions by calling the fetch_nutrition_from_openfoodfacts function and get the accumulated info of Ingredients inside of the recipe,\n",
    "then interactions table, and get the info of that recipe_id, however show only overal rating for that recipe, and 3 most recent reviews not all of them. \n",
    "output in markdown format\"\"\")\n",
    "display(Markdown(resp.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db61d90e-fd22-48cd-86ea-525603605c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country White Bread or Dinner Rolls contains approximately: 448.3 calories, 19.74 carbohydrates, 31.91 fat, 0.42 fiber, 5.45 proteins, 11.28 saturated fat, 5.65 sodium, and 11.05 sugars per 100g.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"\"\"Calculate the total nutritional values of a recipe based on the available data for its ingredients. Follow these rules:\n",
    "\n",
    "1. For each ingredient listed, use the given nutritional information (per 100g).\n",
    "2. If the API request failed or the nutritional info is unavailable for an ingredient, ignore it completely.\n",
    "3. Sum each nutritional column (e.g., calories, carbohydrates, fat, etc.) **across only the ingredients with available data**.\n",
    "4. After summing, divide each value by the number of ingredients that had valid data to get an average per 100g.\n",
    "5. Present the result in a readable sentence format, as in the example below.\n",
    "6. Use the title: â€œCountry White Bread or Dinner Rolls contains approximately: â€¦â€\n",
    "\n",
    "Here is the nutritional data (per 100g, from Open Food Facts):\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "water: API request failed\n",
    "egg: calories_100g: 725, carbohydrates_100g: 1.4, fat_100g: 79, proteins_100g: 1.1, saturated_fat_100g: 6.3, sodium_100g: 0.6, sugars_100g: 1.3\n",
    "vegetable: calories_100g: 675, carbohydrates_100g: 0.2, fat_100g: 75, fiber_100g: 0, proteins_100g: 0.1, saturated_fat_100g: 34, sodium_100g: 0.2, sugars_100g: 0\n",
    "oil: API request failed\n",
    "bread: API request failed\n",
    "flour: calories_100g: 116, carbohydrates_100g: 16.5, fat_100g: 5.1, fiber_100g: 0.1, proteins_100g: 1, saturated_fat_100g: 2.5, sodium_100g: 0.02, sugars_100g: 9.2\n",
    "sugar: calories_100g: 116, carbohydrates_100g: 16.5, fat_100g: 5.1, fiber_100g: 0.1, proteins_100g: 1, saturated_fat_100g: 2.5, sodium_100g: 0.02, sugars_100g: 9.2\n",
    "salt: carbohydrates_100g: 0, fat_100g: 0, fiber_100g: 0, proteins_100g: 0, saturated_fat_100g: 0, sodium_100g: 39.6, sugars_100g: 0\n",
    "instant: calories_100g: 515, carbohydrates_100g: 63, fat_100g: 26, fiber_100g: 1, proteins_100g: 7.1, saturated_fat_100g: 7.74, sodium_100g: 0.098, sugars_100g: 61\n",
    "yeast: calories_100g: 260, carbohydrates_100g: 30, fat_100g: 0.5, fiber_100g: 0, proteins_100g: 34, saturated_fat_100g: 0.1, sodium_100g: 4.32, sugars_100g: 1.2\n",
    "butter: calories_100g: 675, carbohydrates_100g: 0.2, fat_100g: 75, fiber_100g: 0, proteins_100g: 0.1, saturated_fat_100g: 34, sodium_100g: 0.2, sugars_100g: 0\n",
    "shortening: calories_100g: 368, carbohydrates_100g: 49.12, fat_100g: 17.54, fiber_100g: 1.8, proteins_100g: 5.26, saturated_fat_100g: 5.26, sodium_100g: 0.333, sugars_100g: 17.54\n",
    "    \"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "484d9cec-d3ee-4bda-a1d7-1876cc917b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food_normalized': 'PEANUTS',\n",
       " 'source': 'Open Food Facts',\n",
       " 'product_name': \"Menguy's Peanut 100%\",\n",
       " 'calories_100g': 610,\n",
       " 'fat_100g': 48,\n",
       " 'saturated_fat_100g': 5.8,\n",
       " 'carbohydrates_100g': 13,\n",
       " 'sugars_100g': 6.2,\n",
       " 'fiber_100g': 7,\n",
       " 'proteins_100g': 28,\n",
       " 'sodium_100g': 0.02}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_nutrition_from_openfoodfacts(\"PEANUTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9892e238-f836-4475-a427-719f15b3d011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are a few recipes that match your request:\n",
       "\n",
       "**Gluten-Free:**\n",
       "\n",
       "1.  **Quick Easy Chicken in Wine Sauce:** This recipe takes only 17 minutes to make and has a similarity score of -38.33%.\n",
       "2.  **Quick Lemon Garlic Chicken:** This recipe has a similarity score of -38.37%. While the preparation is quick, marinating requires several hours or overnight. The total cooking time is 65 minutes.\n",
       "3.  **Super Easy Crock Pot French Dip Sandwiches:** This recipe has a similarity score of -36.02%. Cooking time is 488 minutes.\n",
       "\n",
       "**Vegetarian:**\n",
       "\n",
       "The search results for vegetarian recipes were the same as for gluten-free.\n",
       "\n",
       "It seems that the search results do not perfectly align with the \"quick\" requirement, as some recipes have longer cooking or preparation times.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp = chat.send_message(\"gluten free or vegeterian recipe but quick and easy\")\n",
    "display(Markdown(resp.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c521b72-6476-43d0-abde-510720f5d8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I'll find two quick and easy gluten-free or vegetarian recipes from Food.com and provide you with the full information on each.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now re-run the same query with search grounding enabled.\n",
    "config_with_search = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    ")\n",
    "\n",
    "def query_with_grounding():\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=\"return full info of two recipes of gluten free or vegeterian recipe but quick and easy from food.com\",\n",
    "        config=config_with_search,\n",
    "    )\n",
    "    return response.candidates[0]\n",
    "\n",
    "\n",
    "rc = query_with_grounding()\n",
    "Markdown(rc.content.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5c3b9",
   "metadata": {},
   "source": [
    "## Step 2: Audio Input & Command Recognition with User Preferences\n",
    "\n",
    "This notebook implements the second step of our Interactive Recipe & Kitchen Management Assistant capstone project for the Google Gen AI Intensive Course. We'll create a voice interface that allows users to interact with our recipe assistant through spoken commands, recognize different types of user requests, and maintain user preferences.\n",
    "\n",
    "\n",
    "\n",
    "This step focuses on the **Audio understanding** Gen AI capability, which enables our assistant to:\n",
    "- Process voice commands using Google Cloud Speech-to-Text\n",
    "- Interpret user intent from natural language using Gemini Flash model\n",
    "- Store and retrieve user preferences for personalized experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d13bb04-a898-44c7-8ac7-16bf2afeb2bc",
   "metadata": {},
   "source": [
    "### Run your test prompt\n",
    "\n",
    "In this step, you will test that your API key is set up correctly by making a request.\n",
    "\n",
    "The Python SDK uses a [`Client` object](https://googleapis.github.io/python-genai/genai.html#genai.client.Client) to make requests to the API. The client lets you control which back-end to use (between the Gemini API and Vertex AI) and handles authentication (the API key).\n",
    "\n",
    "The `gemini-2.0-flash` model has been selected here.\n",
    "\n",
    "**Note**: If you see a `TransportError` on this step, you may need to **ðŸ” Factory reset** the notebook one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9ea50-8c6d-4e2e-8184-4b44ac1aa7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Hi, This is a test message! How are you?\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0df5a4-dfc3-468b-8e91-3c9a2899bb65",
   "metadata": {},
   "source": [
    "## Google Cloud Speech-to-Text API Setup\n",
    "\n",
    "To use Google Cloud Speech-to-Text, we need to set up authentication and configure the client. In a production environment, this would involve creating a service account and downloading the credentials. For demonstration in a Kaggle/local environment, we'll simulate the API response.\n",
    "\n",
    "> Note: In a real implementation, you would:\n",
    "> 1. Create a Google Cloud project\n",
    "> 2. Enable the Speech-to-Text API\n",
    "> 3. Create a service account with appropriate permissions\n",
    "> 4. Download the credentials JSON file\n",
    "> 5. Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to this file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba6157-6af5-4e31-90e8-76ffee0fca80",
   "metadata": {},
   "source": [
    "## Speech-to-Text Conversion\n",
    "\n",
    "Let's implement a real speech-to-text function using Google Cloud Speech-to-Text API. This will allow us to convert voice commands from audio files into text for processing. Unfortunately, the google STT needs a lot of parameters for configuration, for credential, and the auth section is headache! , I decided to move forward with lovely whisper-1 :D, sorry Google!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb032f-39b7-4062-a32c-a3b61d2fe15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(service=\"openai\", file_path=None, language=\"en\", api_key=None, credentials_path=None, credentials_json=None):\n",
    "    \"\"\"\n",
    "    Transcribe audio using either OpenAI or Google Cloud Speech-to-Text API.\n",
    "    \n",
    "    Args:\n",
    "        service (str): The service to use for transcription ('openai' or 'google')\n",
    "        file_path (str): Path to the audio file to transcribe\n",
    "        language (str): Language code (e.g., 'en' for OpenAI, 'en-US' for Google)\n",
    "        api_key (str): OpenAI API key (required for OpenAI service)\n",
    "        credentials_path (str): Path to Google credentials JSON file (optional for Google service)\n",
    "        credentials_json (str): JSON string of Google credentials (optional for Google service)\n",
    "        \n",
    "    Returns:\n",
    "        str: Transcription text or error message\n",
    "    \"\"\"\n",
    "    \n",
    "    if not file_path:\n",
    "        return \"Error: No file path provided\"\n",
    "        \n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"Error: File not found at {file_path}\"\n",
    "    \n",
    "    try:\n",
    "        if service.lower() == \"openai\":\n",
    "            if not api_key:\n",
    "                return \"Error: OpenAI API key required\"\n",
    "                \n",
    "            client = OpenAI(api_key=api_key)\n",
    "            \n",
    "            with open(file_path, \"rb\") as audio_file:\n",
    "                transcription = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\", \n",
    "                    file=audio_file,\n",
    "                    language=language\n",
    "                )\n",
    "            \n",
    "            return transcription.text\n",
    "            \n",
    "        elif service.lower() == \"google\":\n",
    "            temp_cred_file = None\n",
    "            \n",
    "            # Handle Google authentication\n",
    "            if not credentials_path and not credentials_json:\n",
    "                return \"Error: Either credentials_path or credentials_json required for Google service\"\n",
    "            \n",
    "            # If credentials_json is provided, write to a temporary file\n",
    "            if credentials_json:\n",
    "                try:\n",
    "                    # Create a temporary file for credentials\n",
    "                    temp_cred_file = tempfile.NamedTemporaryFile(delete=False, suffix='.json')\n",
    "                    temp_cred_path = temp_cred_file.name\n",
    "                    temp_cred_file.write(credentials_json.encode('utf-8'))\n",
    "                    temp_cred_file.close()\n",
    "                    \n",
    "                    # Set environment variable to the temporary file\n",
    "                    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = temp_cred_path\n",
    "                except Exception as e:\n",
    "                    if temp_cred_file and os.path.exists(temp_cred_file.name):\n",
    "                        os.unlink(temp_cred_file.name)\n",
    "                    return f\"Error creating temporary credentials file: {str(e)}\"\n",
    "            else:\n",
    "                # Use provided credentials_path\n",
    "                os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "            \n",
    "            try:\n",
    "                # Initialize the Speech client\n",
    "                client = speech.SpeechClient()\n",
    "                \n",
    "                # Read the audio file\n",
    "                with io.open(file_path, \"rb\") as audio_file:\n",
    "                    content = audio_file.read()\n",
    "                \n",
    "                # Determine encoding based on file extension\n",
    "                file_ext = os.path.splitext(file_path)[1].lower()\n",
    "                if file_ext == \".ogg\":\n",
    "                    encoding = speech.RecognitionConfig.AudioEncoding.OGG_OPUS\n",
    "                elif file_ext == \".wav\":\n",
    "                    encoding = speech.RecognitionConfig.AudioEncoding.LINEAR16\n",
    "                else:\n",
    "                    return f\"Error: Unsupported file format: {file_ext}\"\n",
    "                \n",
    "                # Configure the speech recognition\n",
    "                audio = speech.RecognitionAudio(content=content)\n",
    "                config = speech.RecognitionConfig(\n",
    "                    encoding=encoding,\n",
    "                    sample_rate_hertz=48000,  # May need adjustment based on actual audio file\n",
    "                    language_code=language if language else \"en-US\",\n",
    "                )\n",
    "                \n",
    "                # Perform the transcription\n",
    "                response = client.recognize(config=config, audio=audio)\n",
    "                \n",
    "                # Extract the transcription\n",
    "                if response.results:\n",
    "                    return response.results[0].alternatives[0].transcript\n",
    "                else:\n",
    "                    return \"No transcription results found\"\n",
    "                    \n",
    "            finally:\n",
    "                # Clean up temp file if it was created\n",
    "                if temp_cred_file and os.path.exists(temp_cred_file.name):\n",
    "                    os.unlink(temp_cred_file.name)\n",
    "        \n",
    "        else:\n",
    "            return f\"Error: Unknown service '{service}'. Use 'openai' or 'google'\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Clean up temp file if exception occurs\n",
    "        if service.lower() == \"google\" and temp_cred_file and os.path.exists(temp_cred_file.name):\n",
    "            os.unlink(temp_cred_file.name)\n",
    "        return f\"Error during transcription: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20988e5-3dbf-40b8-aae8-5e6e431718c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPENAI_API_KEY (openai) or  SecretValueJson (google)\n",
    "transcribe_audio(service=\"openai\", file_path=\"/kaggle/input/voice-tests/test.ogg\", language=\"en\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44bea40-3565-4925-b1a7-8b33fc73745a",
   "metadata": {},
   "source": [
    "## Implemented the Kitchen Management Assistant interface. The assistant provides a modern, interactive interface for users to either:\n",
    "\n",
    "\n",
    "### Text Input\n",
    "1. Click on the \"Text Input\" tab\n",
    "2. Type your kitchen-related request in the text area\n",
    "3. Click the \"Submit\" button\n",
    "4. The system will process your text request\n",
    "\n",
    "### Voice Selection\n",
    "1. Click on the \"Voice Selection\" tab\n",
    "2. Select a voice recording from the dropdown list\n",
    "3. Click the \"Transcribe Voice\" button\n",
    "4. The system will transcribe the audio and process the request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a282e-fb15-444a-9a9b-f2167497bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "voices = {\n",
    "  \"version\": \"1.0\",\n",
    "  \"voices\": [\n",
    "    {\n",
    "      \"file_path\": \"/kaggle/input/voice-tests/test.ogg\",\n",
    "      \"language\": \"en\",\n",
    "      \"description\": \"Voice instruction for baking a pizza\",\n",
    "      \"speaker_id\": \"nariman\",\n",
    "      \"is_processed\": False\n",
    "    },\n",
    "    {\n",
    "      \"file_path\": \"voices/test.wav\",\n",
    "      \"language\": \"en\",\n",
    "      \"description\": \"Test voice recording for the system\",\n",
    "      \"speaker_id\": \"user2\",\n",
    "      \"is_processed\": False\n",
    "    },\n",
    "  \n",
    "    \n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e9d07-9212-4a4e-882b-a92b18f10967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 311962,
     "sourceId": 783630,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7053270,
     "sourceId": 11281977,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7086722,
     "sourceId": 11328936,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
