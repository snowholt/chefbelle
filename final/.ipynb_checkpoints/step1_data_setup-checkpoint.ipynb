{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57806c9c",
   "metadata": {},
   "source": [
    "# Interactive Recipe & Kitchen Management Assistant\n",
    "\n",
    "## Step 1: Data Source & Setup\n",
    "\n",
    "This notebook implements the first step of our Interactive Recipe & Kitchen Management Assistant capstone project for the Google Gen AI Intensive Course. We'll acquire, explore, and prepare the recipe dataset that will serve as the foundation for our recipe retrieval and customization system.\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "The Interactive Recipe & Kitchen Management Assistant helps users:\n",
    "1. Discover recipes based on available ingredients\n",
    "2. Customize recipes according to dietary needs\n",
    "3. Receive step-by-step cooking guidance\n",
    "\n",
    "This assistant will use multiple Gen AI capabilities including:\n",
    "- Audio understanding (for voice input)\n",
    "- Few-shot prompting (for recipe customization)\n",
    "- Function calling (for specific recipe operations)\n",
    "- RAG (Retrieval Augmented Generation for recipe knowledge)\n",
    "- Grounding (using web search for supplemental information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e715f",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "Let's start by installing and importing the necessary libraries for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a33b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q pandas matplotlib seaborn \n",
    "\n",
    "# Install dependencies as needed:\n",
    "!pip install kagglehub[pandas-datasets]\n",
    "# Uncomment if you need to download the dataset via Kaggle API\n",
    "# !pip install -q kaggle\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !echo '{\"username\":\"YOUR_USERNAME\",\"key\":\"YOUR_KEY\"}' > ~/.kaggle/kaggle.json\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle datasets download -d shuyangli94/food-com-recipes-and-user-interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a62e49",
   "metadata": {},
   "source": [
    "## Importing the Dataset in Kaggle\n",
    "\n",
    "Since you're using Kaggle, you can easily import the Food.com Recipes dataset directly:\n",
    "\n",
    "1. Search for \"Food.com Recipes and User Interactions\" in the Kaggle datasets section\n",
    "2. Or use this direct link: https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions\n",
    "\n",
    "In Kaggle, you can either:\n",
    "- Add the dataset to your notebook directly from the \"Add data\" button in the right sidebar\n",
    "- Use the Kaggle datasets API as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8aaa08",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll use the Food.com Recipes and Interactions dataset. This contains recipe information including ingredients, steps, and user interactions.\n",
    "\n",
    "If you've downloaded the dataset using the Kaggle API, uncomment and use the data loading code below. Otherwise, we'll use a direct URL to access the data.\n",
    "\n",
    "loading both the vectorized and raw data and nutritional breakdown dataset that will be used in subsequent steps, particularly for the few-shot prompting recipe customization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f483204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Direct Kaggle dataset import\n",
    "# This is the easiest way to import datasets in Kaggle notebooks\n",
    "\n",
    "try:\n",
    "    # If the dataset is added via the \"Add data\" button, it will be available at /kaggle/input/\n",
    "    recipes_df = pd.read_csv('/kaggle/input/food-com-recipes-and-user-interactions/RAW_recipes.csv')\n",
    "    interactions_df = pd.read_csv('/kaggle/input/food-com-recipes-and-user-interactions/RAW_interactions.csv')\n",
    "    pp_recipes_df = pd.read_csv('/kaggle/input/food-com-recipes-and-user-interactions/PP_recipes.csv')\n",
    "    pp_users_df = pd.read_csv('/kaggle/input/food-com-recipes-and-user-interactions/PP_users.csv')\n",
    "    nutrition_df = pd.read_csv('/kaggle/input/nutritional-breakdown-of-foods/cleaned_nutrition_dataset.csv')\n",
    "\n",
    "    print(f\"Successfully loaded {len(recipes_df)} recipes\")\n",
    "    print(f\"Successfully loaded {len(interactions_df)} interactions\")\n",
    "    print(f\"Successfully loaded nutritional dataset with {len(nutrition_df)} records\")\n",
    "    print(f\"Successfully loaded vectorized recipe data with {len(pp_recipes_df)} records\")\n",
    "    print(f\"Successfully loaded vectorized user data with {len(pp_users_df)} records\")\n",
    "    \n",
    "    \n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset files not found. Please make sure you've added the dataset to your Kaggle notebook.\")\n",
    "    print(\"You can add it by clicking the 'Add data' button in the right sidebar.\")\n",
    "    print(\"Alternatively, you can use direct URLs if available.\")\n",
    "\n",
    "# Let's parse the JSON strings in the columns that contain lists\n",
    "if 'recipes_df' in locals():\n",
    "    # Check the actual structure of the dataframe\n",
    "    \n",
    "    # For Food.com dataset, ingredients, steps, and tags are stored as strings that represent lists\n",
    "    # We need to convert them from string representation to actual Python lists\n",
    "    try:\n",
    "        if 'ingredients' in recipes_df.columns:\n",
    "            recipes_df['ingredients'] = recipes_df['ingredients'].apply(eval)\n",
    "            print(\"Successfully parsed ingredients column\")\n",
    "        \n",
    "        if 'steps' in recipes_df.columns:\n",
    "            recipes_df['steps'] = recipes_df['steps'].apply(eval)\n",
    "            print(\"Successfully parsed steps column\")\n",
    "        \n",
    "        if 'tags' in recipes_df.columns:\n",
    "            recipes_df['tags'] = recipes_df['tags'].apply(eval)\n",
    "            print(\"Successfully parsed tags column\")\n",
    "            \n",
    "            # Add cuisine type based on tags\n",
    "            recipes_df['cuisine_type'] = recipes_df['tags'].apply(\n",
    "                lambda x: next((tag for tag in x if tag in ['italian', 'mexican', 'chinese', 'indian', 'french', 'thai']), 'other')\n",
    "            )\n",
    "        \n",
    "      \n",
    "        # Count number of ingredients\n",
    "        recipes_df['n_ingredients'] = recipes_df['ingredients'].apply(len)\n",
    "            \n",
    "        print(\"\\nDataset successfully processed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {e}\")\n",
    "        print(\"Column sample values:\")\n",
    "        for col in recipes_df.columns:\n",
    "            print(f\"{col}: {recipes_df[col].iloc[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e892e82",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's explore the dataset to understand its structure and content. This will help us plan our cleaning and preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009b822-9917-4cb8-87d7-19dacb33711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic dataset information\n",
    "print(\"Raw Datasets information:\")\n",
    "print(f\"Number of recipes: {len(recipes_df)}\")\n",
    "print(\"\\nDataset columns:\")\n",
    "print(recipes_df.columns.tolist())\n",
    "print(15 * \"-\")\n",
    "print(f\"Number of recipes: {len(interactions_df)}\")\n",
    "print(\"\\nDataset columns:\")\n",
    "print(interactions_df.columns.tolist())\n",
    "print(15 * \"-\")\n",
    "print(f\"Number of recipes: {len(nutrition_df)}\")\n",
    "print(\"\\nDataset columns:\")\n",
    "print(nutrition_df.columns.tolist())\n",
    "print(15 * \"-\")\n",
    "print(\"Vectorized Datasets information:\")\n",
    "\n",
    "print(f\"Number of recipes: {len(pp_recipes_df)}\")\n",
    "print(\"\\nDataset columns:\")\n",
    "print(pp_recipes_df.columns.tolist())\n",
    "print(15 * \"-\")\n",
    "print(f\"Number of recipes: {len(pp_users_df)}\")\n",
    "print(\"\\nDataset columns:\")\n",
    "print(pp_users_df.columns.tolist())\n",
    "print(15 * \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a440e3-ba61-491b-b254-83e24baf611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values using a lighter approach\n",
    "print(\"\\nData types:\")\n",
    "for col in recipes_df.columns:\n",
    "    print(f\"{col}: {recipes_df[col].dtype}\")\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_values = recipes_df.isnull().sum()\n",
    "for col, missing in zip(missing_values.index, missing_values.values):\n",
    "    if missing > 0:\n",
    "        print(f\"{col}: {missing} missing values ({missing/len(recipes_df):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382a397-d63e-4d20-83dc-e39a0db64e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Lighter summary statistics - only for numeric columns\n",
    "print(\"\\nNumeric columns summary:\")\n",
    "numeric_cols = recipes_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if numeric_cols:\n",
    "    # Show basic stats for numeric columns only\n",
    "    print(recipes_df[numeric_cols].describe().T[['count', 'mean', 'min', 'max']])\n",
    "else:\n",
    "    print(\"No numeric columns found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09174682-b8e2-4b0f-b712-072496eaf5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a few rows instead of full stats\n",
    "print(\"\\nSample rows:\")\n",
    "print(recipes_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266570ee-5772-4c3b-afd3-71121cb561fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of cuisine types\n",
    "plt.figure(figsize=(12, 6))\n",
    "if 'cuisine_type' in recipes_df.columns:\n",
    "    # Limit to top 15 cuisines to avoid cluttered plot\n",
    "    recipes_df['cuisine_type'].value_counts().nlargest(15).plot(kind='bar')\n",
    "    plt.title('Top 15 Cuisine Types')\n",
    "    plt.xlabel('Cuisine')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273ccae-3406-4d97-b10c-ab523eb83253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of cooking time - use smaller bins\n",
    "if 'cooking_time' in recipes_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Use log scale for better visualization if the range is large\n",
    "    if recipes_df['cooking_time'].max() > 5 * recipes_df['cooking_time'].median():\n",
    "        sns.histplot(recipes_df['cooking_time'].clip(upper=recipes_df['cooking_time'].quantile(0.95)), bins=20)\n",
    "        plt.title('Distribution of Cooking Time (minutes) - Clipped at 95th percentile')\n",
    "    else:\n",
    "        sns.histplot(recipes_df['cooking_time'], bins=20)\n",
    "        plt.title('Distribution of Cooking Time (minutes)')\n",
    "    plt.xlabel('Cooking Time (minutes)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218adf5-d481-4f8e-983b-d2d3bd010344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of ingredients distribution\n",
    "if 'n_ingredients' in recipes_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(recipes_df['n_ingredients'], bins=range(1, min(30, recipes_df['n_ingredients'].max()+1)))\n",
    "    plt.title('Distribution of Number of Ingredients')\n",
    "    plt.xlabel('Number of Ingredients')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3802d",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "\n",
    "Now we'll clean the data by:\n",
    "1. Removing duplicate recipes\n",
    "2. Normalizing ingredient names\n",
    "3. Standardizing measurements\n",
    "4. Handling missing values\n",
    "5. Creating dietary tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Number of duplicate recipes: {recipes_df.duplicated(subset=['name']).sum()}\")\n",
    "\n",
    "# Remove duplicates\n",
    "recipes_df = recipes_df.drop_duplicates(subset=['name']).reset_index(drop=True)\n",
    "print(f\"Number of recipes after removing duplicates: {len(recipes_df)}\")\n",
    "\n",
    "# Function to normalize ingredient names\n",
    "def normalize_ingredients(ingredient_list):\n",
    "    \"\"\"\n",
    "    Normalize ingredient names by removing quantities and standardizing format\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    # If ingredient_list is already a list of strings\n",
    "    if isinstance(ingredient_list, list):\n",
    "        for ingredient in ingredient_list:\n",
    "            # Skip empty ingredients\n",
    "            if not ingredient or not isinstance(ingredient, str):\n",
    "                continue\n",
    "            \n",
    "            # Remove quantities (simplified for demonstration)\n",
    "            cleaned = re.sub(r'^\\d+\\s+\\d+/\\d+\\s+', '', ingredient)\n",
    "            cleaned = re.sub(r'^\\d+/\\d+\\s+', '', cleaned)\n",
    "            cleaned = re.sub(r'^\\d+\\s+', '', cleaned)\n",
    "            \n",
    "            # Convert to lowercase and strip whitespace\n",
    "            cleaned = cleaned.lower().strip()\n",
    "            \n",
    "            normalized.append(cleaned)\n",
    "    else:\n",
    "        # Handle the case where ingredient_list might be a string or another format\n",
    "        print(\"Warning: Expected ingredient_list to be a list, but got:\", type(ingredient_list))\n",
    "        if isinstance(ingredient_list, str):\n",
    "            # Try to interpret as a string representation of a list\n",
    "            try:\n",
    "                actual_list = eval(ingredient_list) if ingredient_list.startswith('[') else [ingredient_list]\n",
    "                return normalize_ingredients(actual_list)\n",
    "            except:\n",
    "                normalized = [ingredient_list.lower().strip()]\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Apply normalization to ingredients - with error handling\n",
    "recipes_df['normalized_ingredients'] = recipes_df['ingredients'].apply(\n",
    "    lambda x: normalize_ingredients(x) if isinstance(x, list) or isinstance(x, str) else []\n",
    ")\n",
    "\n",
    "# Show a sample recipe with normalized ingredients\n",
    "if len(recipes_df) > 0:\n",
    "    sample_idx = 0\n",
    "    print(f\"Original ingredients: {recipes_df.iloc[sample_idx]['ingredients']}\")\n",
    "    print(f\"Normalized ingredients: {recipes_df.iloc[sample_idx]['normalized_ingredients']}\")\n",
    "else:\n",
    "    print(\"No recipes found in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify dietary tags based on ingredients\n",
    "def identify_dietary_tags(ingredients):\n",
    "    \"\"\"\n",
    "    Identify dietary preferences based on ingredients\n",
    "    \"\"\"\n",
    "    # Handle empty ingredients list\n",
    "    if not ingredients or not isinstance(ingredients, (list, str)):\n",
    "        return []\n",
    "        \n",
    "    # Convert list of ingredients to a single string for easier checking\n",
    "    ingredients_str = ' '.join(ingredients).lower()\n",
    "    \n",
    "    tags = []\n",
    "    \n",
    "    # Vegetarian check (simplified)\n",
    "    meat_ingredients = ['chicken', 'beef', 'pork', 'lamb', 'turkey', 'veal', 'bacon']\n",
    "    if not any(meat in ingredients_str for meat in meat_ingredients):\n",
    "        tags.append('vegetarian')\n",
    "        \n",
    "        # Vegan check (simplified)\n",
    "        animal_products = ['cheese', 'milk', 'cream', 'yogurt', 'butter', 'egg', 'honey']\n",
    "        if not any(product in ingredients_str for product in animal_products):\n",
    "            tags.append('vegan')\n",
    "    \n",
    "    # Gluten-free check (simplified)\n",
    "    gluten_ingredients = ['flour', 'wheat', 'barley', 'rye', 'pasta', 'bread']\n",
    "    if not any(gluten in ingredients_str for gluten in gluten_ingredients):\n",
    "        tags.append('gluten-free')\n",
    "    \n",
    "    # Low-carb check (simplified)\n",
    "    high_carb_ingredients = ['sugar', 'pasta', 'rice', 'potato', 'bread', 'flour']\n",
    "    if not any(carb in ingredients_str for carb in high_carb_ingredients):\n",
    "        tags.append('low-carb')\n",
    "    \n",
    "    # Dairy-free check\n",
    "    dairy_ingredients = ['milk', 'cheese', 'cream', 'yogurt', 'butter']\n",
    "    if not any(dairy in ingredients_str for dairy in dairy_ingredients):\n",
    "        tags.append('dairy-free')\n",
    "    \n",
    "    return tags\n",
    "\n",
    "# Apply dietary tagging\n",
    "recipes_df['dietary_tags'] = recipes_df['normalized_ingredients'].apply(identify_dietary_tags)\n",
    "\n",
    "# Show the distribution of dietary tags\n",
    "diet_counts = {}\n",
    "for tags in recipes_df['dietary_tags']:\n",
    "    for tag in tags:\n",
    "        diet_counts[tag] = diet_counts.get(tag, 0) + 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "diet_df = pd.Series(diet_counts).sort_values(ascending=False)\n",
    "diet_df.plot(kind='bar')\n",
    "plt.title('Distribution of Dietary Tags')\n",
    "plt.xlabel('Dietary Tag')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show sample recipes with their dietary tags\n",
    "print(\"\\nSample recipes with dietary tags:\")\n",
    "sample_recipes = recipes_df[['name', 'normalized_ingredients', 'dietary_tags']].sample(5)\n",
    "for _, recipe in sample_recipes.iterrows():\n",
    "    print(f\"\\nRecipe: {recipe['name']}\")\n",
    "    print(f\"Ingredients: {', '.join(recipe['normalized_ingredients'])}\")\n",
    "    print(f\"Dietary Tags: {', '.join(recipe['dietary_tags']) if recipe['dietary_tags'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63edc2",
   "metadata": {},
   "source": [
    "## Final Data Structure and Storage\n",
    "\n",
    "Now we'll organize the data into the final structure and save it for use in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcac9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for loading and saving data\n",
    "# For Kaggle's output sharing feature\n",
    "DATA_DIR = Path('/kaggle/input/step1-data-setup')\n",
    "FINAL_DIR = Path('.')\n",
    "RECIPE_FILE = FINAL_DIR / 'processed_recipes.json'\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# # Try to load the processed recipe data from Step 1\n",
    "try:\n",
    "    # Check if the file exists in the Kaggle input directory (if step1 was saved as a dataset)\n",
    "    kaggle_json_path = DATA_DIR / 'processed_recipes.json'\n",
    "    \n",
    "    # First check if the file is in the current directory (where step1 might have saved it)\n",
    "    if RECIPE_FILE.exists():\n",
    "        with open(RECIPE_FILE, 'r') as f:\n",
    "            recipes_data = json.load(f)\n",
    "        recipes_df = pd.DataFrame(recipes_data)\n",
    "        print(f\"Loaded {len(recipes_df)} recipes from JSON file in current directory\")\n",
    "    \n",
    "    # Check if JSON file exists in Kaggle input directory\n",
    "    elif kaggle_json_path.exists():\n",
    "        with open(kaggle_json_path, 'r') as f:\n",
    "            recipes_data = json.load(f)\n",
    "        recipes_df = pd.DataFrame(recipes_data)\n",
    "        print(f\"Loaded {len(recipes_df)} recipes from Kaggle dataset input directory (JSON)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading recipe data: {e}\")   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807fea5",
   "metadata": {},
   "source": [
    "## Data Integration for Recipe Customization\n",
    "\n",
    "In Step 3, we'll need to implement few-shot prompting for recipe customization. For this, we'll leverage the following data sources:\n",
    "\n",
    "1. **Raw Recipe Data**: Provides readable recipe ingredients, steps, and descriptions\n",
    "2. **Vectorized Recipe Data**: Contains pre-processed tokens and numerical representations for efficient similarity matching\n",
    "3. **Nutritional Data**: Allows us to make informed decisions about ingredient substitutions\n",
    "\n",
    "This integrated data will enable us to:\n",
    "- Adjust recipes based on dietary requirements\n",
    "- Suggest ingredient substitutions with similar nutritional profiles\n",
    "- Scale recipes for different serving sizes\n",
    "- Adapt cooking methods based on available equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store references to the datasets for use in further steps\n",
    "datasets = {\n",
    "    'raw_recipes': recipes_df if 'recipes_df' in locals() else None,\n",
    "    'raw_interactions': interactions_df if 'interactions_df' in locals() else None,\n",
    "    'vectorized_recipes': pp_recipes_df if 'pp_recipes_df' in locals() else None,\n",
    "    'vectorized_users': pp_users_df if 'pp_users_df' in locals() else None,\n",
    "    'nutrition': nutrition_df if 'nutrition_df' in locals() else None\n",
    "}\n",
    "\n",
    "# Save the datasets dictionary to a pickle file for easy access in step 3\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    with open('datasets.pkl', 'wb') as f:\n",
    "        pickle.dump(datasets, f)\n",
    "    print(\"Successfully saved datasets dictionary to datasets.pkl for use in subsequent steps\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving datasets: {e}\")\n",
    "\n",
    "# Optional: Save a metadata file with information about the datasets\n",
    "metadata = {\n",
    "    'dataset_shapes': {\n",
    "        name: df.shape if df is not None else None for name, df in datasets.items()\n",
    "    },\n",
    "    'dataset_columns': {\n",
    "        name: df.columns.tolist() if df is not None else None for name, df in datasets.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open('dataset_metadata.json', 'w') as f:\n",
    "        # Convert any NumPy types to Python native types for JSON serialization\n",
    "        import json\n",
    "        def convert(o):\n",
    "            if isinstance(o, np.int64): return int(o)\n",
    "            if isinstance(o, np.float64): return float(o)\n",
    "            raise TypeError\n",
    "        \n",
    "        json.dump(metadata, f, default=convert)\n",
    "    print(\"Successfully saved dataset metadata to dataset_metadata.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving metadata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5c3b9",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've completed Step 1 of our Interactive Recipe & Kitchen Management Assistant:\n",
    "\n",
    "1. We loaded and explored the Food.com recipe dataset\n",
    "2. We cleaned the data by removing duplicates and normalizing ingredients\n",
    "3. We enhanced the data with dietary preference tags \n",
    "4. We structured the data in a format that will facilitate future steps\n",
    "\n",
    "This processed dataset will serve as the foundation for:\n",
    "- Building our vector database for RAG implementation\n",
    "- Creating few-shot examples for recipe customization\n",
    "- Developing function calling capabilities for specific recipe operations\n",
    "\n",
    "**Next steps:**\n",
    "- Step 2: Implement audio input and command recognition\n",
    "- Step 3: Develop few-shot prompting for recipe customization\n",
    "- Step 4: Create RAG implementation for recipe retrieval\n",
    "\n",
    "The data preparation steps in this notebook may appear simple, but they're crucial for ensuring our Gen AI components work effectively in subsequent steps. Clean, well-structured data will lead to better embedding representations, more accurate text matching, and overall improved performance."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 311962,
     "sourceId": 783630,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7053270,
     "sourceId": 11281977,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
