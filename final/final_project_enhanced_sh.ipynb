{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11463561,"sourceType":"datasetVersion","datasetId":7183447}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Generative AI Capstone Project\n\n**Title:** Simulating Generative AI Capabilities: Document Understanding + Controlled Output + Few-Shot Prompting  \n**Author:**  Shilan Rashidian \n\n**Date:** April 18, 2025\n\n## Overview\n\nThis project demonstrates key capabilities of Generative AI using local simulation methods due to limitations in accessing external APIs (e.g., Google Gemini) within the Kaggle Notebook environment.\n\n### Covered Capabilities:\n1. Document Understanding  \n2. Controlled Generation (Structured Output)  \n3. Few-Shot Prompting (Simulated)\n\nWe will process a document, extract meaningful answers to questions, format responses as JSON, and simulate how few-shot examples improve model accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"## ğŸ“„ Enhanced PDF Document Understanding with Conversational Interface\n\nThis notebook demonstrates an interactive system for understanding PDF documents using LangChain and Google Generative AI (Gemini).\n\n**Functionality:**\n1.  **Upload PDF:** Allows you to upload a PDF document.\n2.  **Process Document:** Extracts text, splits it into manageable chunks, and creates vector embeddings.\n3.  **Conversational Q&A:** Enables you to ask questions about the document content. The system uses the document context and remembers the conversation history.\n\n**Note:** This notebook requires a Google API Key configured in Kaggle Secrets (or as an environment variable) to use Google's Generative AI models.","metadata":{}},{"cell_type":"markdown","source":"### 1. Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Upgrade pip and install all required libraries together for better dependency resolution\n!pip install --upgrade -q pip\n!pip install -U -q langchain langchain-core langchain-community langchain-google-genai google-generativeai pypdf chromadb tiktoken ipywidgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:29:24.864267Z","iopub.execute_input":"2025-04-18T15:29:24.864652Z","iopub.status.idle":"2025-04-18T15:30:16.912250Z","shell.execute_reply.started":"2025-04-18T15:29:24.864621Z","shell.execute_reply":"2025-04-18T15:30:16.911026Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Setup Google API Key\n\nLoad the Google API Key from Kaggle Secrets and set it as an environment variable. LangChain components will automatically detect and use this key.","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    #os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"âœ… Google API Key loaded successfully from Kaggle Secrets.\")\nexcept ImportError:\n    print(\"ğŸ”‘ Kaggle Secrets not available. Checking for GOOGLE_API_KEY environment variable.\")\n    #GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n    if GOOGLE_API_KEY:\n        print(\"âœ… Google API Key found in environment variables.\")\n    else:\n        warnings.warn(\"ğŸ›‘ Google API Key not found in Kaggle Secrets or environment variables. AI features will fail.\")\n        GOOGLE_API_KEY = None # Explicitly set to None if not found\nexcept Exception as e:\n    warnings.warn(f\"âŒ Failed to load Google API Key: {e}. AI features may fail.\")\n    GOOGLE_API_KEY = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:41:30.871888Z","iopub.execute_input":"2025-04-18T15:41:30.872194Z","iopub.status.idle":"2025-04-18T15:41:31.159251Z","shell.execute_reply.started":"2025-04-18T15:41:30.872173Z","shell.execute_reply":"2025-04-18T15:41:31.158403Z"}},"outputs":[{"name":"stdout","text":"âœ… Google API Key loaded successfully from Kaggle Secrets.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### 3. Import Libraries & Define Helper Functions","metadata":{}},{"cell_type":"code","source":"# Rerun this cell with the modified create_retriever function\n\nimport tempfile\nfrom IPython.display import display, Markdown\nimport ipywidgets as widgets\nimport os # Ensure os is imported\n\n# LangChain components - Using updated/correct import paths\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationalRetrievalChain\n\n# --- Helper Functions ---\n\ndef load_pdf(file_path):\n    \"\"\"Loads and splits the PDF document into chunks.\"\"\"\n    try:\n        loader = PyPDFLoader(file_path)\n        docs = loader.load()\n        if not docs:\n            print(\"âš ï¸ Warning: No text could be extracted from the PDF.\")\n            return []\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n        split_docs = splitter.split_documents(docs)\n        print(f\"ğŸ“„ PDF loaded and split into {len(split_docs)} chunks.\")\n        return split_docs\n    except Exception as e:\n        print(f\"âŒ Error loading or splitting PDF: {e}\")\n        return None\n\n# MODIFIED FUNCTION BELOW\ndef create_retriever(docs):\n    \"\"\"Creates embeddings and a vector store retriever.\"\"\"\n    # Retrieve the API key from the environment where it was set earlier\n    api_key = GOOGLE_API_KEY\n\n    if not api_key:\n        print(\"âŒ Cannot create retriever: Google API Key is missing from environment variables.\")\n        return None\n    if not docs:\n        print(\"âŒ Cannot create retriever: No documents provided.\")\n        return None\n\n    try:\n        # Explicitly pass the API key here\n        embedding = GoogleGenerativeAIEmbeddings(\n            model=\"models/embedding-001\",\n            google_api_key=api_key # <-- Explicitly pass the key\n        )\n        print(\"â³ Creating Chroma vector store (this may take a moment)...\")\n        vectordb = Chroma.from_documents(\n            documents=docs,\n            embedding=embedding\n        )\n        retriever = vectordb.as_retriever(search_kwargs={'k': 5}) # Retrieve top 5 relevant chunks\n        print(\"âœ… Vector store and retriever created successfully.\")\n        return retriever\n    except Exception as e:\n        # Catch potential errors during embedding or Chroma creation\n        print(f\"âŒ Error creating retriever: {e}\")\n        # You might want to print more details for debugging:\n        # import traceback\n        # traceback.print_exc()\n        return None\n# END OF MODIFIED FUNCTION\n\ndef setup_qa_chain(retriever):\n    \"\"\"Sets up the conversational Q&A chain with memory.\"\"\"\n    api_key = GOOGLE_API_KEY\n    if not api_key:\n        print(\"âŒ Cannot setup QA chain: Google API Key is missing.\")\n        return None\n    if not retriever:\n        print(\"âŒ Cannot setup QA chain: Retriever is not available.\")\n        return None\n    try:\n        # Pass the key explicitly to the chat model too for consistency\n        model = ChatGoogleGenerativeAI(\n            model=\"gemini-1.5-flash-latest\",\n            temperature=0.2,\n            convert_system_message_to_human=True,\n            google_api_key=GOOGLE_API_KEY\n            )\n        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n        qa_chain = ConversationalRetrievalChain.from_llm(\n            llm=model,\n            retriever=retriever,\n            memory=memory,\n            verbose=False\n        )\n        print(\"âœ… Conversational Q&A chain is ready.\")\n        return qa_chain\n    except Exception as e:\n        print(f\"âŒ Error setting up QA chain: {e}\")\n        return None\n\nprint(\"Libraries imported and helper functions defined (create_retriever updated).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:42:29.818418Z","iopub.execute_input":"2025-04-18T15:42:29.818730Z","iopub.status.idle":"2025-04-18T15:42:29.831181Z","shell.execute_reply.started":"2025-04-18T15:42:29.818709Z","shell.execute_reply":"2025-04-18T15:42:29.830278Z"}},"outputs":[{"name":"stdout","text":"Libraries imported and helper functions defined (create_retriever updated).\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### 4. Upload PDF Document","metadata":{}},{"cell_type":"code","source":"# Create and display the file uploader widget\nuploader = widgets.FileUpload(\n    accept='.pdf',  # Only accept PDF files\n    multiple=False, # Allow only one file upload\n    description='Upload PDF'\n)\ndisplay(uploader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:31:46.192027Z","iopub.execute_input":"2025-04-18T15:31:46.192536Z","iopub.status.idle":"2025-04-18T15:31:46.201756Z","shell.execute_reply.started":"2025-04-18T15:31:46.192510Z","shell.execute_reply":"2025-04-18T15:31:46.200809Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"FileUpload(value=(), accept='.pdf', description='Upload PDF')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1f210252e7b4ea095bd9b32088384aa"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### 5. Process Uploaded PDF and Initialize Chat System\n\nOnce you upload a file using the widget above, run this cell to process it and set up the Q&A chain.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os # Ensure os is imported\n\n# --- Specify the path to your PDF file within the Kaggle environment ---\nkaggle_file_path = \"/kaggle/input/test-pdf/Learning_Tenserflow_buliding_deep.pdf\"\n# ---------------------------------------------------------------------\n\n# Initialize state variables\npdf_docs = None\nretriever = None\nqa_chain = None\nfile_path = None # Keep track of the path being used\n\nprint(f\"Attempting to process file: {kaggle_file_path}\")\n\n# Check if the file exists\nif os.path.exists(kaggle_file_path):\n    file_path = kaggle_file_path # Set the global file_path variable\n\n    # --- Direct Processing Logic ---\n    try:\n        # 1. Load and split the PDF\n        print(\"â³ Loading and splitting PDF...\")\n        pdf_docs = load_pdf(file_path) # Use the helper function defined earlier\n\n        if pdf_docs:\n            # 2. Create the retriever\n            print(\"â³ Creating vector store and retriever...\")\n            retriever = create_retriever(pdf_docs) # Use the helper function\n\n            if retriever:\n                # 3. Setup the QA chain\n                print(\"â³ Setting up conversational Q&A chain...\")\n                qa_chain = setup_qa_chain(retriever) # Use the helper function\n\n                if qa_chain:\n                    print(\"\\nâœ… Chat system is ready! You can now ask questions in the next cell.\")\n                else:\n                    print(\"\\nâŒ Failed to initialize the chat system after creating retriever.\")\n            else:\n                print(\"\\nâŒ Failed to initialize the chat system because retriever creation failed.\")\n        else:\n            print(\"\\nâŒ Failed to initialize the chat system because PDF processing failed.\")\n\n    except Exception as e:\n        print(f\"âŒ An error occurred during file processing: {e}\")\n    # No finally block needed here for temp file cleanup as we are using a direct path\nelse:\n    print(f\"âŒ Error: File not found at the specified path: {kaggle_file_path}\")\n    print(\"Please ensure the path is correct and the dataset is added to the notebook.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:42:37.714579Z","iopub.execute_input":"2025-04-18T15:42:37.715189Z","iopub.status.idle":"2025-04-18T15:43:01.498207Z","shell.execute_reply.started":"2025-04-18T15:42:37.715160Z","shell.execute_reply":"2025-04-18T15:43:01.497000Z"}},"outputs":[{"name":"stdout","text":"Attempting to process file: /kaggle/input/test-pdf/Learning_Tenserflow_buliding_deep.pdf\nâ³ Loading and splitting PDF...\nğŸ“„ PDF loaded and split into 603 chunks.\nâ³ Creating vector store and retriever...\nâ³ Creating Chroma vector store (this may take a moment)...\nâœ… Vector store and retriever created successfully.\nâ³ Setting up conversational Q&A chain...\nâœ… Conversational Q&A chain is ready.\n\nâœ… Chat system is ready! You can now ask questions in the next cell.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2794782452.py:87: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### 6. Start Chatting!\n\nRun the cell below to start an interactive chat session. Ask questions about the PDF you uploaded. Type `exit` to end the chat.","metadata":{}},{"cell_type":"code","source":"import time\n# os should be imported already, but ensure it is if running cells independently\nimport os\n\n# Check if qa_chain was successfully created in the previous step\nif 'qa_chain' not in globals() or qa_chain is None:\n    print(\"âš ï¸ Chat system is not ready. Please check the output of the previous cell for errors.\")\n    if 'file_path' not in globals() or not file_path:\n         print(\"   Reason: PDF file path was not set or the file was not found/processed.\")\n    elif 'pdf_docs' not in globals() or not pdf_docs:\n         print(\"   Reason: PDF document loading/splitting failed.\")\n    elif 'retriever' not in globals() or not retriever:\n         print(\"   Reason: Vector store retriever creation failed (check API key and embeddings).\")\n    elif 'qa_chain' not in globals() or qa_chain is None:\n         print(\"   Reason: Conversational chain setup failed (check model initialization).\")\n\nelse:\n    print(f\"ğŸ’¬ Starting chat session about '{os.path.basename(file_path)}'. Type 'exit' to quit.\")\n    print(\"---\")\n    while True:\n        try:\n            question = input(\"ğŸ‘¤ You: \")\n            if question.strip().lower() == \"exit\":\n                print(\"\\nğŸ‘‹ Goodbye!\")\n                break\n            if not question.strip():\n                continue\n\n            start_time = time.time()\n            # Invoke the chain\n            result = qa_chain.invoke({\"question\": question})\n            end_time = time.time()\n\n            # Print the answer\n            print(f\"\\nğŸ¤– Assistant ({end_time - start_time:.2f}s):\")\n            # Display the answer using Markdown for better formatting potential\n            display(Markdown(result['answer']))\n            print(\"---\")\n\n        except EOFError:\n            # Handle abrupt termination if running in certain environments\n            print(\"\\nğŸ‘‹ Session ended unexpectedly.\")\n            break\n        except Exception as e:\n            print(f\"\\nâŒ An error occurred during chat: {e}\")\n            # Optional: break the loop on error, or allow user to continue\n            # break\n\n# No temporary file cleanup needed as we used a direct path\nprint(\"\\nChat session finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:43:11.781464Z","iopub.execute_input":"2025-04-18T15:43:11.782201Z"}},"outputs":[{"name":"stdout","text":"ğŸ’¬ Starting chat session about 'Learning_Tenserflow_buliding_deep.pdf'. Type 'exit' to quit.\n---\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"ğŸ‘¤ You:  Hi, what is this about?\n"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ¤– Assistant (3.07s):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This text is an index and excerpts from a book about TensorFlow, a deep learning framework.  The excerpts cover topics such as:\n\n* **TensorFlow's capabilities:**  Including using pre-trained models and utilities.\n* **Image captioning:**  A deep learning application focusing on generating natural language descriptions for images.\n* **TensorFlow Serving:**  A system for deploying and serving TensorFlow models.\n* **Tensors:**  The fundamental data structures in TensorFlow, including their attributes, data types, and manipulation.\n* **Deep learning concepts:**  Such as backpropagation, word embeddings, and various model architectures (RNNs, autoencoders).\n* **Practical examples and code snippets:** Hints at the inclusion of practical examples using TensorFlow.\n\nThe overall subject is a guide to using TensorFlow for deep learning, with a focus on practical applications and implementation details."},"metadata":{}},{"name":"stdout","text":"---\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nThis notebook demonstrated how to build a conversational interface for PDF documents using LangChain and Google Generative AI. Key steps included:\n\n1.  Setting up the environment and API keys.\n2.  Loading and processing PDF documents (`PyPDFLoader`, `RecursiveCharacterTextSplitter`).\n3.  Creating vector embeddings and a retriever (`GoogleGenerativeAIEmbeddings`, `Chroma`).\n4.  Building a conversational chain with memory (`ChatGoogleGenerativeAI`, `ConversationBufferMemory`, `ConversationalRetrievalChain`).\n5.  Providing an interactive chat interface.\n\nThis serves as a foundation for more advanced document interaction applications.","metadata":{}}]}